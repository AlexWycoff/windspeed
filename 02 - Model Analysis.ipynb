{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b73532717ceab7c",
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "source": [
    "## Model Analysis Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89411512a29a662",
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "source": [
    "In this notebook, we train, test, and evaluate the performance of an LSTM model in wind speed prediction and compare results to the persistence method, which is a common benchmark for wind speed prediction algorithms."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 7,
=======
   "execution_count": 190,
>>>>>>> Stashed changes
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T21:42:06.174155Z",
     "start_time": "2024-08-22T21:42:06.171073Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 8,
   "id": "482b2cba580d03b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T04:34:55.985319Z",
     "start_time": "2024-08-18T04:34:55.981804Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
=======
   "execution_count": 191,
   "id": "482b2cba580d03b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T21:42:06.284418Z",
     "start_time": "2024-08-22T21:42:06.282449Z"
>>>>>>> Stashed changes
    }
   },
   "outputs": [],
   "source": [
    "# Define how many time steps will be used in observation and prediction\n",
    "n_past = 28 # The last seven days of data\n",
    "n_features = 6\n",
    "\n",
    "# Set the universal font size for matplotlib\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 9,
   "id": "db83f8ff572993e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T04:34:56.747745Z",
     "start_time": "2024-08-18T04:34:56.743742Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
=======
   "execution_count": 192,
   "id": "db83f8ff572993e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T21:42:06.404788Z",
     "start_time": "2024-08-22T21:42:06.402242Z"
>>>>>>> Stashed changes
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to split the series using a sliding window\n",
    "def split_series(series, n_past=n_past):\n",
    "    X, y = list(), list()\n",
    "    n_past = n_past * 24\n",
    "    for i in range(n_past, len(series)):\n",
    "        X.append(series[range(i - n_past, i, 24), :])\n",
    "        y.append(float(series[i, 2]))\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 10,
   "id": "bbda487b9b141fb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T04:34:57.290203Z",
     "start_time": "2024-08-18T04:34:57.282337Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
=======
   "execution_count": 193,
   "id": "bbda487b9b141fb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T21:42:06.740327Z",
     "start_time": "2024-08-22T21:42:06.735371Z"
>>>>>>> Stashed changes
    }
   },
   "outputs": [],
   "source": [
    "# Process and split the data for a site given its filename\n",
    "def prep_data(filename, cy=2018):\n",
    "    # Import the data for a single point\n",
    "    data = pd.read_csv(\"Data/WTK_LED CONUS [2018-2020] 60min/\" + filename, index_col=0)\n",
    "    startyear = 2018\n",
    "\n",
    "    # Restrict the data to the last 5 years, giving us 4 years of training and 1 year of testing data\n",
    "    data = data.iloc[int(len(data)*(cy-startyear)/20):]\n",
    "\n",
    "    # Split the data into training and testing samples\n",
    "    cutoff = int(len(data)*0.8)\n",
    "    test_data = data.iloc[cutoff:]\n",
    "    data = data.iloc[:cutoff]\n",
    "    \n",
    "    # Designate which columns are used for training\n",
    "    columns = [5, 6, 7, 8, 9, 10]\n",
    "    \n",
    "    # Normalize the testing and training data\n",
    "    test_data.iloc[:, columns], test_norms = normalize(test_data.iloc[:, columns], axis=0, norm='max', return_norm=True)\n",
    "    data.iloc[:, columns], train_norms = normalize(data.iloc[:, columns], axis=0, norm='max', return_norm=True)\n",
    "\n",
    "    # Split the data into series for training\n",
    "    X_train, y_train = split_series(np.array(data.iloc[:, columns]))\n",
    "    X_test, y_test = split_series(np.array(test_data.iloc[:, columns]))\n",
    "\n",
    "    # Adjust the expected output to contain only the wind speed\n",
    "    # y_train, y_test = y_train[:, :, 2], y_test[:, :, 2]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, train_norms, test_norms"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 11,
   "id": "be6ecadfc9437cad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T04:35:00.210891Z",
     "start_time": "2024-08-18T04:35:00.204976Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
=======
   "execution_count": 194,
   "id": "be6ecadfc9437cad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T21:42:06.985766Z",
     "start_time": "2024-08-22T21:42:06.982183Z"
>>>>>>> Stashed changes
    }
   },
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "def define_model():\n",
    "    # Lighter model used for additional training\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Input(shape=(n_past, n_features)))\n",
    "    model.add(LSTM(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(Dense(1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 12,
   "id": "6343d8cad2878f10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T04:35:00.849969Z",
     "start_time": "2024-08-18T04:35:00.770876Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
=======
   "execution_count": 195,
   "id": "6343d8cad2878f10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T21:42:07.573654Z",
     "start_time": "2024-08-22T21:42:07.526555Z"
>>>>>>> Stashed changes
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 256)               269312    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 312,577\n",
      "Trainable params: 312,577\n",
=======
      "Model: \"sequential_311\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_311 (LSTM)             (None, 512)               1062912   \n",
      "                                                                 \n",
      " dropout_311 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1280 (Dense)          (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_1281 (Dense)          (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1282 (Dense)          (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,210,753\n",
      "Trainable params: 1,210,753\n",
>>>>>>> Stashed changes
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
<<<<<<< Updated upstream
       "(None, 7, 6)"
      ]
     },
     "execution_count": 12,
=======
       "(None, 28, 6)"
      ]
     },
     "execution_count": 195,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_model()\n",
    "model.summary()\n",
    "model.input_shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "id": "582fd03776ec9732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T04:35:42.015729Z",
     "start_time": "2024-08-18T04:35:01.166697Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
   "execution_count": 196,
   "id": "1e159820",
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/15\n",
      "163/163 [==============================] - 5s 14ms/step - loss: 0.2594 - val_loss: 0.1732\n",
      "Epoch 2/15\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 0.1484 - val_loss: 0.1398\n",
      "Epoch 3/15\n",
      "146/163 [=========================>....] - ETA: 0s - loss: 0.1286"
=======
      "Epoch 1/75\n",
      "159/159 [==============================] - 15s 90ms/step - loss: 0.2758 - val_loss: 0.1472\n",
      "Epoch 2/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1499 - val_loss: 0.1285\n",
      "Epoch 3/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1370 - val_loss: 0.1191\n",
      "Epoch 4/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1333 - val_loss: 0.1131\n",
      "Epoch 5/75\n",
      "159/159 [==============================] - 15s 92ms/step - loss: 0.1311 - val_loss: 0.1179\n",
      "Epoch 6/75\n",
      "159/159 [==============================] - 14s 89ms/step - loss: 0.1305 - val_loss: 0.1289\n",
      "Epoch 7/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1300 - val_loss: 0.1161\n",
      "Epoch 8/75\n",
      "159/159 [==============================] - 14s 89ms/step - loss: 0.1294 - val_loss: 0.1121\n",
      "Epoch 9/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1295 - val_loss: 0.1245\n",
      "Epoch 10/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1291 - val_loss: 0.1081\n",
      "Epoch 11/75\n",
      "159/159 [==============================] - 14s 89ms/step - loss: 0.1293 - val_loss: 0.1093\n",
      "Epoch 12/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1292 - val_loss: 0.1098\n",
      "Epoch 13/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1289 - val_loss: 0.1214\n",
      "Epoch 14/75\n",
      "159/159 [==============================] - 14s 88ms/step - loss: 0.1289 - val_loss: 0.1100\n",
      "Epoch 15/75\n",
      "159/159 [==============================] - 14s 89ms/step - loss: 0.1284 - val_loss: 0.1150\n",
      "Epoch 16/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1283 - val_loss: 0.1128\n",
      "Epoch 17/75\n",
      "159/159 [==============================] - 14s 89ms/step - loss: 0.1286 - val_loss: 0.1122\n",
      "Epoch 18/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1288 - val_loss: 0.1142\n",
      "Epoch 19/75\n",
      "159/159 [==============================] - 15s 94ms/step - loss: 0.1285 - val_loss: 0.1127\n",
      "Epoch 20/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1281 - val_loss: 0.1255\n",
      "Epoch 21/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1283 - val_loss: 0.1136\n",
      "Epoch 22/75\n",
      "159/159 [==============================] - 14s 89ms/step - loss: 0.1282 - val_loss: 0.1144\n",
      "Epoch 23/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1282 - val_loss: 0.1240\n",
      "Epoch 24/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1281 - val_loss: 0.1178\n",
      "Epoch 25/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1281 - val_loss: 0.1116\n",
      "Epoch 26/75\n",
      "159/159 [==============================] - 15s 91ms/step - loss: 0.1281 - val_loss: 0.1137\n",
      "Epoch 27/75\n",
      "159/159 [==============================] - 15s 94ms/step - loss: 0.1276 - val_loss: 0.1104\n",
      "Epoch 28/75\n",
      "159/159 [==============================] - 15s 92ms/step - loss: 0.1278 - val_loss: 0.1129\n",
      "Epoch 29/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1273 - val_loss: 0.1121\n",
      "Epoch 30/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1273 - val_loss: 0.1161\n",
      "Epoch 31/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1271 - val_loss: 0.1136\n",
      "Epoch 32/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1271 - val_loss: 0.1104\n",
      "Epoch 33/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1273 - val_loss: 0.1145\n",
      "Epoch 34/75\n",
      "159/159 [==============================] - 15s 92ms/step - loss: 0.1267 - val_loss: 0.1176\n",
      "Epoch 35/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1271 - val_loss: 0.1092\n",
      "Epoch 36/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1268 - val_loss: 0.1102\n",
      "Epoch 37/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1265 - val_loss: 0.1119\n",
      "Epoch 38/75\n",
      "159/159 [==============================] - 14s 89ms/step - loss: 0.1264 - val_loss: 0.1121\n",
      "Epoch 39/75\n",
      "159/159 [==============================] - 14s 89ms/step - loss: 0.1264 - val_loss: 0.1197\n",
      "Epoch 40/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1263 - val_loss: 0.1123\n",
      "Epoch 41/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1263 - val_loss: 0.1073\n",
      "Epoch 42/75\n",
      "159/159 [==============================] - 15s 92ms/step - loss: 0.1258 - val_loss: 0.1136\n",
      "Epoch 43/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1259 - val_loss: 0.1103\n",
      "Epoch 44/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1257 - val_loss: 0.1133\n",
      "Epoch 45/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1258 - val_loss: 0.1095\n",
      "Epoch 46/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1255 - val_loss: 0.1156\n",
      "Epoch 47/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1257 - val_loss: 0.1082\n",
      "Epoch 48/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1252 - val_loss: 0.1108\n",
      "Epoch 49/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1258 - val_loss: 0.1131\n",
      "Epoch 50/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1252 - val_loss: 0.1127\n",
      "Epoch 51/75\n",
      "159/159 [==============================] - 15s 92ms/step - loss: 0.1249 - val_loss: 0.1069\n",
      "Epoch 52/75\n",
      "159/159 [==============================] - 15s 92ms/step - loss: 0.1248 - val_loss: 0.1129\n",
      "Epoch 53/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1247 - val_loss: 0.1119\n",
      "Epoch 54/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1251 - val_loss: 0.1114\n",
      "Epoch 55/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1241 - val_loss: 0.1100\n",
      "Epoch 56/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1245 - val_loss: 0.1123\n",
      "Epoch 57/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1245 - val_loss: 0.1162\n",
      "Epoch 58/75\n",
      "159/159 [==============================] - 15s 93ms/step - loss: 0.1242 - val_loss: 0.1194\n",
      "Epoch 59/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1236 - val_loss: 0.1108\n",
      "Epoch 60/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1243 - val_loss: 0.1143\n",
      "Epoch 61/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1243 - val_loss: 0.1121\n",
      "Epoch 62/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1236 - val_loss: 0.1122\n",
      "Epoch 63/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1243 - val_loss: 0.1075\n",
      "Epoch 64/75\n",
      "159/159 [==============================] - 15s 92ms/step - loss: 0.1237 - val_loss: 0.1149\n",
      "Epoch 65/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1238 - val_loss: 0.1174\n",
      "Epoch 66/75\n",
      "159/159 [==============================] - 15s 92ms/step - loss: 0.1231 - val_loss: 0.1070\n",
      "Epoch 67/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1233 - val_loss: 0.1118\n",
      "Epoch 68/75\n",
      "159/159 [==============================] - 15s 92ms/step - loss: 0.1233 - val_loss: 0.1091\n",
      "Epoch 69/75\n",
      "159/159 [==============================] - 14s 90ms/step - loss: 0.1238 - val_loss: 0.1125\n",
      "Epoch 70/75\n",
      "159/159 [==============================] - 15s 92ms/step - loss: 0.1230 - val_loss: 0.1074\n",
      "Epoch 71/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1230 - val_loss: 0.1077\n",
      "Epoch 72/75\n",
      "159/159 [==============================] - 15s 92ms/step - loss: 0.1233 - val_loss: 0.1131\n",
      "Epoch 73/75\n",
      "159/159 [==============================] - 15s 93ms/step - loss: 0.1223 - val_loss: 0.1170\n",
      "Epoch 74/75\n",
      "159/159 [==============================] - 15s 94ms/step - loss: 0.1229 - val_loss: 0.1097\n",
      "Epoch 75/75\n",
      "159/159 [==============================] - 14s 91ms/step - loss: 0.1226 - val_loss: 0.1098\n",
      "144/144 [==============================] - 2s 16ms/step\n",
      "2.771233368115483\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "# Train a model for a single site\n",
    "filename = '7871.csv'\n",
    "X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename)\n",
    "\n",
    "model = define_model()\n",
    "model.fit(X_train, y_train, epochs=75, validation_data=(X_test, y_test), batch_size=128)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print(mean_absolute_error(y_test * test_norms[2], predictions * train_norms[2]))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "id": "418e8571b6a50426",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-18T04:35:57.123696Z"
    },
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
=======
   "execution_count": 133,
   "id": "418e8571b6a50426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T21:48:21.749287Z",
     "start_time": "2024-08-22T21:42:26.166374Z"
    },
    "is_executing": true
>>>>>>> Stashed changes
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on site number 1 of 100\n",
<<<<<<< Updated upstream
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.3325 - val_loss: 0.1971\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.1400 - val_loss: 0.1664\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.1214 - val_loss: 0.1736\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.1156 - val_loss: 0.1599\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.1120 - val_loss: 0.1579\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.1095 - val_loss: 0.1486\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.1080 - val_loss: 0.1463\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.1066 - val_loss: 0.1513\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.1060 - val_loss: 0.1478\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.1053 - val_loss: 0.1485\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.1047 - val_loss: 0.1463\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.1053 - val_loss: 0.1476\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1040 - val_loss: 0.1503\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1034 - val_loss: 0.1486\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1034 - val_loss: 0.1442\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1034 - val_loss: 0.1530\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1018 - val_loss: 0.1536\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1019 - val_loss: 0.1467\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1031 - val_loss: 0.1493\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1025 - val_loss: 0.1557\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1019 - val_loss: 0.1593\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1023 - val_loss: 0.1494\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1013 - val_loss: 0.1507\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1007 - val_loss: 0.1463\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1005 - val_loss: 0.1446\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1007 - val_loss: 0.1536\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0996 - val_loss: 0.1548\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0986 - val_loss: 0.1488\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0991 - val_loss: 0.1534\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1003 - val_loss: 0.1516\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 2 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.2800 - val_loss: 0.2594\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0972 - val_loss: 0.2090\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0796 - val_loss: 0.2027\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0725 - val_loss: 0.2230\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0704 - val_loss: 0.2104\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0685 - val_loss: 0.2106\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0672 - val_loss: 0.2076\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0671 - val_loss: 0.1894\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0666 - val_loss: 0.2008\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0650 - val_loss: 0.2013\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0656 - val_loss: 0.1958\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0648 - val_loss: 0.1973\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0653 - val_loss: 0.2020\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0645 - val_loss: 0.2016\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0646 - val_loss: 0.2004\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0647 - val_loss: 0.1832\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0637 - val_loss: 0.1946\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0635 - val_loss: 0.2136\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0641 - val_loss: 0.2058\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0637 - val_loss: 0.2119\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0634 - val_loss: 0.2154\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0642 - val_loss: 0.1931\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0631 - val_loss: 0.2051\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0633 - val_loss: 0.1978\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0632 - val_loss: 0.1967\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0636 - val_loss: 0.2097\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0632 - val_loss: 0.2101\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0630 - val_loss: 0.1955\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0629 - val_loss: 0.2000\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0625 - val_loss: 0.2004\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 3 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - loss: 0.3531 - val_loss: 0.1875\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1601 - val_loss: 0.1531\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1392 - val_loss: 0.1420\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1331 - val_loss: 0.1399\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1290 - val_loss: 0.1410\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1267 - val_loss: 0.1343\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1263 - val_loss: 0.1321\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1232 - val_loss: 0.1326\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1226 - val_loss: 0.1359\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1220 - val_loss: 0.1341\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1230 - val_loss: 0.1313\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1212 - val_loss: 0.1342\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1216 - val_loss: 0.1319\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1200 - val_loss: 0.1332\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1195 - val_loss: 0.1307\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1199 - val_loss: 0.1323\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1180 - val_loss: 0.1332\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1187 - val_loss: 0.1337\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1172 - val_loss: 0.1338\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1159 - val_loss: 0.1332\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1181 - val_loss: 0.1340\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1178 - val_loss: 0.1345\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1160 - val_loss: 0.1362\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1152 - val_loss: 0.1404\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1167 - val_loss: 0.1331\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1153 - val_loss: 0.1355\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1158 - val_loss: 0.1370\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1144 - val_loss: 0.1355\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1154 - val_loss: 0.1346\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1148 - val_loss: 0.1343\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 4 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.3350 - val_loss: 0.1823\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1503 - val_loss: 0.1489\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1300 - val_loss: 0.1404\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1228 - val_loss: 0.1377\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1203 - val_loss: 0.1416\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1186 - val_loss: 0.1353\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1174 - val_loss: 0.1373\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1169 - val_loss: 0.1358\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1172 - val_loss: 0.1354\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1168 - val_loss: 0.1340\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1151 - val_loss: 0.1351\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1156 - val_loss: 0.1369\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1159 - val_loss: 0.1338\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1151 - val_loss: 0.1360\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1150 - val_loss: 0.1347\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1149 - val_loss: 0.1359\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1136 - val_loss: 0.1362\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1142 - val_loss: 0.1363\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1141 - val_loss: 0.1357\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1130 - val_loss: 0.1363\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1118 - val_loss: 0.1367\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1126 - val_loss: 0.1365\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1115 - val_loss: 0.1370\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1121 - val_loss: 0.1379\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1112 - val_loss: 0.1401\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1110 - val_loss: 0.1366\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1119 - val_loss: 0.1379\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1095 - val_loss: 0.1390\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1094 - val_loss: 0.1401\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1085 - val_loss: 0.1406\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 5 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.3538 - val_loss: 0.1936\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1598 - val_loss: 0.1532\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1375 - val_loss: 0.1432\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1322 - val_loss: 0.1417\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1280 - val_loss: 0.1388\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1257 - val_loss: 0.1411\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1254 - val_loss: 0.1357\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1239 - val_loss: 0.1339\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1210 - val_loss: 0.1410\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1219 - val_loss: 0.1319\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1214 - val_loss: 0.1318\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1206 - val_loss: 0.1329\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1184 - val_loss: 0.1379\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1204 - val_loss: 0.1325\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1211 - val_loss: 0.1347\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1174 - val_loss: 0.1330\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1189 - val_loss: 0.1354\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1184 - val_loss: 0.1355\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1177 - val_loss: 0.1373\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1174 - val_loss: 0.1355\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1172 - val_loss: 0.1340\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1167 - val_loss: 0.1346\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1167 - val_loss: 0.1381\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1156 - val_loss: 0.1332\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1159 - val_loss: 0.1339\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1157 - val_loss: 0.1347\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1143 - val_loss: 0.1361\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1144 - val_loss: 0.1350\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1140 - val_loss: 0.1395\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1155 - val_loss: 0.1411\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 6 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.3530 - val_loss: 0.1563\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1620 - val_loss: 0.1211\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1381 - val_loss: 0.1087\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1269 - val_loss: 0.1068\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1241 - val_loss: 0.1033\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1215 - val_loss: 0.1012\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1196 - val_loss: 0.1092\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1178 - val_loss: 0.0994\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1160 - val_loss: 0.0977\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1168 - val_loss: 0.0961\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1154 - val_loss: 0.1013\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1154 - val_loss: 0.0970\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1141 - val_loss: 0.0975\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1136 - val_loss: 0.0953\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1118 - val_loss: 0.0948\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1115 - val_loss: 0.0967\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1117 - val_loss: 0.0947\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1113 - val_loss: 0.0952\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1108 - val_loss: 0.0967\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1115 - val_loss: 0.0973\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1109 - val_loss: 0.0968\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1106 - val_loss: 0.0975\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1103 - val_loss: 0.0964\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1083 - val_loss: 0.0967\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1079 - val_loss: 0.0980\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1083 - val_loss: 0.0982\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1089 - val_loss: 0.0984\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1068 - val_loss: 0.0984\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1069 - val_loss: 0.0988\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1071 - val_loss: 0.1012\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 7 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.2999 - val_loss: 0.1960\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1232 - val_loss: 0.1541\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1034 - val_loss: 0.1499\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0984 - val_loss: 0.1549\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0953 - val_loss: 0.1489\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0934 - val_loss: 0.1462\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0938 - val_loss: 0.1378\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0926 - val_loss: 0.1461\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0927 - val_loss: 0.1421\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0920 - val_loss: 0.1449\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0912 - val_loss: 0.1335\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0917 - val_loss: 0.1368\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0926 - val_loss: 0.1358\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0910 - val_loss: 0.1329\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0908 - val_loss: 0.1392\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0904 - val_loss: 0.1349\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0891 - val_loss: 0.1416\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0910 - val_loss: 0.1309\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0903 - val_loss: 0.1336\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0901 - val_loss: 0.1342\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0885 - val_loss: 0.1491\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0886 - val_loss: 0.1378\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0873 - val_loss: 0.1389\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0882 - val_loss: 0.1376\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0868 - val_loss: 0.1374\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0868 - val_loss: 0.1343\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0873 - val_loss: 0.1482\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0864 - val_loss: 0.1359\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0856 - val_loss: 0.1391\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0860 - val_loss: 0.1480\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 8 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.3257 - val_loss: 0.1625\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.1389 - val_loss: 0.1367\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1206 - val_loss: 0.1317\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1161 - val_loss: 0.1291\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1144 - val_loss: 0.1288\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1124 - val_loss: 0.1276\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1121 - val_loss: 0.1282\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1112 - val_loss: 0.1266\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1126 - val_loss: 0.1266\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1109 - val_loss: 0.1265\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1111 - val_loss: 0.1296\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1107 - val_loss: 0.1267\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1098 - val_loss: 0.1265\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1107 - val_loss: 0.1279\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1107 - val_loss: 0.1277\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1090 - val_loss: 0.1291\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1107 - val_loss: 0.1257\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1091 - val_loss: 0.1266\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1094 - val_loss: 0.1252\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1084 - val_loss: 0.1255\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1086 - val_loss: 0.1269\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1072 - val_loss: 0.1246\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1075 - val_loss: 0.1260\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1073 - val_loss: 0.1250\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1066 - val_loss: 0.1257\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1050 - val_loss: 0.1243\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1046 - val_loss: 0.1281\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1048 - val_loss: 0.1273\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1047 - val_loss: 0.1269\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1028 - val_loss: 0.1289\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 9 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.2684 - val_loss: 0.2000\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0887 - val_loss: 0.1556\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0707 - val_loss: 0.1489\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0642 - val_loss: 0.1641\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0616 - val_loss: 0.1473\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0601 - val_loss: 0.1333\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0590 - val_loss: 0.1302\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0579 - val_loss: 0.1329\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0584 - val_loss: 0.1405\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0572 - val_loss: 0.1366\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0565 - val_loss: 0.1326\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0563 - val_loss: 0.1436\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0557 - val_loss: 0.1352\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0561 - val_loss: 0.1415\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0560 - val_loss: 0.1389\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0558 - val_loss: 0.1364\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0550 - val_loss: 0.1477\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0550 - val_loss: 0.1303\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0560 - val_loss: 0.1371\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0554 - val_loss: 0.1369\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0554 - val_loss: 0.1276\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0545 - val_loss: 0.1295\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0546 - val_loss: 0.1349\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0545 - val_loss: 0.1251\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0554 - val_loss: 0.1480\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0552 - val_loss: 0.1430\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0539 - val_loss: 0.1289\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0550 - val_loss: 0.1361\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0538 - val_loss: 0.1415\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0548 - val_loss: 0.1401\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 10 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.3270 - val_loss: 0.1638\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1451 - val_loss: 0.1352\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1241 - val_loss: 0.1288\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1169 - val_loss: 0.1257\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1155 - val_loss: 0.1250\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1139 - val_loss: 0.1243\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1140 - val_loss: 0.1240\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1122 - val_loss: 0.1241\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1090 - val_loss: 0.1247\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1103 - val_loss: 0.1238\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1091 - val_loss: 0.1255\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1074 - val_loss: 0.1263\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1081 - val_loss: 0.1268\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1087 - val_loss: 0.1257\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1067 - val_loss: 0.1272\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1063 - val_loss: 0.1251\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1051 - val_loss: 0.1251\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1055 - val_loss: 0.1244\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1040 - val_loss: 0.1256\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1042 - val_loss: 0.1265\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1047 - val_loss: 0.1250\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1035 - val_loss: 0.1266\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1028 - val_loss: 0.1264\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1039 - val_loss: 0.1265\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1022 - val_loss: 0.1260\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1035 - val_loss: 0.1265\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1031 - val_loss: 0.1272\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1000 - val_loss: 0.1317\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0997 - val_loss: 0.1282\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1001 - val_loss: 0.1289\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 11 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.2959 - val_loss: 0.2495\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1144 - val_loss: 0.1900\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0921 - val_loss: 0.1892\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0840 - val_loss: 0.1734\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0809 - val_loss: 0.1736\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0792 - val_loss: 0.1766\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0778 - val_loss: 0.1711\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0767 - val_loss: 0.1693\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0765 - val_loss: 0.1741\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0757 - val_loss: 0.1651\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0753 - val_loss: 0.1758\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0746 - val_loss: 0.1746\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0745 - val_loss: 0.1601\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0742 - val_loss: 0.1609\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0739 - val_loss: 0.1729\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0735 - val_loss: 0.1645\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0743 - val_loss: 0.1709\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0739 - val_loss: 0.1636\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0738 - val_loss: 0.1760\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0725 - val_loss: 0.1571\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0739 - val_loss: 0.1672\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0734 - val_loss: 0.1755\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0727 - val_loss: 0.1709\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0723 - val_loss: 0.1783\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0725 - val_loss: 0.1622\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0728 - val_loss: 0.1687\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0719 - val_loss: 0.1725\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0724 - val_loss: 0.1582\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0724 - val_loss: 0.1731\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0716 - val_loss: 0.1831\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 12 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.3143 - val_loss: 0.2151\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1371 - val_loss: 0.1694\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1098 - val_loss: 0.1567\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1026 - val_loss: 0.1596\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0969 - val_loss: 0.1388\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0949 - val_loss: 0.1367\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0940 - val_loss: 0.1439\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0924 - val_loss: 0.1347\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0916 - val_loss: 0.1517\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0915 - val_loss: 0.1471\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0913 - val_loss: 0.1300\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0913 - val_loss: 0.1382\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0899 - val_loss: 0.1462\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0900 - val_loss: 0.1246\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0901 - val_loss: 0.1393\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0903 - val_loss: 0.1397\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0897 - val_loss: 0.1467\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0894 - val_loss: 0.1359\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0890 - val_loss: 0.1404\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0898 - val_loss: 0.1364\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0879 - val_loss: 0.1392\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0889 - val_loss: 0.1337\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0884 - val_loss: 0.1285\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0878 - val_loss: 0.1325\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0886 - val_loss: 0.1430\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0872 - val_loss: 0.1370\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0875 - val_loss: 0.1465\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0878 - val_loss: 0.1397\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0881 - val_loss: 0.1375\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0881 - val_loss: 0.1348\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 13 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.3326 - val_loss: 0.1728\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1496 - val_loss: 0.1447\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1298 - val_loss: 0.1406\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1238 - val_loss: 0.1346\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1206 - val_loss: 0.1335\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1196 - val_loss: 0.1327\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1179 - val_loss: 0.1333\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1175 - val_loss: 0.1330\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1162 - val_loss: 0.1314\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1172 - val_loss: 0.1314\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1176 - val_loss: 0.1312\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1162 - val_loss: 0.1323\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1168 - val_loss: 0.1313\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1158 - val_loss: 0.1312\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1158 - val_loss: 0.1307\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1153 - val_loss: 0.1324\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1157 - val_loss: 0.1329\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1160 - val_loss: 0.1308\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1140 - val_loss: 0.1315\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1146 - val_loss: 0.1330\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1140 - val_loss: 0.1325\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1151 - val_loss: 0.1324\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1141 - val_loss: 0.1327\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1132 - val_loss: 0.1340\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1131 - val_loss: 0.1315\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1121 - val_loss: 0.1325\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1132 - val_loss: 0.1326\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1123 - val_loss: 0.1310\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1107 - val_loss: 0.1325\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1101 - val_loss: 0.1338\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 14 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.3388 - val_loss: 0.1581\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1503 - val_loss: 0.1266\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1278 - val_loss: 0.1204\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1208 - val_loss: 0.1171\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1180 - val_loss: 0.1178\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1173 - val_loss: 0.1145\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1161 - val_loss: 0.1146\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1156 - val_loss: 0.1149\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1134 - val_loss: 0.1153\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1131 - val_loss: 0.1152\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1133 - val_loss: 0.1148\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1127 - val_loss: 0.1164\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1118 - val_loss: 0.1172\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1135 - val_loss: 0.1157\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1129 - val_loss: 0.1151\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1101 - val_loss: 0.1163\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1115 - val_loss: 0.1160\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1106 - val_loss: 0.1169\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1101 - val_loss: 0.1167\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1086 - val_loss: 0.1160\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1090 - val_loss: 0.1172\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1074 - val_loss: 0.1167\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1074 - val_loss: 0.1147\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1062 - val_loss: 0.1196\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1065 - val_loss: 0.1180\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1063 - val_loss: 0.1171\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1060 - val_loss: 0.1185\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1043 - val_loss: 0.1202\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1050 - val_loss: 0.1164\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1036 - val_loss: 0.1173\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 15 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.3464 - val_loss: 0.1769\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1589 - val_loss: 0.1425\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1361 - val_loss: 0.1325\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1279 - val_loss: 0.1277\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1238 - val_loss: 0.1262\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1235 - val_loss: 0.1239\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1212 - val_loss: 0.1207\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1205 - val_loss: 0.1223\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1198 - val_loss: 0.1228\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1188 - val_loss: 0.1196\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1191 - val_loss: 0.1223\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1182 - val_loss: 0.1201\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1184 - val_loss: 0.1226\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1164 - val_loss: 0.1192\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1149 - val_loss: 0.1270\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1156 - val_loss: 0.1189\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1147 - val_loss: 0.1200\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1133 - val_loss: 0.1203\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1142 - val_loss: 0.1208\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1143 - val_loss: 0.1203\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1139 - val_loss: 0.1212\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1132 - val_loss: 0.1212\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1121 - val_loss: 0.1213\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1115 - val_loss: 0.1232\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1116 - val_loss: 0.1243\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1111 - val_loss: 0.1265\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1118 - val_loss: 0.1224\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1106 - val_loss: 0.1261\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1099 - val_loss: 0.1240\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1092 - val_loss: 0.1231\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 16 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.3241 - val_loss: 0.1634\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.1382 - val_loss: 0.1384\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1203 - val_loss: 0.1332\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1153 - val_loss: 0.1307\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1138 - val_loss: 0.1305\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1123 - val_loss: 0.1296\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1116 - val_loss: 0.1293\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1111 - val_loss: 0.1294\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1105 - val_loss: 0.1292\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1109 - val_loss: 0.1297\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1114 - val_loss: 0.1285\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1100 - val_loss: 0.1296\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1101 - val_loss: 0.1289\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1113 - val_loss: 0.1286\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1091 - val_loss: 0.1289\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1098 - val_loss: 0.1285\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1074 - val_loss: 0.1287\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1091 - val_loss: 0.1303\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1096 - val_loss: 0.1287\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1078 - val_loss: 0.1281\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1076 - val_loss: 0.1302\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1078 - val_loss: 0.1338\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1070 - val_loss: 0.1294\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1062 - val_loss: 0.1284\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1048 - val_loss: 0.1301\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1059 - val_loss: 0.1285\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1053 - val_loss: 0.1299\n",
      "Epoch 28/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1055 - val_loss: 0.1317\n",
      "Epoch 29/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1063 - val_loss: 0.1292\n",
      "Epoch 30/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1049 - val_loss: 0.1296\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training on site number 17 of 100\n",
      "Epoch 1/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.3442 - val_loss: 0.1696\n",
      "Epoch 2/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1493 - val_loss: 0.1398\n",
      "Epoch 3/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1272 - val_loss: 0.1325\n",
      "Epoch 4/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1219 - val_loss: 0.1295\n",
      "Epoch 5/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1195 - val_loss: 0.1294\n",
      "Epoch 6/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1181 - val_loss: 0.1257\n",
      "Epoch 7/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1161 - val_loss: 0.1253\n",
      "Epoch 8/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1165 - val_loss: 0.1249\n",
      "Epoch 9/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1149 - val_loss: 0.1239\n",
      "Epoch 10/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1152 - val_loss: 0.1219\n",
      "Epoch 11/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1138 - val_loss: 0.1217\n",
      "Epoch 12/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1133 - val_loss: 0.1214\n",
      "Epoch 13/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1126 - val_loss: 0.1251\n",
      "Epoch 14/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1131 - val_loss: 0.1219\n",
      "Epoch 15/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1113 - val_loss: 0.1221\n",
      "Epoch 16/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1103 - val_loss: 0.1216\n",
      "Epoch 17/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1099 - val_loss: 0.1211\n",
      "Epoch 18/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1118 - val_loss: 0.1219\n",
      "Epoch 19/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1097 - val_loss: 0.1210\n",
      "Epoch 20/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1107 - val_loss: 0.1233\n",
      "Epoch 21/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1087 - val_loss: 0.1222\n",
      "Epoch 22/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1102 - val_loss: 0.1218\n",
      "Epoch 23/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1093 - val_loss: 0.1223\n",
      "Epoch 24/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.1096 - val_loss: 0.1257\n",
      "Epoch 25/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1071 - val_loss: 0.1244\n",
      "Epoch 26/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1072 - val_loss: 0.1248\n",
      "Epoch 27/30\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.1070 - val_loss: 0.1228\n",
      "Epoch 28/30\n",
      "\u001b[1m110/163\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1068"
=======
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.3602 - val_loss: 0.3251\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.3004 - val_loss: 0.3190\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.2976 - val_loss: 0.3177\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.2968 - val_loss: 0.3172\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.2964 - val_loss: 0.3170\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.2962 - val_loss: 0.3168\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.2961 - val_loss: 0.3167\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.2960 - val_loss: 0.3166\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.2960 - val_loss: 0.3166\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.2959 - val_loss: 0.3166\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.2959 - val_loss: 0.3165\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.2959 - val_loss: 0.3165\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.2959 - val_loss: 0.3165\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.2959 - val_loss: 0.3165\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.2959 - val_loss: 0.3165\n",
      "156/156 [==============================] - 1s 8ms/step\n",
      "Training on site number 2 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1961 - val_loss: 0.1454\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1175 - val_loss: 0.1320\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1117 - val_loss: 0.1259\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1085 - val_loss: 0.1248\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1068 - val_loss: 0.1228\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1060 - val_loss: 0.1317\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1054 - val_loss: 0.1226\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1047 - val_loss: 0.1243\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1045 - val_loss: 0.1232\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1047 - val_loss: 0.1283\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1042 - val_loss: 0.1213\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1039 - val_loss: 0.1229\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1038 - val_loss: 0.1280\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1039 - val_loss: 0.1240\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1040 - val_loss: 0.1242\n",
      "156/156 [==============================] - 1s 8ms/step\n",
      "Training on site number 3 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.2116 - val_loss: 0.1605\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1315 - val_loss: 0.1452\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1226 - val_loss: 0.1477\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1199 - val_loss: 0.1383\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 46ms/step - loss: 0.1191 - val_loss: 0.1391\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 46ms/step - loss: 0.1182 - val_loss: 0.1478\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 46ms/step - loss: 0.1177 - val_loss: 0.1439\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 46ms/step - loss: 0.1174 - val_loss: 0.1406\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 46ms/step - loss: 0.1172 - val_loss: 0.1368\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 46ms/step - loss: 0.1173 - val_loss: 0.1427\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1169 - val_loss: 0.1395\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 46ms/step - loss: 0.1168 - val_loss: 0.1407\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1167 - val_loss: 0.1413\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1169 - val_loss: 0.1411\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1165 - val_loss: 0.1409\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 4 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3653 - val_loss: 0.3354\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3055 - val_loss: 0.3293\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3027 - val_loss: 0.3281\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3020 - val_loss: 0.3277\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3018 - val_loss: 0.3275\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3017 - val_loss: 0.3275\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3016 - val_loss: 0.3275\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3016 - val_loss: 0.3275\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3016 - val_loss: 0.3275\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3016 - val_loss: 0.3275\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3016 - val_loss: 0.3275\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3016 - val_loss: 0.3275\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3016 - val_loss: 0.3275\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3016 - val_loss: 0.3275\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3016 - val_loss: 0.3275\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 5 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2532 - val_loss: 0.2084\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1936 - val_loss: 0.2023\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1908 - val_loss: 0.2012\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1901 - val_loss: 0.2008\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1899 - val_loss: 0.2007\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1898 - val_loss: 0.2006\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1898 - val_loss: 0.2006\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1897 - val_loss: 0.2006\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1897 - val_loss: 0.2006\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1897 - val_loss: 0.2006\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1897 - val_loss: 0.2006\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1897 - val_loss: 0.2006\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1897 - val_loss: 0.2006\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1897 - val_loss: 0.2006\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1897 - val_loss: 0.2006\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 6 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.3156 - val_loss: 0.3006\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.2551 - val_loss: 0.2939\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.2518 - val_loss: 0.2923\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2508 - val_loss: 0.2917\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.2503 - val_loss: 0.2913\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.2500 - val_loss: 0.2911\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.2499 - val_loss: 0.2910\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.2498 - val_loss: 0.2909\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.2497 - val_loss: 0.2909\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2497 - val_loss: 0.2909\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2497 - val_loss: 0.2909\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2497 - val_loss: 0.2908\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2497 - val_loss: 0.2908\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.2497 - val_loss: 0.2908\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.2497 - val_loss: 0.2908\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 7 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.3664 - val_loss: 0.3048\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.3077 - val_loss: 0.2994\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.3054 - val_loss: 0.2986\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.3050 - val_loss: 0.2985\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.3049 - val_loss: 0.2984\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 450s 3s/step - loss: 0.3049 - val_loss: 0.2984\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 46ms/step - loss: 0.3049 - val_loss: 0.2984\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 8s 49ms/step - loss: 0.3049 - val_loss: 0.2984\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.3049 - val_loss: 0.2984\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.3049 - val_loss: 0.2984\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.3049 - val_loss: 0.2984\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.3049 - val_loss: 0.2984\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.3049 - val_loss: 0.2984\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.3049 - val_loss: 0.2984\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.3049 - val_loss: 0.2984\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 8 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.3164 - val_loss: 0.1523\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1285 - val_loss: 0.1446\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1247 - val_loss: 0.1418\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1230 - val_loss: 0.1423\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1222 - val_loss: 0.1402\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1214 - val_loss: 0.1404\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1207 - val_loss: 0.1403\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1205 - val_loss: 0.1393\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1201 - val_loss: 0.1388\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1199 - val_loss: 0.1389\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1196 - val_loss: 0.1379\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1195 - val_loss: 0.1394\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1193 - val_loss: 0.1388\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1192 - val_loss: 0.1380\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1191 - val_loss: 0.1380\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 9 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2270 - val_loss: 0.1706\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1416 - val_loss: 0.1514\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1305 - val_loss: 0.1465\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1264 - val_loss: 0.1438\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1243 - val_loss: 0.1421\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1232 - val_loss: 0.1405\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1227 - val_loss: 0.1405\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1225 - val_loss: 0.1397\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1221 - val_loss: 0.1402\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1219 - val_loss: 0.1390\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1217 - val_loss: 0.1388\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1218 - val_loss: 0.1387\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1216 - val_loss: 0.1391\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1215 - val_loss: 0.1389\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1214 - val_loss: 0.1397\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 10 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.3552 - val_loss: 0.3789\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2958 - val_loss: 0.3732\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.2933 - val_loss: 0.3722\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.2927 - val_loss: 0.3719\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.2925 - val_loss: 0.3718\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.2924 - val_loss: 0.3717\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.2924 - val_loss: 0.3717\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.2924 - val_loss: 0.3717\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.2924 - val_loss: 0.3717\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.2924 - val_loss: 0.3717\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.2924 - val_loss: 0.3717\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.2924 - val_loss: 0.3717\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.2924 - val_loss: 0.3717\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.2924 - val_loss: 0.3717\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 8s 47ms/step - loss: 0.2924 - val_loss: 0.3717\n",
      "156/156 [==============================] - 1s 8ms/step\n",
      "Training on site number 11 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 8s 48ms/step - loss: 0.2220 - val_loss: 0.1527\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 8s 48ms/step - loss: 0.1360 - val_loss: 0.1366\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 8s 47ms/step - loss: 0.1239 - val_loss: 0.1293\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 8s 47ms/step - loss: 0.1189 - val_loss: 0.1251\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 8s 47ms/step - loss: 0.1171 - val_loss: 0.1235\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 8s 47ms/step - loss: 0.1159 - val_loss: 0.1230\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 8s 47ms/step - loss: 0.1151 - val_loss: 0.1225\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 8s 47ms/step - loss: 0.1141 - val_loss: 0.1228\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 46ms/step - loss: 0.1141 - val_loss: 0.1232\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 8s 47ms/step - loss: 0.1132 - val_loss: 0.1240\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 8s 47ms/step - loss: 0.1131 - val_loss: 0.1222\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1127 - val_loss: 0.1227\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1128 - val_loss: 0.1215\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1131 - val_loss: 0.1281\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1133 - val_loss: 0.1210\n",
      "156/156 [==============================] - 1s 8ms/step\n",
      "Training on site number 12 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 8s 45ms/step - loss: 0.2069 - val_loss: 0.1486\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1221 - val_loss: 0.1353\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1135 - val_loss: 0.1369\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1106 - val_loss: 0.1257\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1089 - val_loss: 0.1288\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1082 - val_loss: 0.1284\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1074 - val_loss: 0.1275\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1069 - val_loss: 0.1318\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1069 - val_loss: 0.1270\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1064 - val_loss: 0.1246\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1067 - val_loss: 0.1265\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1066 - val_loss: 0.1247\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1064 - val_loss: 0.1251\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1064 - val_loss: 0.1362\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1060 - val_loss: 0.1278\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 13 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.2139 - val_loss: 0.1457\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1278 - val_loss: 0.1306\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1198 - val_loss: 0.1250\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1172 - val_loss: 0.1245\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1155 - val_loss: 0.1224\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1147 - val_loss: 0.1214\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1139 - val_loss: 0.1210\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1134 - val_loss: 0.1220\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1133 - val_loss: 0.1204\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1129 - val_loss: 0.1211\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1126 - val_loss: 0.1202\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1126 - val_loss: 0.1201\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1125 - val_loss: 0.1201\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1122 - val_loss: 0.1202\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1121 - val_loss: 0.1200\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 14 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 37ms/step - loss: 0.2147 - val_loss: 0.1580\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1314 - val_loss: 0.1411\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1217 - val_loss: 0.1346\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 35ms/step - loss: 0.1180 - val_loss: 0.1348\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1165 - val_loss: 0.1327\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1150 - val_loss: 0.1316\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1142 - val_loss: 0.1324\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1136 - val_loss: 0.1308\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1129 - val_loss: 0.1307\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 35ms/step - loss: 0.1130 - val_loss: 0.1309\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1126 - val_loss: 0.1298\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1129 - val_loss: 0.1310\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1126 - val_loss: 0.1318\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1123 - val_loss: 0.1315\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1120 - val_loss: 0.1310\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 15 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2257 - val_loss: 0.1550\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1434 - val_loss: 0.1364\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1314 - val_loss: 0.1247\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1266 - val_loss: 0.1215\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1244 - val_loss: 0.1199\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1233 - val_loss: 0.1208\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1227 - val_loss: 0.1189\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1221 - val_loss: 0.1194\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1218 - val_loss: 0.1201\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1216 - val_loss: 0.1195\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1223 - val_loss: 0.1189\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1212 - val_loss: 0.1196\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1211 - val_loss: 0.1195\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1205 - val_loss: 0.1180\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1205 - val_loss: 0.1212\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 16 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.3621 - val_loss: 0.3622\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3020 - val_loss: 0.3559\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2991 - val_loss: 0.3546\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2982 - val_loss: 0.3540\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2978 - val_loss: 0.3538\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2977 - val_loss: 0.3537\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2976 - val_loss: 0.3536\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2975 - val_loss: 0.3536\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2975 - val_loss: 0.3536\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2975 - val_loss: 0.3536\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2975 - val_loss: 0.3536\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2975 - val_loss: 0.3536\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2975 - val_loss: 0.3536\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2975 - val_loss: 0.3536\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2975 - val_loss: 0.3536\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 17 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2029 - val_loss: 0.1711\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1193 - val_loss: 0.1502\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1071 - val_loss: 0.1495\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1023 - val_loss: 0.1458\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1000 - val_loss: 0.1420\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.0987 - val_loss: 0.1411\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.0982 - val_loss: 0.1392\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.0976 - val_loss: 0.1430\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.0973 - val_loss: 0.1405\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.0969 - val_loss: 0.1421\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.0966 - val_loss: 0.1397\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.0966 - val_loss: 0.1395\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.0964 - val_loss: 0.1422\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.0960 - val_loss: 0.1419\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.0958 - val_loss: 0.1373\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 18 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2102 - val_loss: 0.1544\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1314 - val_loss: 0.1392\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1252 - val_loss: 0.1356\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1220 - val_loss: 0.1348\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1200 - val_loss: 0.1330\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1188 - val_loss: 0.1356\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1182 - val_loss: 0.1325\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1177 - val_loss: 0.1319\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1170 - val_loss: 0.1320\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1170 - val_loss: 0.1315\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1169 - val_loss: 0.1313\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1167 - val_loss: 0.1312\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1163 - val_loss: 0.1331\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1160 - val_loss: 0.1310\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1159 - val_loss: 0.1337\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 19 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2178 - val_loss: 0.1513\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1334 - val_loss: 0.1275\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1236 - val_loss: 0.1219\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1195 - val_loss: 0.1199\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1179 - val_loss: 0.1188\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1161 - val_loss: 0.1224\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1153 - val_loss: 0.1166\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1146 - val_loss: 0.1161\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1139 - val_loss: 0.1158\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1137 - val_loss: 0.1161\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1136 - val_loss: 0.1154\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1132 - val_loss: 0.1154\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1138 - val_loss: 0.1158\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1130 - val_loss: 0.1181\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1130 - val_loss: 0.1150\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 20 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2213 - val_loss: 0.1670\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1338 - val_loss: 0.1469\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1213 - val_loss: 0.1471\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1165 - val_loss: 0.1362\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1147 - val_loss: 0.1360\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1133 - val_loss: 0.1371\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1130 - val_loss: 0.1361\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1123 - val_loss: 0.1353\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1124 - val_loss: 0.1370\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1123 - val_loss: 0.1304\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1119 - val_loss: 0.1342\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1117 - val_loss: 0.1352\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1116 - val_loss: 0.1403\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1115 - val_loss: 0.1333\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1113 - val_loss: 0.1333\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 21 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2147 - val_loss: 0.1551\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1334 - val_loss: 0.1288\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1247 - val_loss: 0.1257\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1214 - val_loss: 0.1224\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1197 - val_loss: 0.1215\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1187 - val_loss: 0.1260\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1177 - val_loss: 0.1207\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1177 - val_loss: 0.1201\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1175 - val_loss: 0.1217\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1171 - val_loss: 0.1196\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1167 - val_loss: 0.1205\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1167 - val_loss: 0.1268\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1168 - val_loss: 0.1191\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1164 - val_loss: 0.1187\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1162 - val_loss: 0.1203\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 22 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2104 - val_loss: 0.1438\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1279 - val_loss: 0.1289\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1192 - val_loss: 0.1248\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1161 - val_loss: 0.1233\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1150 - val_loss: 0.1228\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1146 - val_loss: 0.1235\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1138 - val_loss: 0.1227\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1133 - val_loss: 0.1213\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1130 - val_loss: 0.1215\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1130 - val_loss: 0.1238\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1130 - val_loss: 0.1212\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1124 - val_loss: 0.1215\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1122 - val_loss: 0.1205\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1126 - val_loss: 0.1206\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1118 - val_loss: 0.1209\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 23 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3569 - val_loss: 0.3241\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2970 - val_loss: 0.3178\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2940 - val_loss: 0.3164\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2932 - val_loss: 0.3159\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2928 - val_loss: 0.3156\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2926 - val_loss: 0.3154\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2924 - val_loss: 0.3153\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2923 - val_loss: 0.3153\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2923 - val_loss: 0.3152\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2923 - val_loss: 0.3152\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2922 - val_loss: 0.3152\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2922 - val_loss: 0.3152\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2922 - val_loss: 0.3152\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2922 - val_loss: 0.3152\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2922 - val_loss: 0.3152\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 24 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2311 - val_loss: 0.1588\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1455 - val_loss: 0.1374\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1318 - val_loss: 0.1340\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1272 - val_loss: 0.1274\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1256 - val_loss: 0.1351\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1246 - val_loss: 0.1262\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1236 - val_loss: 0.1261\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1229 - val_loss: 0.1242\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1231 - val_loss: 0.1241\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1220 - val_loss: 0.1257\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1217 - val_loss: 0.1241\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1216 - val_loss: 0.1245\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1215 - val_loss: 0.1235\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1213 - val_loss: 0.1250\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1211 - val_loss: 0.1243\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 25 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2288 - val_loss: 0.1663\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1447 - val_loss: 0.1461\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1341 - val_loss: 0.1435\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1306 - val_loss: 0.1432\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1290 - val_loss: 0.1379\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1279 - val_loss: 0.1393\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1272 - val_loss: 0.1381\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1268 - val_loss: 0.1369\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1264 - val_loss: 0.1372\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1264 - val_loss: 0.1376\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1264 - val_loss: 0.1361\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1259 - val_loss: 0.1375\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1258 - val_loss: 0.1360\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1254 - val_loss: 0.1353\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1255 - val_loss: 0.1358\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 26 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2032 - val_loss: 0.1528\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1226 - val_loss: 0.1344\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1137 - val_loss: 0.1263\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1111 - val_loss: 0.1267\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1093 - val_loss: 0.1243\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1089 - val_loss: 0.1229\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1088 - val_loss: 0.1220\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1085 - val_loss: 0.1230\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1080 - val_loss: 0.1261\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 35ms/step - loss: 0.1079 - val_loss: 0.1233\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 35ms/step - loss: 0.1083 - val_loss: 0.1260\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1078 - val_loss: 0.1217\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1078 - val_loss: 0.1258\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1078 - val_loss: 0.1230\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1079 - val_loss: 0.1266\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 27 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3724 - val_loss: 0.3417\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3128 - val_loss: 0.3355\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3098 - val_loss: 0.3341\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3088 - val_loss: 0.3334\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3083 - val_loss: 0.3330\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3080 - val_loss: 0.3328\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3078 - val_loss: 0.3326\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3076 - val_loss: 0.3325\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3076 - val_loss: 0.3324\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3075 - val_loss: 0.3324\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3075 - val_loss: 0.3324\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3074 - val_loss: 0.3323\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3074 - val_loss: 0.3323\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3074 - val_loss: 0.3323\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3074 - val_loss: 0.3323\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 28 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2109 - val_loss: 0.2002\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1278 - val_loss: 0.1725\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1201 - val_loss: 0.1616\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1169 - val_loss: 0.1749\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1162 - val_loss: 0.1786\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1147 - val_loss: 0.1690\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1140 - val_loss: 0.1690\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1138 - val_loss: 0.1716\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1136 - val_loss: 0.1780\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 35ms/step - loss: 0.1132 - val_loss: 0.1676\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 35ms/step - loss: 0.1128 - val_loss: 0.1784\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1127 - val_loss: 0.1646\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 35ms/step - loss: 0.1125 - val_loss: 0.1723\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 35ms/step - loss: 0.1124 - val_loss: 0.1703\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 35ms/step - loss: 0.1122 - val_loss: 0.1768\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 29 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.2888 - val_loss: 0.1407\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1338 - val_loss: 0.1281\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1275 - val_loss: 0.1242\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1244 - val_loss: 0.1234\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1226 - val_loss: 0.1230\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1214 - val_loss: 0.1213\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 35ms/step - loss: 0.1206 - val_loss: 0.1205\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1206 - val_loss: 0.1196\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1196 - val_loss: 0.1188\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1197 - val_loss: 0.1185\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1190 - val_loss: 0.1187\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1188 - val_loss: 0.1181\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1187 - val_loss: 0.1205\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1187 - val_loss: 0.1176\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1186 - val_loss: 0.1175\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 30 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2195 - val_loss: 0.1716\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1355 - val_loss: 0.1595\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1245 - val_loss: 0.1463\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1203 - val_loss: 0.1498\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1179 - val_loss: 0.1424\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1171 - val_loss: 0.1441\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1163 - val_loss: 0.1395\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1160 - val_loss: 0.1450\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1151 - val_loss: 0.1394\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1149 - val_loss: 0.1375\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1146 - val_loss: 0.1449\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1149 - val_loss: 0.1428\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1143 - val_loss: 0.1473\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1140 - val_loss: 0.1432\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1140 - val_loss: 0.1429\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 31 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.2026 - val_loss: 0.1665\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1243 - val_loss: 0.1472\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1158 - val_loss: 0.1402\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1130 - val_loss: 0.1394\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1119 - val_loss: 0.1393\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1110 - val_loss: 0.1362\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1105 - val_loss: 0.1377\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1102 - val_loss: 0.1339\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1097 - val_loss: 0.1336\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1092 - val_loss: 0.1371\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1089 - val_loss: 0.1354\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1091 - val_loss: 0.1390\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1087 - val_loss: 0.1302\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1086 - val_loss: 0.1369\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1086 - val_loss: 0.1329\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 32 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2186 - val_loss: 0.1676\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1340 - val_loss: 0.1462\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1222 - val_loss: 0.1398\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1167 - val_loss: 0.1371\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1148 - val_loss: 0.1384\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1135 - val_loss: 0.1346\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1129 - val_loss: 0.1350\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1122 - val_loss: 0.1337\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1121 - val_loss: 0.1332\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1119 - val_loss: 0.1335\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1115 - val_loss: 0.1332\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1114 - val_loss: 0.1327\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1112 - val_loss: 0.1329\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1111 - val_loss: 0.1328\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1108 - val_loss: 0.1328\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 33 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2224 - val_loss: 0.1558\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1373 - val_loss: 0.1374\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1265 - val_loss: 0.1329\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1231 - val_loss: 0.1294\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1211 - val_loss: 0.1285\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1203 - val_loss: 0.1274\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1196 - val_loss: 0.1299\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1190 - val_loss: 0.1273\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1183 - val_loss: 0.1263\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1176 - val_loss: 0.1260\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1178 - val_loss: 0.1265\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1173 - val_loss: 0.1258\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1170 - val_loss: 0.1253\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1170 - val_loss: 0.1253\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1163 - val_loss: 0.1262\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 34 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3722 - val_loss: 0.3573\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3121 - val_loss: 0.3511\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3093 - val_loss: 0.3499\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3085 - val_loss: 0.3495\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3082 - val_loss: 0.3493\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3080 - val_loss: 0.3491\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3079 - val_loss: 0.3490\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3079 - val_loss: 0.3490\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3078 - val_loss: 0.3490\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3078 - val_loss: 0.3489\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3078 - val_loss: 0.3489\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3078 - val_loss: 0.3489\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3078 - val_loss: 0.3489\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3078 - val_loss: 0.3489\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3078 - val_loss: 0.3489\n",
      "156/156 [==============================] - 1s 8ms/step\n",
      "Training on site number 35 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2097 - val_loss: 0.1581\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1292 - val_loss: 0.1466\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1198 - val_loss: 0.1417\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1169 - val_loss: 0.1330\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1146 - val_loss: 0.1301\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1143 - val_loss: 0.1301\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1134 - val_loss: 0.1314\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1128 - val_loss: 0.1319\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1128 - val_loss: 0.1333\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1119 - val_loss: 0.1305\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1119 - val_loss: 0.1363\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1117 - val_loss: 0.1326\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1117 - val_loss: 0.1321\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1112 - val_loss: 0.1329\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1113 - val_loss: 0.1332\n",
      "156/156 [==============================] - 1s 8ms/step\n",
      "Training on site number 36 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3411 - val_loss: 0.3238\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2815 - val_loss: 0.3178\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2789 - val_loss: 0.3167\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2782 - val_loss: 0.3164\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2780 - val_loss: 0.3163\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2779 - val_loss: 0.3162\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2778 - val_loss: 0.3161\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2778 - val_loss: 0.3161\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2778 - val_loss: 0.3161\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2778 - val_loss: 0.3161\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2778 - val_loss: 0.3161\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2778 - val_loss: 0.3161\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2778 - val_loss: 0.3161\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2778 - val_loss: 0.3161\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2778 - val_loss: 0.3161\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 37 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 38ms/step - loss: 0.3871 - val_loss: 0.3451\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3268 - val_loss: 0.3389\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3240 - val_loss: 0.3377\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3233 - val_loss: 0.3374\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3231 - val_loss: 0.3372\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3230 - val_loss: 0.3371\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3229 - val_loss: 0.3371\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3229 - val_loss: 0.3371\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3229 - val_loss: 0.3370\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3228 - val_loss: 0.3370\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3228 - val_loss: 0.3370\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3228 - val_loss: 0.3370\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3228 - val_loss: 0.3370\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3228 - val_loss: 0.3370\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3228 - val_loss: 0.3370\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 38 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2205 - val_loss: 0.1544\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1350 - val_loss: 0.1371\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1232 - val_loss: 0.1285\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1195 - val_loss: 0.1287\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1178 - val_loss: 0.1245\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1172 - val_loss: 0.1239\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1168 - val_loss: 0.1239\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1163 - val_loss: 0.1253\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1160 - val_loss: 0.1237\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1158 - val_loss: 0.1224\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1157 - val_loss: 0.1224\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1152 - val_loss: 0.1222\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1152 - val_loss: 0.1221\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1149 - val_loss: 0.1224\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1148 - val_loss: 0.1218\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 39 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2630 - val_loss: 0.1539\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1100 - val_loss: 0.1325\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1061 - val_loss: 0.1311\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1044 - val_loss: 0.1515\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1037 - val_loss: 0.1316\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1033 - val_loss: 0.1468\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1030 - val_loss: 0.1367\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1025 - val_loss: 0.1465\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1027 - val_loss: 0.1310\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1022 - val_loss: 0.1321\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1024 - val_loss: 0.1344\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1020 - val_loss: 0.1367\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1021 - val_loss: 0.1371\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1021 - val_loss: 0.1307\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1019 - val_loss: 0.1325\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 40 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2062 - val_loss: 0.1560\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1255 - val_loss: 0.1467\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1174 - val_loss: 0.1363\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1143 - val_loss: 0.1377\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1130 - val_loss: 0.1421\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1121 - val_loss: 0.1386\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1118 - val_loss: 0.1368\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1116 - val_loss: 0.1370\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1110 - val_loss: 0.1321\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1115 - val_loss: 0.1373\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1102 - val_loss: 0.1361\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1107 - val_loss: 0.1334\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1102 - val_loss: 0.1338\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1104 - val_loss: 0.1369\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1103 - val_loss: 0.1405\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 41 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.2077 - val_loss: 0.1500\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1240 - val_loss: 0.1362\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1134 - val_loss: 0.1297\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1102 - val_loss: 0.1258\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1087 - val_loss: 0.1266\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1077 - val_loss: 0.1267\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1075 - val_loss: 0.1222\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1070 - val_loss: 0.1244\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1065 - val_loss: 0.1231\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1063 - val_loss: 0.1274\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1059 - val_loss: 0.1244\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1061 - val_loss: 0.1211\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1057 - val_loss: 0.1237\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1060 - val_loss: 0.1247\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1055 - val_loss: 0.1251\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 42 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3527 - val_loss: 0.3023\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.2926 - val_loss: 0.2961\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.2897 - val_loss: 0.2948\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.2889 - val_loss: 0.2944\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.2887 - val_loss: 0.2942\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2885 - val_loss: 0.2942\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.2885 - val_loss: 0.2941\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2885 - val_loss: 0.2941\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2885 - val_loss: 0.2941\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2885 - val_loss: 0.2941\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.2885 - val_loss: 0.2941\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.2885 - val_loss: 0.2941\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.2885 - val_loss: 0.2941\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.2885 - val_loss: 0.2941\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.2885 - val_loss: 0.2941\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 43 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2159 - val_loss: 0.1728\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1355 - val_loss: 0.1530\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1260 - val_loss: 0.1538\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1230 - val_loss: 0.1480\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1213 - val_loss: 0.1477\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1201 - val_loss: 0.1489\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1198 - val_loss: 0.1530\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1189 - val_loss: 0.1474\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1186 - val_loss: 0.1621\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1187 - val_loss: 0.1463\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1181 - val_loss: 0.1481\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1181 - val_loss: 0.1472\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1180 - val_loss: 0.1469\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1177 - val_loss: 0.1481\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1174 - val_loss: 0.1517\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 44 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2029 - val_loss: 0.1733\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1231 - val_loss: 0.1592\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1157 - val_loss: 0.1591\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1129 - val_loss: 0.1529\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1114 - val_loss: 0.1528\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1105 - val_loss: 0.1497\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1102 - val_loss: 0.1529\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1099 - val_loss: 0.1523\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1095 - val_loss: 0.1483\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1095 - val_loss: 0.1517\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1088 - val_loss: 0.1554\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1088 - val_loss: 0.1573\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1087 - val_loss: 0.1529\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1085 - val_loss: 0.1541\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1083 - val_loss: 0.1491\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 45 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.3522 - val_loss: 0.3334\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2918 - val_loss: 0.3268\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2887 - val_loss: 0.3254\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2878 - val_loss: 0.3249\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2874 - val_loss: 0.3246\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2872 - val_loss: 0.3245\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2871 - val_loss: 0.3244\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2871 - val_loss: 0.3244\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2870 - val_loss: 0.3244\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2870 - val_loss: 0.3244\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2870 - val_loss: 0.3244\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2870 - val_loss: 0.3244\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2870 - val_loss: 0.3244\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2870 - val_loss: 0.3244\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2870 - val_loss: 0.3244\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 46 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2004 - val_loss: 0.1520\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1182 - val_loss: 0.1366\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1082 - val_loss: 0.1277\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1051 - val_loss: 0.1264\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1033 - val_loss: 0.1259\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1027 - val_loss: 0.1240\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1024 - val_loss: 0.1245\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1020 - val_loss: 0.1251\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1020 - val_loss: 0.1232\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1017 - val_loss: 0.1271\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1015 - val_loss: 0.1304\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1015 - val_loss: 0.1241\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1008 - val_loss: 0.1240\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1009 - val_loss: 0.1222\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1007 - val_loss: 0.1230\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 47 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2169 - val_loss: 0.1567\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1340 - val_loss: 0.1336\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1239 - val_loss: 0.1281\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1206 - val_loss: 0.1365\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1189 - val_loss: 0.1244\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1179 - val_loss: 0.1258\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1174 - val_loss: 0.1310\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1168 - val_loss: 0.1231\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1166 - val_loss: 0.1242\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1163 - val_loss: 0.1229\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1168 - val_loss: 0.1267\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1159 - val_loss: 0.1240\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1162 - val_loss: 0.1217\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1158 - val_loss: 0.1294\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1158 - val_loss: 0.1215\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 48 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2366 - val_loss: 0.1769\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1455 - val_loss: 0.1531\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1309 - val_loss: 0.1474\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1258 - val_loss: 0.1428\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1237 - val_loss: 0.1423\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1221 - val_loss: 0.1413\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1212 - val_loss: 0.1415\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1208 - val_loss: 0.1386\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1205 - val_loss: 0.1404\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1202 - val_loss: 0.1445\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1201 - val_loss: 0.1405\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1197 - val_loss: 0.1401\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1193 - val_loss: 0.1383\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1192 - val_loss: 0.1428\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1195 - val_loss: 0.1386\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 49 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 37ms/step - loss: 0.1844 - val_loss: 0.1607\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1068 - val_loss: 0.1372\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.0975 - val_loss: 0.1416\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.0942 - val_loss: 0.1330\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.0927 - val_loss: 0.1400\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.0912 - val_loss: 0.1259\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.0907 - val_loss: 0.1298\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.0902 - val_loss: 0.1344\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.0902 - val_loss: 0.1319\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.0898 - val_loss: 0.1307\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.0897 - val_loss: 0.1342\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.0895 - val_loss: 0.1301\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.0894 - val_loss: 0.1295\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.0890 - val_loss: 0.1344\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.0892 - val_loss: 0.1289\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 50 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2968 - val_loss: 0.3193\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2369 - val_loss: 0.3133\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2342 - val_loss: 0.3122\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2335 - val_loss: 0.3118\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2332 - val_loss: 0.3116\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2331 - val_loss: 0.3115\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2330 - val_loss: 0.3114\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2329 - val_loss: 0.3113\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2329 - val_loss: 0.3113\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2328 - val_loss: 0.3113\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2328 - val_loss: 0.3113\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2328 - val_loss: 0.3113\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2328 - val_loss: 0.3113\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2328 - val_loss: 0.3113\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2328 - val_loss: 0.3113\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 51 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2071 - val_loss: 0.1491\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1263 - val_loss: 0.1332\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1171 - val_loss: 0.1263\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1140 - val_loss: 0.1291\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1127 - val_loss: 0.1241\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1117 - val_loss: 0.1212\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1112 - val_loss: 0.1227\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1109 - val_loss: 0.1225\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1108 - val_loss: 0.1220\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1104 - val_loss: 0.1264\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1103 - val_loss: 0.1210\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1101 - val_loss: 0.1207\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1102 - val_loss: 0.1226\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1098 - val_loss: 0.1221\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1094 - val_loss: 0.1220\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 52 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2230 - val_loss: 0.1680\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1394 - val_loss: 0.1492\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1266 - val_loss: 0.1460\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1223 - val_loss: 0.1422\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1200 - val_loss: 0.1413\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1189 - val_loss: 0.1387\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1183 - val_loss: 0.1369\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1178 - val_loss: 0.1367\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1176 - val_loss: 0.1383\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1170 - val_loss: 0.1387\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1164 - val_loss: 0.1442\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1164 - val_loss: 0.1408\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1160 - val_loss: 0.1341\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1159 - val_loss: 0.1360\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1157 - val_loss: 0.1326\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 53 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2112 - val_loss: 0.1501\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1283 - val_loss: 0.1395\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1190 - val_loss: 0.1350\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1155 - val_loss: 0.1338\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1147 - val_loss: 0.1296\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1137 - val_loss: 0.1394\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1131 - val_loss: 0.1301\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1125 - val_loss: 0.1284\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1126 - val_loss: 0.1335\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1118 - val_loss: 0.1278\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1119 - val_loss: 0.1293\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1120 - val_loss: 0.1299\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1115 - val_loss: 0.1302\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1114 - val_loss: 0.1318\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1114 - val_loss: 0.1292\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 54 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2133 - val_loss: 0.1550\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1277 - val_loss: 0.1296\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1151 - val_loss: 0.1237\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1100 - val_loss: 0.1209\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1076 - val_loss: 0.1184\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1063 - val_loss: 0.1199\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1055 - val_loss: 0.1165\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1053 - val_loss: 0.1162\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1044 - val_loss: 0.1164\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1052 - val_loss: 0.1164\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1041 - val_loss: 0.1154\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1040 - val_loss: 0.1168\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1033 - val_loss: 0.1163\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1032 - val_loss: 0.1156\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1032 - val_loss: 0.1149\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 55 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2029 - val_loss: 0.1496\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1233 - val_loss: 0.1374\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1150 - val_loss: 0.1304\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1123 - val_loss: 0.1303\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1110 - val_loss: 0.1296\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1104 - val_loss: 0.1308\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1101 - val_loss: 0.1285\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1098 - val_loss: 0.1277\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1095 - val_loss: 0.1275\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1095 - val_loss: 0.1285\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1093 - val_loss: 0.1286\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1093 - val_loss: 0.1272\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1092 - val_loss: 0.1278\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1092 - val_loss: 0.1290\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1090 - val_loss: 0.1284\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 56 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2173 - val_loss: 0.1768\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1310 - val_loss: 0.1592\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1202 - val_loss: 0.1571\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1160 - val_loss: 0.1483\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1146 - val_loss: 0.1451\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1129 - val_loss: 0.1475\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1126 - val_loss: 0.1518\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1120 - val_loss: 0.1518\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1123 - val_loss: 0.1523\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1116 - val_loss: 0.1432\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1116 - val_loss: 0.1516\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1112 - val_loss: 0.1497\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1109 - val_loss: 0.1519\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1109 - val_loss: 0.1496\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1105 - val_loss: 0.1472\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 57 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.3618 - val_loss: 0.3090\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.3013 - val_loss: 0.3022\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2981 - val_loss: 0.3009\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2973 - val_loss: 0.3005\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2971 - val_loss: 0.3003\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2969 - val_loss: 0.3002\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2969 - val_loss: 0.3002\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2968 - val_loss: 0.3001\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2968 - val_loss: 0.3001\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2968 - val_loss: 0.3001\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2968 - val_loss: 0.3001\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2968 - val_loss: 0.3001\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2968 - val_loss: 0.3001\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2968 - val_loss: 0.3001\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2968 - val_loss: 0.3001\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 58 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1843 - val_loss: 0.1584\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1029 - val_loss: 0.1623\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.0941 - val_loss: 0.1449\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.0909 - val_loss: 0.1464\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.0896 - val_loss: 0.1472\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.0887 - val_loss: 0.1387\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.0883 - val_loss: 0.1475\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.0882 - val_loss: 0.1409\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.0875 - val_loss: 0.1402\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.0875 - val_loss: 0.1352\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.0873 - val_loss: 0.1279\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.0871 - val_loss: 0.1337\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.0874 - val_loss: 0.1389\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.0870 - val_loss: 0.1522\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.0871 - val_loss: 0.1345\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 59 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.3467 - val_loss: 0.3301\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2865 - val_loss: 0.3235\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2833 - val_loss: 0.3220\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2823 - val_loss: 0.3213\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2818 - val_loss: 0.3210\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2816 - val_loss: 0.3208\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2814 - val_loss: 0.3207\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2813 - val_loss: 0.3206\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2813 - val_loss: 0.3206\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2812 - val_loss: 0.3205\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2812 - val_loss: 0.3205\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2812 - val_loss: 0.3205\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2812 - val_loss: 0.3205\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2812 - val_loss: 0.3205\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2812 - val_loss: 0.3205\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 60 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.1918 - val_loss: 0.1758\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1123 - val_loss: 0.1704\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1067 - val_loss: 0.1465\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1036 - val_loss: 0.1363\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1024 - val_loss: 0.1439\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1019 - val_loss: 0.1482\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1010 - val_loss: 0.1472\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1004 - val_loss: 0.1413\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1000 - val_loss: 0.1447\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.0998 - val_loss: 0.1450\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.0999 - val_loss: 0.1397\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.0994 - val_loss: 0.1453\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.0998 - val_loss: 0.1455\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.0994 - val_loss: 0.1436\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.0995 - val_loss: 0.1427\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 61 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 37ms/step - loss: 0.2167 - val_loss: 0.1555\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1346 - val_loss: 0.1416\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1269 - val_loss: 0.1391\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1238 - val_loss: 0.1366\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1214 - val_loss: 0.1339\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1201 - val_loss: 0.1355\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1195 - val_loss: 0.1345\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1183 - val_loss: 0.1342\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1178 - val_loss: 0.1323\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1177 - val_loss: 0.1312\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1171 - val_loss: 0.1327\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1168 - val_loss: 0.1325\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1170 - val_loss: 0.1307\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1166 - val_loss: 0.1313\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1167 - val_loss: 0.1356\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 62 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3750 - val_loss: 0.3609\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3150 - val_loss: 0.3548\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3123 - val_loss: 0.3537\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3116 - val_loss: 0.3533\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3114 - val_loss: 0.3532\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3112 - val_loss: 0.3531\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3112 - val_loss: 0.3531\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3112 - val_loss: 0.3530\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3111 - val_loss: 0.3530\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3111 - val_loss: 0.3530\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3111 - val_loss: 0.3530\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3111 - val_loss: 0.3530\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3111 - val_loss: 0.3530\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3111 - val_loss: 0.3530\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 35ms/step - loss: 0.3111 - val_loss: 0.3530\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 63 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3823 - val_loss: 0.3860\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3221 - val_loss: 0.3797\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.3192 - val_loss: 0.3785\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.3185 - val_loss: 0.3781\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3183 - val_loss: 0.3779\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3181 - val_loss: 0.3778\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3180 - val_loss: 0.3777\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3180 - val_loss: 0.3777\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3180 - val_loss: 0.3776\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3179 - val_loss: 0.3776\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.3179 - val_loss: 0.3776\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3179 - val_loss: 0.3776\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3179 - val_loss: 0.3776\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3179 - val_loss: 0.3776\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3179 - val_loss: 0.3776\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 64 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2105 - val_loss: 0.1497\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1277 - val_loss: 0.1336\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1187 - val_loss: 0.1308\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1150 - val_loss: 0.1279\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1142 - val_loss: 0.1266\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1128 - val_loss: 0.1260\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1123 - val_loss: 0.1261\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1115 - val_loss: 0.1278\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1117 - val_loss: 0.1257\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1109 - val_loss: 0.1258\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1113 - val_loss: 0.1255\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1109 - val_loss: 0.1251\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1106 - val_loss: 0.1255\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1108 - val_loss: 0.1259\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1104 - val_loss: 0.1252\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 65 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2288 - val_loss: 0.1791\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1412 - val_loss: 0.1543\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1281 - val_loss: 0.1468\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1234 - val_loss: 0.1389\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1213 - val_loss: 0.1455\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1203 - val_loss: 0.1362\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1194 - val_loss: 0.1440\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1189 - val_loss: 0.1359\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1184 - val_loss: 0.1335\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1182 - val_loss: 0.1353\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1180 - val_loss: 0.1343\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1176 - val_loss: 0.1326\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1182 - val_loss: 0.1335\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1177 - val_loss: 0.1379\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1175 - val_loss: 0.1331\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 66 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2246 - val_loss: 0.1564\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1359 - val_loss: 0.1361\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1259 - val_loss: 0.1316\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1216 - val_loss: 0.1294\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1194 - val_loss: 0.1293\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1178 - val_loss: 0.1275\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1172 - val_loss: 0.1256\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1162 - val_loss: 0.1273\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1161 - val_loss: 0.1249\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1158 - val_loss: 0.1244\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1155 - val_loss: 0.1253\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1153 - val_loss: 0.1242\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1150 - val_loss: 0.1238\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1148 - val_loss: 0.1238\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1143 - val_loss: 0.1236\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 67 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2294 - val_loss: 0.1497\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1481 - val_loss: 0.1277\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1392 - val_loss: 0.1162\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1360 - val_loss: 0.1174\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1342 - val_loss: 0.1182\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1323 - val_loss: 0.1160\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1319 - val_loss: 0.1181\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1312 - val_loss: 0.1167\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1308 - val_loss: 0.1151\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1310 - val_loss: 0.1118\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1298 - val_loss: 0.1130\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1298 - val_loss: 0.1113\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1298 - val_loss: 0.1146\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1295 - val_loss: 0.1109\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1293 - val_loss: 0.1187\n",
      "156/156 [==============================] - 1s 8ms/step\n",
      "Training on site number 68 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2188 - val_loss: 0.1480\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1322 - val_loss: 0.1296\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1201 - val_loss: 0.1225\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1161 - val_loss: 0.1204\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1142 - val_loss: 0.1178\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1132 - val_loss: 0.1174\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1124 - val_loss: 0.1168\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1118 - val_loss: 0.1166\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1120 - val_loss: 0.1162\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1116 - val_loss: 0.1160\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1115 - val_loss: 0.1159\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1113 - val_loss: 0.1160\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1110 - val_loss: 0.1161\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1109 - val_loss: 0.1159\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1110 - val_loss: 0.1159\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 69 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2237 - val_loss: 0.1618\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1386 - val_loss: 0.1485\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1286 - val_loss: 0.1408\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1246 - val_loss: 0.1377\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1230 - val_loss: 0.1379\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1223 - val_loss: 0.1370\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1213 - val_loss: 0.1334\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1207 - val_loss: 0.1339\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1204 - val_loss: 0.1362\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1203 - val_loss: 0.1346\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1199 - val_loss: 0.1419\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1194 - val_loss: 0.1368\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1194 - val_loss: 0.1335\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1195 - val_loss: 0.1349\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1189 - val_loss: 0.1355\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 70 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2178 - val_loss: 0.1708\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1348 - val_loss: 0.1505\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1254 - val_loss: 0.1464\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1223 - val_loss: 0.1462\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1202 - val_loss: 0.1425\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1194 - val_loss: 0.1455\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1187 - val_loss: 0.1466\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1179 - val_loss: 0.1423\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1177 - val_loss: 0.1482\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1173 - val_loss: 0.1461\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1171 - val_loss: 0.1492\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1171 - val_loss: 0.1505\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1168 - val_loss: 0.1412\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1173 - val_loss: 0.1445\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1166 - val_loss: 0.1422\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 71 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.2132 - val_loss: 0.1526\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1282 - val_loss: 0.1363\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1187 - val_loss: 0.1351\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1150 - val_loss: 0.1303\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1135 - val_loss: 0.1293\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1121 - val_loss: 0.1289\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1115 - val_loss: 0.1276\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1110 - val_loss: 0.1275\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1108 - val_loss: 0.1272\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1108 - val_loss: 0.1268\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1102 - val_loss: 0.1266\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1103 - val_loss: 0.1265\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1105 - val_loss: 0.1282\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1101 - val_loss: 0.1265\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1100 - val_loss: 0.1292\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 72 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2258 - val_loss: 0.1640\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1428 - val_loss: 0.1464\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1325 - val_loss: 0.1403\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1292 - val_loss: 0.1373\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1274 - val_loss: 0.1362\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1265 - val_loss: 0.1352\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1258 - val_loss: 0.1365\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1255 - val_loss: 0.1345\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1250 - val_loss: 0.1360\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1246 - val_loss: 0.1392\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1245 - val_loss: 0.1359\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1244 - val_loss: 0.1356\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1239 - val_loss: 0.1332\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1238 - val_loss: 0.1351\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1236 - val_loss: 0.1336\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 73 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2146 - val_loss: 0.1614\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1289 - val_loss: 0.1529\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1184 - val_loss: 0.1379\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1140 - val_loss: 0.1345\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1126 - val_loss: 0.1412\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1112 - val_loss: 0.1331\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1107 - val_loss: 0.1382\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1104 - val_loss: 0.1333\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1109 - val_loss: 0.1332\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1095 - val_loss: 0.1365\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1097 - val_loss: 0.1328\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1093 - val_loss: 0.1343\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1093 - val_loss: 0.1319\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1093 - val_loss: 0.1329\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1090 - val_loss: 0.1363\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 74 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.3652 - val_loss: 0.3300\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3049 - val_loss: 0.3237\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3020 - val_loss: 0.3226\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3013 - val_loss: 0.3222\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3011 - val_loss: 0.3220\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3010 - val_loss: 0.3220\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3009 - val_loss: 0.3219\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3009 - val_loss: 0.3219\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3009 - val_loss: 0.3219\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3009 - val_loss: 0.3219\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3009 - val_loss: 0.3219\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3009 - val_loss: 0.3219\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3009 - val_loss: 0.3219\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3009 - val_loss: 0.3219\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3009 - val_loss: 0.3219\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 75 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2246 - val_loss: 0.1524\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1405 - val_loss: 0.1378\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1326 - val_loss: 0.1326\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1295 - val_loss: 0.1298\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1267 - val_loss: 0.1283\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1258 - val_loss: 0.1272\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1243 - val_loss: 0.1270\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1240 - val_loss: 0.1268\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1234 - val_loss: 0.1276\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1230 - val_loss: 0.1271\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1231 - val_loss: 0.1261\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1231 - val_loss: 0.1253\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1222 - val_loss: 0.1257\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1223 - val_loss: 0.1264\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1225 - val_loss: 0.1251\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 76 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2175 - val_loss: 0.1569\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1344 - val_loss: 0.1414\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1260 - val_loss: 0.1386\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1225 - val_loss: 0.1357\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1206 - val_loss: 0.1354\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1195 - val_loss: 0.1340\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1189 - val_loss: 0.1341\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1188 - val_loss: 0.1346\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1182 - val_loss: 0.1347\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1182 - val_loss: 0.1355\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1175 - val_loss: 0.1378\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1176 - val_loss: 0.1332\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1175 - val_loss: 0.1332\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1178 - val_loss: 0.1326\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1173 - val_loss: 0.1335\n",
      "156/156 [==============================] - 1s 7ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on site number 77 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2075 - val_loss: 0.1592\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1279 - val_loss: 0.1439\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1185 - val_loss: 0.1398\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1164 - val_loss: 0.1383\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1149 - val_loss: 0.1403\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1148 - val_loss: 0.1370\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1142 - val_loss: 0.1394\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1137 - val_loss: 0.1368\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1137 - val_loss: 0.1354\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1136 - val_loss: 0.1361\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1131 - val_loss: 0.1351\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1127 - val_loss: 0.1395\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1132 - val_loss: 0.1358\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1127 - val_loss: 0.1358\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1128 - val_loss: 0.1355\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 78 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1948 - val_loss: 0.1641\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1146 - val_loss: 0.1555\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1080 - val_loss: 0.1526\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1053 - val_loss: 0.1554\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1040 - val_loss: 0.1412\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1037 - val_loss: 0.1407\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1027 - val_loss: 0.1384\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1023 - val_loss: 0.1445\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1027 - val_loss: 0.1444\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1023 - val_loss: 0.1463\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1018 - val_loss: 0.1408\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1015 - val_loss: 0.1502\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1017 - val_loss: 0.1399\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1019 - val_loss: 0.1535\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 43ms/step - loss: 0.1013 - val_loss: 0.1435\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 79 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2105 - val_loss: 0.1612\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1252 - val_loss: 0.1490\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1155 - val_loss: 0.1426\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1115 - val_loss: 0.1410\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1093 - val_loss: 0.1329\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1090 - val_loss: 0.1384\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1077 - val_loss: 0.1355\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1077 - val_loss: 0.1395\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1072 - val_loss: 0.1474\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1071 - val_loss: 0.1404\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1068 - val_loss: 0.1354\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1063 - val_loss: 0.1353\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1063 - val_loss: 0.1374\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1063 - val_loss: 0.1384\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.1056 - val_loss: 0.1372\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 80 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2088 - val_loss: 0.1556\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1288 - val_loss: 0.1372\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1203 - val_loss: 0.1347\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1165 - val_loss: 0.1368\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1148 - val_loss: 0.1328\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1137 - val_loss: 0.1327\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1130 - val_loss: 0.1322\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1129 - val_loss: 0.1346\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1127 - val_loss: 0.1309\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1120 - val_loss: 0.1313\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1122 - val_loss: 0.1320\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1121 - val_loss: 0.1311\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1119 - val_loss: 0.1289\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1115 - val_loss: 0.1355\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1116 - val_loss: 0.1301\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 81 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.3448 - val_loss: 0.3234\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2852 - val_loss: 0.3172\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2822 - val_loss: 0.3159\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2814 - val_loss: 0.3153\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2809 - val_loss: 0.3150\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2807 - val_loss: 0.3148\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2806 - val_loss: 0.3147\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2805 - val_loss: 0.3147\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.2805 - val_loss: 0.3147\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2805 - val_loss: 0.3146\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2805 - val_loss: 0.3146\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2804 - val_loss: 0.3146\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2804 - val_loss: 0.3146\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2804 - val_loss: 0.3146\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2804 - val_loss: 0.3146\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 82 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2152 - val_loss: 0.1547\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1310 - val_loss: 0.1311\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1190 - val_loss: 0.1249\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1154 - val_loss: 0.1242\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1136 - val_loss: 0.1225\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1122 - val_loss: 0.1216\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1117 - val_loss: 0.1204\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1113 - val_loss: 0.1199\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1111 - val_loss: 0.1204\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1107 - val_loss: 0.1198\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1105 - val_loss: 0.1195\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1107 - val_loss: 0.1196\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1102 - val_loss: 0.1190\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1103 - val_loss: 0.1189\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1102 - val_loss: 0.1199\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 83 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.3555 - val_loss: 0.2852\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2953 - val_loss: 0.2789\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2923 - val_loss: 0.2775\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2914 - val_loss: 0.2770\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2910 - val_loss: 0.2768\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2909 - val_loss: 0.2767\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2908 - val_loss: 0.2766\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2908 - val_loss: 0.2766\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.2908 - val_loss: 0.2766\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2908 - val_loss: 0.2766\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2908 - val_loss: 0.2766\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2908 - val_loss: 0.2766\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2908 - val_loss: 0.2766\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2908 - val_loss: 0.2766\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.2908 - val_loss: 0.2766\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 84 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2513 - val_loss: 0.1422\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1188 - val_loss: 0.1326\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1131 - val_loss: 0.1317\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1116 - val_loss: 0.1287\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1114 - val_loss: 0.1322\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1107 - val_loss: 0.1325\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1104 - val_loss: 0.1300\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1106 - val_loss: 0.1350\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1105 - val_loss: 0.1327\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1111 - val_loss: 0.1317\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1102 - val_loss: 0.1300\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1100 - val_loss: 0.1292\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1104 - val_loss: 0.1326\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1100 - val_loss: 0.1322\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1102 - val_loss: 0.1306\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 85 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 38ms/step - loss: 0.2188 - val_loss: 0.1557\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1349 - val_loss: 0.1409\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1252 - val_loss: 0.1330\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1215 - val_loss: 0.1335\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1195 - val_loss: 0.1302\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1189 - val_loss: 0.1298\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1182 - val_loss: 0.1289\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1178 - val_loss: 0.1285\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1173 - val_loss: 0.1278\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1171 - val_loss: 0.1301\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1167 - val_loss: 0.1295\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1170 - val_loss: 0.1273\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1164 - val_loss: 0.1282\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1165 - val_loss: 0.1268\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1162 - val_loss: 0.1270\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 86 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.2237 - val_loss: 0.1606\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1388 - val_loss: 0.1474\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1275 - val_loss: 0.1389\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1237 - val_loss: 0.1367\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1229 - val_loss: 0.1401\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1215 - val_loss: 0.1371\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1209 - val_loss: 0.1350\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1203 - val_loss: 0.1385\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1201 - val_loss: 0.1335\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1200 - val_loss: 0.1383\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1197 - val_loss: 0.1358\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1196 - val_loss: 0.1352\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1197 - val_loss: 0.1348\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1196 - val_loss: 0.1369\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1196 - val_loss: 0.1377\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 87 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.1984 - val_loss: 0.1522\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1191 - val_loss: 0.1341\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1106 - val_loss: 0.1281\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1071 - val_loss: 0.1242\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1052 - val_loss: 0.1249\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1047 - val_loss: 0.1225\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1039 - val_loss: 0.1228\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1034 - val_loss: 0.1264\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1031 - val_loss: 0.1203\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1031 - val_loss: 0.1207\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1030 - val_loss: 0.1268\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1027 - val_loss: 0.1239\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1024 - val_loss: 0.1226\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1023 - val_loss: 0.1230\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1026 - val_loss: 0.1240\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 88 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2165 - val_loss: 0.1442\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1328 - val_loss: 0.1278\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1227 - val_loss: 0.1246\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1192 - val_loss: 0.1229\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1177 - val_loss: 0.1200\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1169 - val_loss: 0.1189\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1158 - val_loss: 0.1243\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1160 - val_loss: 0.1217\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1151 - val_loss: 0.1194\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1151 - val_loss: 0.1192\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1152 - val_loss: 0.1177\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1148 - val_loss: 0.1204\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1147 - val_loss: 0.1191\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1144 - val_loss: 0.1191\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1146 - val_loss: 0.1170\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 89 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2166 - val_loss: 0.1745\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1344 - val_loss: 0.1603\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1251 - val_loss: 0.1498\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1212 - val_loss: 0.1587\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1199 - val_loss: 0.1524\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1186 - val_loss: 0.1472\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1179 - val_loss: 0.1477\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1175 - val_loss: 0.1559\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1173 - val_loss: 0.1438\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1176 - val_loss: 0.1481\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1170 - val_loss: 0.1471\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1174 - val_loss: 0.1506\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1167 - val_loss: 0.1439\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1165 - val_loss: 0.1450\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1166 - val_loss: 0.1450\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 90 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2191 - val_loss: 0.1660\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1383 - val_loss: 0.1538\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1281 - val_loss: 0.1421\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1237 - val_loss: 0.1399\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1222 - val_loss: 0.1394\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1207 - val_loss: 0.1388\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1198 - val_loss: 0.1393\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1194 - val_loss: 0.1357\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1189 - val_loss: 0.1356\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1184 - val_loss: 0.1373\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1181 - val_loss: 0.1355\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1180 - val_loss: 0.1358\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1180 - val_loss: 0.1346\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1175 - val_loss: 0.1356\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1177 - val_loss: 0.1356\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 91 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3842 - val_loss: 0.3859\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3239 - val_loss: 0.3793\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3206 - val_loss: 0.3776\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3196 - val_loss: 0.3769\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3190 - val_loss: 0.3766\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3188 - val_loss: 0.3764\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3186 - val_loss: 0.3763\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3186 - val_loss: 0.3762\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3185 - val_loss: 0.3762\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3185 - val_loss: 0.3762\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3185 - val_loss: 0.3762\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3185 - val_loss: 0.3762\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3185 - val_loss: 0.3762\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3185 - val_loss: 0.3762\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3185 - val_loss: 0.3762\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 92 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1995 - val_loss: 0.1603\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1232 - val_loss: 0.1419\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1162 - val_loss: 0.1410\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1133 - val_loss: 0.1380\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1116 - val_loss: 0.1373\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1107 - val_loss: 0.1379\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1102 - val_loss: 0.1373\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1096 - val_loss: 0.1370\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1092 - val_loss: 0.1389\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1091 - val_loss: 0.1375\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1088 - val_loss: 0.1371\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1089 - val_loss: 0.1377\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1084 - val_loss: 0.1366\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1084 - val_loss: 0.1365\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1082 - val_loss: 0.1366\n",
      "156/156 [==============================] - 1s 5ms/step\n",
      "Training on site number 93 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3698 - val_loss: 0.3366\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3101 - val_loss: 0.3305\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3072 - val_loss: 0.3291\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.3063 - val_loss: 0.3286\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3060 - val_loss: 0.3284\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3058 - val_loss: 0.3283\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3057 - val_loss: 0.3282\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3057 - val_loss: 0.3282\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3057 - val_loss: 0.3282\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3057 - val_loss: 0.3282\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3057 - val_loss: 0.3282\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3056 - val_loss: 0.3282\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3056 - val_loss: 0.3282\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3056 - val_loss: 0.3282\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3056 - val_loss: 0.3282\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 94 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3645 - val_loss: 0.3516\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3047 - val_loss: 0.3454\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3018 - val_loss: 0.3440\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3010 - val_loss: 0.3435\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3006 - val_loss: 0.3432\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3004 - val_loss: 0.3431\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3002 - val_loss: 0.3430\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3001 - val_loss: 0.3429\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3001 - val_loss: 0.3428\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.3000 - val_loss: 0.3428\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.3000 - val_loss: 0.3428\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3000 - val_loss: 0.3428\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3000 - val_loss: 0.3428\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3000 - val_loss: 0.3428\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.3000 - val_loss: 0.3428\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 95 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.2056 - val_loss: 0.1585\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1248 - val_loss: 0.1422\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1160 - val_loss: 0.1437\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1123 - val_loss: 0.1367\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1109 - val_loss: 0.1346\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 82s 509ms/step - loss: 0.1108 - val_loss: 0.1345\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 238s 1s/step - loss: 0.1097 - val_loss: 0.1353\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1093 - val_loss: 0.1345\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1089 - val_loss: 0.1314\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1095 - val_loss: 0.1386\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 14s 85ms/step - loss: 0.1083 - val_loss: 0.1358\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1082 - val_loss: 0.1405\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1087 - val_loss: 0.1371\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1081 - val_loss: 0.1322\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 30s 185ms/step - loss: 0.1081 - val_loss: 0.1306\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 96 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.2283 - val_loss: 0.1672\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1435 - val_loss: 0.1464\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1315 - val_loss: 0.1406\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1272 - val_loss: 0.1391\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1256 - val_loss: 0.1362\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1244 - val_loss: 0.1377\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1241 - val_loss: 0.1365\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1234 - val_loss: 0.1374\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1230 - val_loss: 0.1363\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1227 - val_loss: 0.1361\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1224 - val_loss: 0.1340\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1228 - val_loss: 0.1371\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1222 - val_loss: 0.1363\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 36ms/step - loss: 0.1224 - val_loss: 0.1353\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1221 - val_loss: 0.1346\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 97 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 39ms/step - loss: 0.2309 - val_loss: 0.1818\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1447 - val_loss: 0.1596\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1307 - val_loss: 0.1487\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1260 - val_loss: 0.1446\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1238 - val_loss: 0.1438\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1223 - val_loss: 0.1424\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1214 - val_loss: 0.1392\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1206 - val_loss: 0.1435\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1201 - val_loss: 0.1371\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1197 - val_loss: 0.1401\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 44ms/step - loss: 0.1194 - val_loss: 0.1363\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1189 - val_loss: 0.1449\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1186 - val_loss: 0.1416\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 38ms/step - loss: 0.1188 - val_loss: 0.1391\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 37ms/step - loss: 0.1181 - val_loss: 0.1461\n",
      "156/156 [==============================] - 1s 6ms/step\n",
      "Training on site number 98 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.3823 - val_loss: 0.3289\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3219 - val_loss: 0.3223\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3187 - val_loss: 0.3207\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3176 - val_loss: 0.3200\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3170 - val_loss: 0.3195\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3167 - val_loss: 0.3193\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3164 - val_loss: 0.3191\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3163 - val_loss: 0.3190\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3162 - val_loss: 0.3189\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3161 - val_loss: 0.3188\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.3161 - val_loss: 0.3188\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3161 - val_loss: 0.3188\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3161 - val_loss: 0.3188\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.3161 - val_loss: 0.3188\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.3161 - val_loss: 0.3188\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 99 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.2358 - val_loss: 0.1607\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1466 - val_loss: 0.1355\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1322 - val_loss: 0.1329\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1272 - val_loss: 0.1259\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1249 - val_loss: 0.1235\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1235 - val_loss: 0.1230\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1227 - val_loss: 0.1223\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1221 - val_loss: 0.1223\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 7s 45ms/step - loss: 0.1222 - val_loss: 0.1258\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1218 - val_loss: 0.1215\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1213 - val_loss: 0.1221\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1214 - val_loss: 0.1253\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1211 - val_loss: 0.1204\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1209 - val_loss: 0.1209\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1207 - val_loss: 0.1212\n",
      "156/156 [==============================] - 1s 7ms/step\n",
      "Training on site number 100 of 100\n",
      "Epoch 1/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.3704 - val_loss: 0.2902\n",
      "Epoch 2/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1493 - val_loss: 0.1224\n",
      "Epoch 3/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1148 - val_loss: 0.1192\n",
      "Epoch 4/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1131 - val_loss: 0.1208\n",
      "Epoch 5/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1123 - val_loss: 0.1181\n",
      "Epoch 6/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1122 - val_loss: 0.1180\n",
      "Epoch 7/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1117 - val_loss: 0.1189\n",
      "Epoch 8/15\n",
      "162/162 [==============================] - 7s 41ms/step - loss: 0.1117 - val_loss: 0.1178\n",
      "Epoch 9/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1115 - val_loss: 0.1183\n",
      "Epoch 10/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1113 - val_loss: 0.1178\n",
      "Epoch 11/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1112 - val_loss: 0.1196\n",
      "Epoch 12/15\n",
      "162/162 [==============================] - 6s 39ms/step - loss: 0.1112 - val_loss: 0.1182\n",
      "Epoch 13/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1112 - val_loss: 0.1175\n",
      "Epoch 14/15\n",
      "162/162 [==============================] - 7s 40ms/step - loss: 0.1111 - val_loss: 0.1219\n",
      "Epoch 15/15\n",
      "162/162 [==============================] - 6s 40ms/step - loss: 0.1109 - val_loss: 0.1187\n",
      "156/156 [==============================] - 1s 7ms/step\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "# Train models for every selected site\n",
    "i = 1\n",
    "df1 = pd.DataFrame()\n",
    "df1['SiteID'], df1['MAE'] = list(), list()\n",
    "\n",
    "for filename in os.listdir(\"Data/Models\"):\n",
    "    print(f\"Training on site number {i} of 100\")\n",
    "    i += 1\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename[:-6] + '.csv', cy=2018)\n",
    "    model = define_model()\n",
    "    model.fit(X_train, y_train, epochs=15, validation_data=(X_test, y_test), batch_size=128)\n",
    "    model.save(\"Data/WTK Models/\" + filename)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    df1.loc[len(df1)] = [filename[:-6], mean_absolute_error(y_test * test_norms[2], predictions * train_norms[2])]\n",
    "    \n",
    "df1.to_csv(\"Data/Raw Experiment Data/WTK Model Performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd7e806169f1a8",
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "# Train one model with data starting at different years\n",
    "df = pd.DataFrame()\n",
    "df['Years'] = list()\n",
    "df['MAE'] = list()\n",
    "for year in range(2000, 2020):\n",
    "    print(f\"{year}\")\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data('7871.csv', cy=year)\n",
    "    model = define_model()\n",
    "    model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), batch_size=128)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test[:, 0] * test_norms[2], predictions * train_norms[2])\n",
    "\n",
    "    df.loc[len(df) + 1] = [int(year), mae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "861bcd17d03a0dc5",
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
=======
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "df = pd.read_csv(r'Data/Raw Experiment Data/starting year experiment.csv')\n",
    "df['Years'] = [2020 - x for x in df['Years']]\n",
    "# Display the data for use as a figure\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"white\")\n",
    "plt.xlabel('Years of Data')\n",
    "plt.ylabel('Mean Absolute Error (m/s)')\n",
    "plt.xlim(-2, 22)\n",
    "plt.scatter(df['Years'], df['MAE'], color='skyblue')\n",
    "y_lim = ax.get_ylim()\n",
    "plt.grid(True, axis='y', color='grey')\n",
    "\n",
    "# Plot the polynomial line of best fit\n",
    "def plot_best_fit(x, y, deg):\n",
    "    coeffs = np.polyfit(x, y, deg)\n",
    "    \n",
    "    ylist = list()\n",
    "    for n in x:\n",
    "        yvalue = np.sum([coeffs[i]*n**(len(coeffs)-1-i) for i in range(len(coeffs))])\n",
    "        ylist.append(yvalue)\n",
    "    plt.plot(x, ylist, color='skyblue')\n",
    "\n",
    "# plot_best_fit(df['Years'], df['MAE'], 3)\n",
    "\n",
    "# Plot the logarithmic line of best fit\n",
    "def plot_log_fit(x, y, deg):\n",
    "    coeffs = np.polyfit(np.log(x), y, deg)\n",
    "\n",
    "    ylist = list()\n",
    "    for n in np.log(x):\n",
    "        yvalue = np.sum([coeffs[i]*n**(len(coeffs)-1-i) for i in range(len(coeffs))])\n",
    "        ylist.append(yvalue)\n",
    "    plt.plot(x, ylist, color='skyblue')\n",
    "\n",
    "plot_log_fit(df['Years'], df['MAE'], 3)\n",
    "\n",
    "# Remove the border from the graph\n",
    "for direction in ['top', 'right', 'bottom', 'left']:\n",
    "    ax.spines[direction].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "de2650c1c0f9da24",
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
=======
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Epoch 1/10\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.2136 - val_loss: 0.1464\n",
      "Epoch 2/10\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1557 - val_loss: 0.1256\n",
      "Epoch 3/10\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1427 - val_loss: 0.1208\n",
      "Epoch 4/10\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1375 - val_loss: 0.1181\n",
      "Epoch 5/10\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1346 - val_loss: 0.1243\n",
      "Epoch 6/10\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1330 - val_loss: 0.1127\n",
      "Epoch 7/10\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1317 - val_loss: 0.1192\n",
      "Epoch 8/10\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1310 - val_loss: 0.1101\n",
      "Epoch 9/10\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1304 - val_loss: 0.1140\n",
      "Epoch 10/10\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1303 - val_loss: 0.1151\n",
      "154/154 [==============================] - 0s 1ms/step\n",
      "20\n",
      "Epoch 1/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.2182 - val_loss: 0.1471\n",
      "Epoch 2/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1576 - val_loss: 0.1334\n",
      "Epoch 3/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1441 - val_loss: 0.1248\n",
      "Epoch 4/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1383 - val_loss: 0.1164\n",
      "Epoch 5/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1350 - val_loss: 0.1191\n",
      "Epoch 6/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1331 - val_loss: 0.1222\n",
      "Epoch 7/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1316 - val_loss: 0.1151\n",
      "Epoch 8/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1316 - val_loss: 0.1135\n",
      "Epoch 9/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1305 - val_loss: 0.1109\n",
      "Epoch 10/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1299 - val_loss: 0.1204\n",
      "Epoch 11/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1297 - val_loss: 0.1153\n",
      "Epoch 12/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1293 - val_loss: 0.1144\n",
      "Epoch 13/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1290 - val_loss: 0.1110\n",
      "Epoch 14/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1289 - val_loss: 0.1099\n",
      "Epoch 15/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1290 - val_loss: 0.1145\n",
      "Epoch 16/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1285 - val_loss: 0.1094\n",
      "Epoch 17/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1284 - val_loss: 0.1109\n",
      "Epoch 18/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1290 - val_loss: 0.1229\n",
      "Epoch 19/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1282 - val_loss: 0.1102\n",
      "Epoch 20/20\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1280 - val_loss: 0.1083\n",
      "154/154 [==============================] - 0s 1ms/step\n",
      "30\n",
      "Epoch 1/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.2163 - val_loss: 0.1478\n",
      "Epoch 2/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1546 - val_loss: 0.1244\n",
      "Epoch 3/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1419 - val_loss: 0.1168\n",
      "Epoch 4/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1364 - val_loss: 0.1147\n",
      "Epoch 5/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1337 - val_loss: 0.1157\n",
      "Epoch 6/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1325 - val_loss: 0.1173\n",
      "Epoch 7/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1310 - val_loss: 0.1190\n",
      "Epoch 8/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1306 - val_loss: 0.1116\n",
      "Epoch 9/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1299 - val_loss: 0.1115\n",
      "Epoch 10/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1298 - val_loss: 0.1131\n",
      "Epoch 11/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1292 - val_loss: 0.1127\n",
      "Epoch 12/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1293 - val_loss: 0.1089\n",
      "Epoch 13/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1291 - val_loss: 0.1180\n",
      "Epoch 14/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1287 - val_loss: 0.1095\n",
      "Epoch 15/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1286 - val_loss: 0.1081\n",
      "Epoch 16/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1283 - val_loss: 0.1110\n",
      "Epoch 17/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1282 - val_loss: 0.1107\n",
      "Epoch 18/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1283 - val_loss: 0.1127\n",
      "Epoch 19/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1280 - val_loss: 0.1084\n",
      "Epoch 20/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1281 - val_loss: 0.1088\n",
      "Epoch 21/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1278 - val_loss: 0.1135\n",
      "Epoch 22/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1278 - val_loss: 0.1100\n",
      "Epoch 23/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1277 - val_loss: 0.1076\n",
      "Epoch 24/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1277 - val_loss: 0.1098\n",
      "Epoch 25/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1277 - val_loss: 0.1098\n",
      "Epoch 26/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1276 - val_loss: 0.1188\n",
      "Epoch 27/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1280 - val_loss: 0.1154\n",
      "Epoch 28/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1274 - val_loss: 0.1151\n",
      "Epoch 29/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1275 - val_loss: 0.1142\n",
      "Epoch 30/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1272 - val_loss: 0.1112\n",
      "154/154 [==============================] - 0s 1ms/step\n",
      "40\n",
      "Epoch 1/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.2225 - val_loss: 0.1510\n",
      "Epoch 2/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1610 - val_loss: 0.1333\n",
      "Epoch 3/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1468 - val_loss: 0.1204\n",
      "Epoch 4/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1397 - val_loss: 0.1404\n",
      "Epoch 5/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1364 - val_loss: 0.1185\n",
      "Epoch 6/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1340 - val_loss: 0.1174\n",
      "Epoch 7/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1324 - val_loss: 0.1150\n",
      "Epoch 8/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1313 - val_loss: 0.1166\n",
      "Epoch 9/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1306 - val_loss: 0.1238\n",
      "Epoch 10/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1302 - val_loss: 0.1147\n",
      "Epoch 11/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1300 - val_loss: 0.1103\n",
      "Epoch 12/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1297 - val_loss: 0.1206\n",
      "Epoch 13/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1292 - val_loss: 0.1144\n",
      "Epoch 14/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1287 - val_loss: 0.1113\n",
      "Epoch 15/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1288 - val_loss: 0.1163\n",
      "Epoch 16/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1289 - val_loss: 0.1200\n",
      "Epoch 17/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1285 - val_loss: 0.1142\n",
      "Epoch 18/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1283 - val_loss: 0.1140\n",
      "Epoch 19/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1280 - val_loss: 0.1127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1281 - val_loss: 0.1085\n",
      "Epoch 21/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1278 - val_loss: 0.1069\n",
      "Epoch 22/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1276 - val_loss: 0.1094\n",
      "Epoch 23/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1277 - val_loss: 0.1103\n",
      "Epoch 24/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1272 - val_loss: 0.1097\n",
      "Epoch 25/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1273 - val_loss: 0.1101\n",
      "Epoch 26/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1271 - val_loss: 0.1098\n",
      "Epoch 27/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1276 - val_loss: 0.1126\n",
      "Epoch 28/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1270 - val_loss: 0.1119\n",
      "Epoch 29/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1268 - val_loss: 0.1092\n",
      "Epoch 30/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1269 - val_loss: 0.1100\n",
      "Epoch 31/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1266 - val_loss: 0.1114\n",
      "Epoch 32/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1272 - val_loss: 0.1070\n",
      "Epoch 33/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1266 - val_loss: 0.1101\n",
      "Epoch 34/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1266 - val_loss: 0.1101\n",
      "Epoch 35/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1271 - val_loss: 0.1186\n",
      "Epoch 36/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1267 - val_loss: 0.1132\n",
      "Epoch 37/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1265 - val_loss: 0.1137\n",
      "Epoch 38/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1264 - val_loss: 0.1085\n",
      "Epoch 39/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1263 - val_loss: 0.1161\n",
      "Epoch 40/40\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1264 - val_loss: 0.1109\n",
      "154/154 [==============================] - 0s 1ms/step\n",
      "50\n",
      "Epoch 1/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.2623 - val_loss: 0.1471\n",
      "Epoch 2/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1413 - val_loss: 0.1155\n",
      "Epoch 3/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1342 - val_loss: 0.1206\n",
      "Epoch 4/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1328 - val_loss: 0.1131\n",
      "Epoch 5/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1314 - val_loss: 0.1141\n",
      "Epoch 6/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1313 - val_loss: 0.1204\n",
      "Epoch 7/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1308 - val_loss: 0.1119\n",
      "Epoch 8/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1302 - val_loss: 0.1161\n",
      "Epoch 9/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1302 - val_loss: 0.1151\n",
      "Epoch 10/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1300 - val_loss: 0.1150\n",
      "Epoch 11/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1298 - val_loss: 0.1180\n",
      "Epoch 12/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1295 - val_loss: 0.1291\n",
      "Epoch 13/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1295 - val_loss: 0.1183\n",
      "Epoch 14/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1297 - val_loss: 0.1116\n",
      "Epoch 15/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1293 - val_loss: 0.1141\n",
      "Epoch 16/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1288 - val_loss: 0.1085\n",
      "Epoch 17/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1292 - val_loss: 0.1125\n",
      "Epoch 18/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1288 - val_loss: 0.1115\n",
      "Epoch 19/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1287 - val_loss: 0.1088\n",
      "Epoch 20/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1285 - val_loss: 0.1092\n",
      "Epoch 21/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1284 - val_loss: 0.1244\n",
      "Epoch 22/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1286 - val_loss: 0.1080\n",
      "Epoch 23/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1282 - val_loss: 0.1118\n",
      "Epoch 24/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1283 - val_loss: 0.1082\n",
      "Epoch 25/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1286 - val_loss: 0.1108\n",
      "Epoch 26/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1279 - val_loss: 0.1159\n",
      "Epoch 27/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1281 - val_loss: 0.1089\n",
      "Epoch 28/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1281 - val_loss: 0.1198\n",
      "Epoch 29/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1283 - val_loss: 0.1179\n",
      "Epoch 30/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1277 - val_loss: 0.1162\n",
      "Epoch 31/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1276 - val_loss: 0.1123\n",
      "Epoch 32/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1276 - val_loss: 0.1104\n",
      "Epoch 33/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1277 - val_loss: 0.1125\n",
      "Epoch 34/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1274 - val_loss: 0.1094\n",
      "Epoch 35/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1275 - val_loss: 0.1090\n",
      "Epoch 36/50\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1275 - val_loss: 0.1069\n",
      "Epoch 37/50\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1274 - val_loss: 0.1094\n",
      "Epoch 38/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1279 - val_loss: 0.1143\n",
      "Epoch 39/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1275 - val_loss: 0.1078\n",
      "Epoch 40/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1275 - val_loss: 0.1105\n",
      "Epoch 41/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1274 - val_loss: 0.1076\n",
      "Epoch 42/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1275 - val_loss: 0.1097\n",
      "Epoch 43/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1276 - val_loss: 0.1123\n",
      "Epoch 44/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1273 - val_loss: 0.1103\n",
      "Epoch 45/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1275 - val_loss: 0.1136\n",
      "Epoch 46/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1271 - val_loss: 0.1126\n",
      "Epoch 47/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1273 - val_loss: 0.1118\n",
      "Epoch 48/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1272 - val_loss: 0.1099\n",
      "Epoch 49/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1274 - val_loss: 0.1108\n",
      "Epoch 50/50\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1270 - val_loss: 0.1091\n",
      "154/154 [==============================] - 0s 1ms/step\n",
      "60\n",
      "Epoch 1/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.2161 - val_loss: 0.1511\n",
      "Epoch 2/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1552 - val_loss: 0.1291\n",
      "Epoch 3/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1430 - val_loss: 0.1208\n",
      "Epoch 4/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1371 - val_loss: 0.1163\n",
      "Epoch 5/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1340 - val_loss: 0.1206\n",
      "Epoch 6/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1330 - val_loss: 0.1139\n",
      "Epoch 7/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1316 - val_loss: 0.1180\n",
      "Epoch 8/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1305 - val_loss: 0.1116\n",
      "Epoch 9/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1302 - val_loss: 0.1137\n",
      "Epoch 10/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1296 - val_loss: 0.1143\n",
      "Epoch 11/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1296 - val_loss: 0.1172\n",
      "Epoch 12/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1293 - val_loss: 0.1114\n",
      "Epoch 13/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1292 - val_loss: 0.1134\n",
      "Epoch 14/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1291 - val_loss: 0.1141\n",
      "Epoch 15/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1285 - val_loss: 0.1130\n",
      "Epoch 16/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1283 - val_loss: 0.1117\n",
      "Epoch 17/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1287 - val_loss: 0.1102\n",
      "Epoch 18/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1280 - val_loss: 0.1087\n",
      "Epoch 19/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1280 - val_loss: 0.1092\n",
      "Epoch 20/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1282 - val_loss: 0.1099\n",
      "Epoch 21/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1276 - val_loss: 0.1131\n",
      "Epoch 22/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1274 - val_loss: 0.1135\n",
      "Epoch 23/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1276 - val_loss: 0.1130\n",
      "Epoch 24/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1278 - val_loss: 0.1115\n",
      "Epoch 25/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1278 - val_loss: 0.1139\n",
      "Epoch 26/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1273 - val_loss: 0.1121\n",
      "Epoch 27/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1270 - val_loss: 0.1093\n",
      "Epoch 28/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1273 - val_loss: 0.1094\n",
      "Epoch 29/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1267 - val_loss: 0.1118\n",
      "Epoch 30/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1269 - val_loss: 0.1174\n",
      "Epoch 31/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1271 - val_loss: 0.1150\n",
      "Epoch 32/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1267 - val_loss: 0.1088\n",
      "Epoch 33/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1266 - val_loss: 0.1108\n",
      "Epoch 34/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1266 - val_loss: 0.1090\n",
      "Epoch 35/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1267 - val_loss: 0.1090\n",
      "Epoch 36/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1263 - val_loss: 0.1244\n",
      "Epoch 37/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1269 - val_loss: 0.1107\n",
      "Epoch 38/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1267 - val_loss: 0.1114\n",
      "Epoch 39/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1266 - val_loss: 0.1110\n",
      "Epoch 40/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1263 - val_loss: 0.1122\n",
      "Epoch 41/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1262 - val_loss: 0.1080\n",
      "Epoch 42/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1264 - val_loss: 0.1084\n",
      "Epoch 43/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1262 - val_loss: 0.1107\n",
      "Epoch 44/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1263 - val_loss: 0.1103\n",
      "Epoch 45/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1265 - val_loss: 0.1086\n",
      "Epoch 46/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1261 - val_loss: 0.1095\n",
      "Epoch 47/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1262 - val_loss: 0.1097\n",
      "Epoch 48/60\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1260 - val_loss: 0.1114\n",
      "Epoch 49/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1259 - val_loss: 0.1101\n",
      "Epoch 50/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1258 - val_loss: 0.1074\n",
      "Epoch 51/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1258 - val_loss: 0.1116\n",
      "Epoch 52/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1256 - val_loss: 0.1117\n",
      "Epoch 53/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1258 - val_loss: 0.1091\n",
      "Epoch 54/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1255 - val_loss: 0.1081\n",
      "Epoch 55/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1256 - val_loss: 0.1110\n",
      "Epoch 56/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1257 - val_loss: 0.1137\n",
      "Epoch 57/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1255 - val_loss: 0.1107\n",
      "Epoch 58/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1256 - val_loss: 0.1094\n",
      "Epoch 59/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1252 - val_loss: 0.1109\n",
      "Epoch 60/60\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1255 - val_loss: 0.1164\n",
      "154/154 [==============================] - 0s 1ms/step\n",
      "70\n",
      "Epoch 1/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.2184 - val_loss: 0.1492\n",
      "Epoch 2/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1605 - val_loss: 0.1320\n",
      "Epoch 3/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1469 - val_loss: 0.1301\n",
      "Epoch 4/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1399 - val_loss: 0.1202\n",
      "Epoch 5/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1363 - val_loss: 0.1142\n",
      "Epoch 6/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1337 - val_loss: 0.1126\n",
      "Epoch 7/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1324 - val_loss: 0.1137\n",
      "Epoch 8/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1311 - val_loss: 0.1109\n",
      "Epoch 9/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1306 - val_loss: 0.1132\n",
      "Epoch 10/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1299 - val_loss: 0.1119\n",
      "Epoch 11/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1296 - val_loss: 0.1086\n",
      "Epoch 12/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1295 - val_loss: 0.1084\n",
      "Epoch 13/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1292 - val_loss: 0.1080\n",
      "Epoch 14/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1290 - val_loss: 0.1129\n",
      "Epoch 15/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1288 - val_loss: 0.1082\n",
      "Epoch 16/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1285 - val_loss: 0.1182\n",
      "Epoch 17/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1284 - val_loss: 0.1164\n",
      "Epoch 18/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1283 - val_loss: 0.1108\n",
      "Epoch 19/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1283 - val_loss: 0.1097\n",
      "Epoch 20/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1281 - val_loss: 0.1116\n",
      "Epoch 21/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1279 - val_loss: 0.1136\n",
      "Epoch 22/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1279 - val_loss: 0.1080\n",
      "Epoch 23/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1286 - val_loss: 0.1108\n",
      "Epoch 24/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1275 - val_loss: 0.1077\n",
      "Epoch 25/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1278 - val_loss: 0.1090\n",
      "Epoch 26/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1275 - val_loss: 0.1095\n",
      "Epoch 27/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1273 - val_loss: 0.1109\n",
      "Epoch 28/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1271 - val_loss: 0.1073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1276 - val_loss: 0.1079\n",
      "Epoch 30/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1270 - val_loss: 0.1098\n",
      "Epoch 31/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1269 - val_loss: 0.1146\n",
      "Epoch 32/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1269 - val_loss: 0.1077\n",
      "Epoch 33/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1273 - val_loss: 0.1140\n",
      "Epoch 34/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1273 - val_loss: 0.1081\n",
      "Epoch 35/70\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1264 - val_loss: 0.1115\n",
      "Epoch 36/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1264 - val_loss: 0.1082\n",
      "Epoch 37/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1269 - val_loss: 0.1162\n",
      "Epoch 38/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1265 - val_loss: 0.1182\n",
      "Epoch 39/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1264 - val_loss: 0.1112\n",
      "Epoch 40/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1262 - val_loss: 0.1144\n",
      "Epoch 41/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1265 - val_loss: 0.1125\n",
      "Epoch 42/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1266 - val_loss: 0.1122\n",
      "Epoch 43/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1265 - val_loss: 0.1080\n",
      "Epoch 44/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1264 - val_loss: 0.1128\n",
      "Epoch 45/70\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1259 - val_loss: 0.1086\n",
      "Epoch 46/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1260 - val_loss: 0.1120\n",
      "Epoch 47/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1261 - val_loss: 0.1105\n",
      "Epoch 48/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1259 - val_loss: 0.1092\n",
      "Epoch 49/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1259 - val_loss: 0.1137\n",
      "Epoch 50/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1259 - val_loss: 0.1122\n",
      "Epoch 51/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1260 - val_loss: 0.1148\n",
      "Epoch 52/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1264 - val_loss: 0.1131\n",
      "Epoch 53/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1261 - val_loss: 0.1138\n",
      "Epoch 54/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1258 - val_loss: 0.1081\n",
      "Epoch 55/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1258 - val_loss: 0.1112\n",
      "Epoch 56/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1258 - val_loss: 0.1085\n",
      "Epoch 57/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1258 - val_loss: 0.1068\n",
      "Epoch 58/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1257 - val_loss: 0.1145\n",
      "Epoch 59/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1256 - val_loss: 0.1103\n",
      "Epoch 60/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1257 - val_loss: 0.1094\n",
      "Epoch 61/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1257 - val_loss: 0.1097\n",
      "Epoch 62/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1256 - val_loss: 0.1094\n",
      "Epoch 63/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1254 - val_loss: 0.1108\n",
      "Epoch 64/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1250 - val_loss: 0.1102\n",
      "Epoch 65/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1258 - val_loss: 0.1093\n",
      "Epoch 66/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1253 - val_loss: 0.1104\n",
      "Epoch 67/70\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1256 - val_loss: 0.1084\n",
      "Epoch 68/70\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1252 - val_loss: 0.1115\n",
      "Epoch 69/70\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1254 - val_loss: 0.1090\n",
      "Epoch 70/70\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1256 - val_loss: 0.1094\n",
      "154/154 [==============================] - 0s 1ms/step\n",
      "80\n",
      "Epoch 1/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.2169 - val_loss: 0.1460\n",
      "Epoch 2/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1557 - val_loss: 0.1313\n",
      "Epoch 3/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1427 - val_loss: 0.1237\n",
      "Epoch 4/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1370 - val_loss: 0.1181\n",
      "Epoch 5/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1341 - val_loss: 0.1229\n",
      "Epoch 6/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1320 - val_loss: 0.1177\n",
      "Epoch 7/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1314 - val_loss: 0.1121\n",
      "Epoch 8/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1302 - val_loss: 0.1157\n",
      "Epoch 9/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1298 - val_loss: 0.1091\n",
      "Epoch 10/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1299 - val_loss: 0.1107\n",
      "Epoch 11/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1292 - val_loss: 0.1166\n",
      "Epoch 12/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1288 - val_loss: 0.1099\n",
      "Epoch 13/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1288 - val_loss: 0.1148\n",
      "Epoch 14/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1285 - val_loss: 0.1107\n",
      "Epoch 15/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1285 - val_loss: 0.1108\n",
      "Epoch 16/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1284 - val_loss: 0.1106\n",
      "Epoch 17/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1283 - val_loss: 0.1085\n",
      "Epoch 18/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1284 - val_loss: 0.1091\n",
      "Epoch 19/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1280 - val_loss: 0.1106\n",
      "Epoch 20/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1280 - val_loss: 0.1094\n",
      "Epoch 21/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1279 - val_loss: 0.1084\n",
      "Epoch 22/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1280 - val_loss: 0.1112\n",
      "Epoch 23/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1278 - val_loss: 0.1138\n",
      "Epoch 24/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1277 - val_loss: 0.1135\n",
      "Epoch 25/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1276 - val_loss: 0.1093\n",
      "Epoch 26/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1279 - val_loss: 0.1083\n",
      "Epoch 27/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1278 - val_loss: 0.1140\n",
      "Epoch 28/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1271 - val_loss: 0.1101\n",
      "Epoch 29/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1274 - val_loss: 0.1092\n",
      "Epoch 30/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1279 - val_loss: 0.1070\n",
      "Epoch 31/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1272 - val_loss: 0.1172\n",
      "Epoch 32/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1274 - val_loss: 0.1114\n",
      "Epoch 33/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1273 - val_loss: 0.1115\n",
      "Epoch 34/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1273 - val_loss: 0.1092\n",
      "Epoch 35/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1273 - val_loss: 0.1114\n",
      "Epoch 36/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1273 - val_loss: 0.1093\n",
      "Epoch 37/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1272 - val_loss: 0.1171\n",
      "Epoch 38/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1272 - val_loss: 0.1092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1271 - val_loss: 0.1148\n",
      "Epoch 40/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1271 - val_loss: 0.1098\n",
      "Epoch 41/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1273 - val_loss: 0.1113\n",
      "Epoch 42/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1271 - val_loss: 0.1144\n",
      "Epoch 43/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1265 - val_loss: 0.1144\n",
      "Epoch 44/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1267 - val_loss: 0.1139\n",
      "Epoch 45/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1271 - val_loss: 0.1120\n",
      "Epoch 46/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1267 - val_loss: 0.1098\n",
      "Epoch 47/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1267 - val_loss: 0.1079\n",
      "Epoch 48/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1269 - val_loss: 0.1130\n",
      "Epoch 49/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1269 - val_loss: 0.1104\n",
      "Epoch 50/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1262 - val_loss: 0.1076\n",
      "Epoch 51/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1265 - val_loss: 0.1140\n",
      "Epoch 52/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1263 - val_loss: 0.1083\n",
      "Epoch 53/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1266 - val_loss: 0.1099\n",
      "Epoch 54/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1262 - val_loss: 0.1110\n",
      "Epoch 55/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1264 - val_loss: 0.1144\n",
      "Epoch 56/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1263 - val_loss: 0.1120\n",
      "Epoch 57/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1261 - val_loss: 0.1094\n",
      "Epoch 58/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1262 - val_loss: 0.1127\n",
      "Epoch 59/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1260 - val_loss: 0.1146\n",
      "Epoch 60/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1256 - val_loss: 0.1123\n",
      "Epoch 61/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1258 - val_loss: 0.1077\n",
      "Epoch 62/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1259 - val_loss: 0.1081\n",
      "Epoch 63/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1256 - val_loss: 0.1110\n",
      "Epoch 64/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1260 - val_loss: 0.1086\n",
      "Epoch 65/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1257 - val_loss: 0.1104\n",
      "Epoch 66/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1256 - val_loss: 0.1083\n",
      "Epoch 67/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1256 - val_loss: 0.1101\n",
      "Epoch 68/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1253 - val_loss: 0.1105\n",
      "Epoch 69/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1257 - val_loss: 0.1067\n",
      "Epoch 70/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1254 - val_loss: 0.1129\n",
      "Epoch 71/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1256 - val_loss: 0.1093\n",
      "Epoch 72/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1250 - val_loss: 0.1079\n",
      "Epoch 73/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1250 - val_loss: 0.1138\n",
      "Epoch 74/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1255 - val_loss: 0.1117\n",
      "Epoch 75/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1253 - val_loss: 0.1107\n",
      "Epoch 76/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1248 - val_loss: 0.1075\n",
      "Epoch 77/80\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1250 - val_loss: 0.1071\n",
      "Epoch 78/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1247 - val_loss: 0.1098\n",
      "Epoch 79/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1252 - val_loss: 0.1078\n",
      "Epoch 80/80\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1248 - val_loss: 0.1093\n",
      "154/154 [==============================] - 0s 1ms/step\n",
      "90\n",
      "Epoch 1/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.2120 - val_loss: 0.1488\n",
      "Epoch 2/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1551 - val_loss: 0.1277\n",
      "Epoch 3/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1428 - val_loss: 0.1182\n",
      "Epoch 4/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1376 - val_loss: 0.1175\n",
      "Epoch 5/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1344 - val_loss: 0.1171\n",
      "Epoch 6/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1326 - val_loss: 0.1197\n",
      "Epoch 7/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1316 - val_loss: 0.1149\n",
      "Epoch 8/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1308 - val_loss: 0.1117\n",
      "Epoch 9/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1299 - val_loss: 0.1148\n",
      "Epoch 10/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1299 - val_loss: 0.1090\n",
      "Epoch 11/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1295 - val_loss: 0.1138\n",
      "Epoch 12/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1288 - val_loss: 0.1139\n",
      "Epoch 13/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1292 - val_loss: 0.1083\n",
      "Epoch 14/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1287 - val_loss: 0.1190\n",
      "Epoch 15/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1289 - val_loss: 0.1089\n",
      "Epoch 16/90\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1283 - val_loss: 0.1113\n",
      "Epoch 17/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1282 - val_loss: 0.1170\n",
      "Epoch 18/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1279 - val_loss: 0.1081\n",
      "Epoch 19/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1277 - val_loss: 0.1164\n",
      "Epoch 20/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1278 - val_loss: 0.1085\n",
      "Epoch 21/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1282 - val_loss: 0.1102\n",
      "Epoch 22/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1272 - val_loss: 0.1193\n",
      "Epoch 23/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1275 - val_loss: 0.1083\n",
      "Epoch 24/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1274 - val_loss: 0.1190\n",
      "Epoch 25/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1274 - val_loss: 0.1109\n",
      "Epoch 26/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1268 - val_loss: 0.1131\n",
      "Epoch 27/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1269 - val_loss: 0.1113\n",
      "Epoch 28/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1275 - val_loss: 0.1098\n",
      "Epoch 29/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1270 - val_loss: 0.1089\n",
      "Epoch 30/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1270 - val_loss: 0.1088\n",
      "Epoch 31/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1273 - val_loss: 0.1087\n",
      "Epoch 32/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1271 - val_loss: 0.1187\n",
      "Epoch 33/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1269 - val_loss: 0.1080\n",
      "Epoch 34/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1268 - val_loss: 0.1087\n",
      "Epoch 35/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1265 - val_loss: 0.1099\n",
      "Epoch 36/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1269 - val_loss: 0.1136\n",
      "Epoch 37/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1262 - val_loss: 0.1091\n",
      "Epoch 38/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1266 - val_loss: 0.1090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1267 - val_loss: 0.1104\n",
      "Epoch 40/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1265 - val_loss: 0.1072\n",
      "Epoch 41/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1263 - val_loss: 0.1143\n",
      "Epoch 42/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1259 - val_loss: 0.1098\n",
      "Epoch 43/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1264 - val_loss: 0.1144\n",
      "Epoch 44/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1264 - val_loss: 0.1091\n",
      "Epoch 45/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1258 - val_loss: 0.1122\n",
      "Epoch 46/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1263 - val_loss: 0.1076\n",
      "Epoch 47/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1264 - val_loss: 0.1089\n",
      "Epoch 48/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1260 - val_loss: 0.1069\n",
      "Epoch 49/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1260 - val_loss: 0.1082\n",
      "Epoch 50/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1260 - val_loss: 0.1087\n",
      "Epoch 51/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1259 - val_loss: 0.1122\n",
      "Epoch 52/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1257 - val_loss: 0.1176\n",
      "Epoch 53/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1259 - val_loss: 0.1082\n",
      "Epoch 54/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1256 - val_loss: 0.1084\n",
      "Epoch 55/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1257 - val_loss: 0.1113\n",
      "Epoch 56/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1253 - val_loss: 0.1088\n",
      "Epoch 57/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1255 - val_loss: 0.1085\n",
      "Epoch 58/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1254 - val_loss: 0.1077\n",
      "Epoch 59/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1252 - val_loss: 0.1103\n",
      "Epoch 60/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1256 - val_loss: 0.1073\n",
      "Epoch 61/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1250 - val_loss: 0.1125\n",
      "Epoch 62/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1259 - val_loss: 0.1094\n",
      "Epoch 63/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1258 - val_loss: 0.1104\n",
      "Epoch 64/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1252 - val_loss: 0.1115\n",
      "Epoch 65/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1254 - val_loss: 0.1087\n",
      "Epoch 66/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1247 - val_loss: 0.1111\n",
      "Epoch 67/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1250 - val_loss: 0.1068\n",
      "Epoch 68/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1255 - val_loss: 0.1081\n",
      "Epoch 69/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1248 - val_loss: 0.1074\n",
      "Epoch 70/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1254 - val_loss: 0.1080\n",
      "Epoch 71/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1248 - val_loss: 0.1086\n",
      "Epoch 72/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1247 - val_loss: 0.1109\n",
      "Epoch 73/90\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1251 - val_loss: 0.1076\n",
      "Epoch 74/90\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1245 - val_loss: 0.1076\n",
      "Epoch 75/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1245 - val_loss: 0.1081\n",
      "Epoch 76/90\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1251 - val_loss: 0.1080\n",
      "Epoch 77/90\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1244 - val_loss: 0.1098\n",
      "Epoch 78/90\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1243 - val_loss: 0.1100\n",
      "Epoch 79/90\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1241 - val_loss: 0.1119\n",
      "Epoch 80/90\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1245 - val_loss: 0.1100\n",
      "Epoch 81/90\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1242 - val_loss: 0.1073\n",
      "Epoch 82/90\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1240 - val_loss: 0.1107\n",
      "Epoch 83/90\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1240 - val_loss: 0.1072\n",
      "Epoch 84/90\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1236 - val_loss: 0.1099\n",
      "Epoch 85/90\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1238 - val_loss: 0.1075\n",
      "Epoch 86/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1240 - val_loss: 0.1087\n",
      "Epoch 87/90\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1236 - val_loss: 0.1093\n",
      "Epoch 88/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1236 - val_loss: 0.1074\n",
      "Epoch 89/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1237 - val_loss: 0.1107\n",
      "Epoch 90/90\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1231 - val_loss: 0.1117\n",
      "154/154 [==============================] - 0s 1ms/step\n",
      "100\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.2196 - val_loss: 0.1576\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1596 - val_loss: 0.1343\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1446 - val_loss: 0.1190\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1388 - val_loss: 0.1170\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1350 - val_loss: 0.1178\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1330 - val_loss: 0.1107\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1318 - val_loss: 0.1136\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1313 - val_loss: 0.1103\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1305 - val_loss: 0.1159\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1299 - val_loss: 0.1131\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1294 - val_loss: 0.1249\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1294 - val_loss: 0.1139\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1291 - val_loss: 0.1117\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1285 - val_loss: 0.1122\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1283 - val_loss: 0.1120\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1283 - val_loss: 0.1125\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1282 - val_loss: 0.1096\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1280 - val_loss: 0.1117\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1281 - val_loss: 0.1096\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1279 - val_loss: 0.1134\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1278 - val_loss: 0.1113\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1276 - val_loss: 0.1138\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1272 - val_loss: 0.1175\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1274 - val_loss: 0.1135\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1280 - val_loss: 0.1103\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1273 - val_loss: 0.1175\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1272 - val_loss: 0.1136\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1271 - val_loss: 0.1096\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1271 - val_loss: 0.1126\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1267 - val_loss: 0.1090\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1267 - val_loss: 0.1102\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1269 - val_loss: 0.1174\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1268 - val_loss: 0.1101\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1265 - val_loss: 0.1095\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1266 - val_loss: 0.1096\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1263 - val_loss: 0.1082\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1261 - val_loss: 0.1088\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1263 - val_loss: 0.1099\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1258 - val_loss: 0.1087\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1264 - val_loss: 0.1081\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1266 - val_loss: 0.1121\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1263 - val_loss: 0.1115\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1262 - val_loss: 0.1099\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1259 - val_loss: 0.1086\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1262 - val_loss: 0.1081\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1260 - val_loss: 0.1104\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1258 - val_loss: 0.1097\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1258 - val_loss: 0.1084\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1259 - val_loss: 0.1099\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1255 - val_loss: 0.1093\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1256 - val_loss: 0.1110\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1257 - val_loss: 0.1092\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1256 - val_loss: 0.1131\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1256 - val_loss: 0.1107\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1258 - val_loss: 0.1134\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1253 - val_loss: 0.1082\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1256 - val_loss: 0.1121\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1254 - val_loss: 0.1091\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1252 - val_loss: 0.1097\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1250 - val_loss: 0.1129\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1252 - val_loss: 0.1073\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1254 - val_loss: 0.1107\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1249 - val_loss: 0.1089\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1249 - val_loss: 0.1116\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1244 - val_loss: 0.1078\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1248 - val_loss: 0.1102\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1245 - val_loss: 0.1067\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1252 - val_loss: 0.1107\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1249 - val_loss: 0.1094\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1249 - val_loss: 0.1097\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1245 - val_loss: 0.1112\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1246 - val_loss: 0.1121\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1246 - val_loss: 0.1095\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1243 - val_loss: 0.1079\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1240 - val_loss: 0.1077\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1244 - val_loss: 0.1106\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1239 - val_loss: 0.1080\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1236 - val_loss: 0.1106\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1242 - val_loss: 0.1079\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1240 - val_loss: 0.1179\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1236 - val_loss: 0.1066\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1236 - val_loss: 0.1105\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1235 - val_loss: 0.1081\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1236 - val_loss: 0.1117\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1229 - val_loss: 0.1095\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1232 - val_loss: 0.1093\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1235 - val_loss: 0.1073\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1231 - val_loss: 0.1067\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1231 - val_loss: 0.1102\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1232 - val_loss: 0.1093\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1227 - val_loss: 0.1072\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1231 - val_loss: 0.1109\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1230 - val_loss: 0.1128\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1220 - val_loss: 0.1158\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1225 - val_loss: 0.1107\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1223 - val_loss: 0.1084\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1226 - val_loss: 0.1125\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1227 - val_loss: 0.1093\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1218 - val_loss: 0.1107\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.1219 - val_loss: 0.1105\n",
      "154/154 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "# Train one model with a varying number of epochs\n",
    "df = pd.DataFrame()\n",
    "df['Epochs'] = list()\n",
    "df['MAE'] = list()\n",
    "for epoch in range(10, 110, 10):\n",
    "    print(f\"{epoch}\")\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data('7871.csv')\n",
    "    model = define_model()\n",
    "    model.fit(X_train,y_train,epochs=epoch,validation_data=(X_test,y_test),batch_size=128)\n",
    "        \n",
    "    predictions = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test * test_norms[2], predictions * train_norms[2])\n",
    "        \n",
    "    df.loc[len(df)+1] = [int(epoch), mae]\n",
    "df.to_csv('Data\\Raw Experiment Data\\epoch experiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "36b14b9e73fe7e0b",
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
=======
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAG0CAYAAADehEiZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABPhElEQVR4nO3deVxU5eIG8GeG2diGxRQtTVERXFHCxDRJlEq9dNW6N8vtKpXlkj8NrwqllaGVBpaWgBllhJl7LtQVtaRuVqJXywVScaFcWGRfZpg5vz+A0ZFtBmcY4Dzfz4fPwHveOfMejuXjux2JIAgCiIiIiERIausGEBEREdkKgxARERGJFoMQERERiRaDEBEREYkWgxARERGJFoMQERERiRaDEBEREYkWgxARERGJFoMQERERiRaDEBEREYkWgxARERGJFoMQERERiRaDEBEREYkWgxARERGJFoMQERERiRaDEBEREYkWgxARERGJFoMQERERiRaDEBEREYkWgxARERGJlszWDSAiup1eEHClSItirQBHuQSdnOSQSiS2bhYRtVIMQkTUbKTllSM5sxiFWr2hzFkuxciOjvB2VdqwZUTUWkkEQRBs3QgiorS8cuzIKKzz+DhPZ4YhIrI4zhEiIpvTCwKSM4vrrZOcWQw9/91GRBbGIERENnelSGs0HFabQq0eV4q0TdQiIhILBiEisrlirWk9PabWIyIyFYMQEdmco9y0VWGm1iMiMhWDEBHZXCcnOZzl9f/vyFkuRScneRO1iIjEgkGIiGxOKpFgZEfHeuuM7OjI/YSIyOK4fJ6Img3uI0RETY1BiIiaFe4sTURNiUGIiIiIRItzhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLRsGoTy8vKwZMkSDBs2DH5+fnjmmWdw9OjROutnZWVh/vz5GDRoEAYNGoS5c+fi2rVrRnVOnjyJiRMnol+/fggMDMQHH3wAvV5v7UshIiKiFsimQWj+/Pk4ceIEoqKisHXrVvTu3RuhoaE4f/58rfXnzZuHq1evIj4+HvHx8bh27RpmzpxpOJ6RkYEpU6bg/vvvx65du7Bo0SLEx8djw4YNTXVJRERE1ILIbPXBly5dwo8//ohNmzbBz88PABAREYHDhw9jz549mDt3rlH9goIC/Prrr1i3bh169eoFAHjhhRcwc+ZM3Lx5E25uboiNjUX37t2xfPlySCQSeHp64o8//sCxY8ea/PqIiIio+bNZEHJzc0NcXBz69OljKJNIJBAEAfn5+TXqK5VKODg4YOfOnXjwwQcBALt27UKXLl3g4uICAEhJScHzzz8PiURieN/LL79s5SshIiKilspmQUitViMwMNCoLCkpCZcvX8bQoUNr1FcqlYiMjMSbb74Jf39/SCQStG3bFgkJCZBKpSgqKkJ2djacnZ0RHh6Ow4cPQ61WY+zYsQgNDYWdnV2dbdFoNNBoNHUel0ql9b6fiIiImh+lUtlgHYkgCEITtKVBqampeO655zB48GB89NFHNY4LgoDVq1cjPT0dzz33HHQ6HaKjo1FUVIRNmzahqKgIgYGBcHFxwZQpUzBixAicOXMGkZGRmDJlSo2httutWbMGa9eurfO4n58ffHx8LHKdrYsEkjYdIFE5QigrhpBzFUCz+ONERESEpUuXNlinWQSh5ORkhIWFwdfXF7GxsVCpVDXq7NmzB2+88QYOHToEJycnAEB+fj6GDx+OuXPn4m9/+xseeughjBo1CqtXrza87+OPP8aHH36IY8eOGQ2Z3Y49Qub7o0CL766Vo6ji1h8fJ5kEj7RXwkstt2HLiIiIKpnSI2SzobFqCQkJiIyMRHBwMFatWgWFQlFrvdTUVHh6ehpCEAC4uLjA09MTFy9ehKurK5RKJXr06GH0Pi8vL5SUlCA3Nxdt2rSp9dwKhaLOz6Wa0vLKsSezrEZ5UYWAPZllGOcph7drw3/4iIiIbM2my+cTExOxbNkyTJw4EatXr643jHTo0AGXLl1CeXm5oay0tBSZmZno3Lkz7Ozs4OfnhxMnThi9Ly0tDWq1Gq6urta6DFHRCwKSM4vrrZOcWQy97TsaiYiIGmSzIJSRkYHly5cjODgYM2bMQE5ODrKyspCVlYXCwkLodDpkZWWhrKyy52Hs2LEAgP/7v//D2bNncfbsWcybNw8KhQLjx48HALz00ktISUnBmjVrcPnyZSQlJSEuLg5Tp07l0JaFXCnSolBb/waVhVo9rhRpm6hFREREjWezOUIxMTGIjo6u9di4ceMwe/ZsjBgxAitWrDAEnfPnz2PlypU4fvw4pFIp/P39sXDhQnTs2NHw3pSUFERHRyM9PR1t27bFM888g+eeew5SKZ8mYgmnc8vx9aXCBus90dkZvdw5PEZERM1bs5gsTS3HpUINNp0raLDeM93V6OzMeVdERNS8sZuEzNLJSQ5nef1/bJzlUnRy4soxIiJq/hiEyCxSiQQjOzrWW2dkR0dI69iqgIiIqDnh0Bg1SlpeOZIzi40mTjvLpRjZ0ZFL54mIqMVgEKJG0wsCrhRpUawV4CiXoJOTnD1BRETUojAIERERkWhxjhARERGJFoMQERERiRaDEBEREYkWgxARERGJFoMQERERiRaDEBEREYkWgxARERGJFoMQERERiRaDEBEREYkWgxARERGJFoMQERERiRaDEBEREYkWgxARERGJFoMQERERiZbM1g0gIiIi8dELAq4UaVGsFeAol6CTkxxSiaTJ28EgRERERE0qLa8cyZnFKNTqDWXOcilGdnSEt6uySdsiEQRBaNJPJCIiItFKyyvHjozCOo+P83Ru0jDEOUJERETUJPSCgOTM4nrrJGcWQ9+EfTQMQkRERNQkrhRpjYbDalOo1eNKkbaJWsQgRERERE2kWGtaT4+p9SyBQYiIiIiahKPctFVhptazBAYhIiIiahKdnORwltcfPZzlUnRykjdRixiEiIiIqIlIJRKM7OhYb52RHR2bdD8hLp8nIiKiJsV9hIiIiEjUmsvO0gxCREREJFqcI0RERESixSBEREREosUgRERERKLFIERERESixSBEREREoiWzdQOIiIiaQnNZrk3Ni02DUF5eHqKiovDdd9+hqKgI3t7eeOWVV+Dv719r/aysLKxYsQI//vgjACAgIACLFy9G+/bta9TVaDR48skn0bt3b7z99ttWvQ4iImremtMGftS82HRobP78+Thx4gSioqKwdetW9O7dG6GhoTh//nyt9efNm4erV68iPj4e8fHxuHbtGmbOnFlr3XfffRfp6enWbD4REbUAaXnl2JFRaBSCAKBQq8eOjEKk5ZXbqGXUHNgsCF26dAk//vgjli5dCn9/f3Tt2hURERHw8PDAnj17atQvKCjAr7/+iueffx69evVCr1698MILL+DUqVO4efOmUd2UlBQkJSXBy8urqS6HiIiaIb0gIDmzuN46yZnF0HNvYdGyWRByc3NDXFwc+vTpYyiTSCQQBAH5+fk16iuVSjg4OGDnzp0oKipCUVERdu3ahS5dusDFxcVQLzc3F4sXL8ayZcvg5ubWJNdCRETN05UibY2eoDsVavW4UqRtohZRc2OzIKRWqxEYGAiFQmEoS0pKwuXLlzF06NAa9ZVKJSIjI/HLL7/A398fAwcOxP/+9z+sX78eUumty4iIiMDw4cMRFBTUJNdBRETNV7HWtJ4eU+tR69NsVo2lpqYiPDwcI0aMqDXECIKAtLQ0DBgwAM899xx0Oh2io6Mxa9YsbNq0CU5OTvjyyy9x/vx5vPfee2Z9tkajgUajqfO4VCqFnZ2d2ddERES2pUCFyfXKOVWo1VEqG54I3yyCUHJyMsLCwuDr64uoqKha6+zduxeJiYk4dOgQnJycAAAxMTEYPnw4tm3bhocffhgrV67Ehg0b4ODgYNbnx8bGYu3atXUe9/Pzg4+Pj1nnJCKi5kACRfBkQOUESS1L5QVBAEqL8MWajwCwV6i1Wbp0aYN1zHr6vF6vxzfffIODBw/i5MmTyMrKgp2dHdq2bYv+/fsbenNuH6pqSEJCAiIjIxEcHIxVq1YZDZXd7o033sCpU6fw1VdfGZU/+eST6NevH9q0aYO1a9fC3t7ecKysrAxSqRQKhQJ79+7FvffeW+u52SNERNR6/VGgxZ7MsjqP/62jCl5qeRO2iJqKRXuE9uzZg+joaBQUFOChhx7C2LFj4e7uDp1Oh5s3b+LUqVOIiIiAWq3Gyy+/jJCQkAbPmZiYiGXLlmHy5MkIDw+vN0B16NAB+/btQ3l5ueHCSktLkZmZiZCQEIwdO7bGZ4aFhaF9+/YICwtDu3bt6jy3QqGoM4AREVHL1qetEnK5nPsIUa1M6hGaNWsWCgoKMG3aNDz88MOQy2tPzhUVFfjPf/6DjRs3ws3NDevWravznBkZGQgJCcEjjzxSo+tKpVLBwcEBubm5cHZ2hkqlwo0bNxASEgI/Pz/MnTsXALB69WqcOnUKe/fuhVqtrvEZkydPxn333ccNFYmIiDtLU61M6hEaO3YsgoODGz6ZTIbRo0dj9OjR+Oabb+qt++2330Kr1WL//v3Yv3+/0bFx48Zh9uzZGDFiBFasWIHx48ejXbt2SExMxMqVKzF16lRIpVL4+/tj06ZNtYYgIiKi20klEnR2Zu8/GTNrjhARERFRa9KofYR2796Na9euAQA++ugj/O1vf8OSJUtQzrWHRERE1IKYHYQ++ugjRERE4K+//sLx48fxwQcfYMCAAfj555+xatUqa7SRiIiIyCrMDkLbtm3DO++8Az8/P/znP/9B//79sWzZMkRGRjY4L4iIiIioOTE7CN24cQMDBgwAAPz3v/81PA6jQ4cOKCgosGzriIiIiKzI7CDUvn17ZGRk4PLly0hLS8OQIUMAAEePHkX79u0t3kAiIiIiazH7ERsTJkzA3LlzoVQq4e3tjQEDBuCLL77AypUrMWfOHGu0kYiIiMgqGrV8/uDBg7hy5QqeeOIJuLm54euvv0Z5eTn+8Y9/WKONRERERFZhUhBasmQJhg0bhoceesjsB5oSERERNVcmzRHq2LEj4uPjMWTIEEybNg2ffvopLly4YO22EREREVmVWUNjhYWFSElJweHDh/HDDz9ApVJh2LBhCAwMREBAgElPeSUiIiJqLu7qERu//fYbDh8+jMOHDyM9PR3+/v5Yv369JdtHREREZDUWe9bYzZs38cMPPyAkJMQSpyMiIiKyOrOXzwPA999/j/T0dD5bjIiIiFo0s3uE3nrrLSQkJOCee+6BQqEwPplEggMHDli0gURERETWYnYQGjRoEObPn4+nn37aWm0iIiIiahJmP2JDJpPhwQcftEZbiIiIiJqU2UFo0qRJWLduHTQajTXaQ0RERNRkzB4au3jxIp5++mmUlJSgbdu2kEgkRsc5R4iIiIhaCrNXjS1atAhqtRpPPfUU7O3trdEmIiIioiZhdhA6ffo0vvrqK/j4+FijPURERERNxuw5Qp06deL8ICIiImoVzJ4j9PPPP+Odd97B3Llz4enpCZnMuFPp3nvvtWgDiYiIiKzF7CDUu3dv6HS6yjffNlFaEARIJBKcOXPGsi0kIiIishKz5wjFx8dbox1ERERETc6kHqHS0lKzV4g15j1ERERETcmkydJPPvkktm/fDr1e32BdrVaLLVu2YPz48XfdOCIiIiJrMqlH6Nq1a3jttddw+vRpPPbYYxg+fDh69OgBd3d3CIKA3Nxc/P777zhy5Aj27t2Lnj174q233uLEaSIiImrWzJosfeTIEcTHx+O///0vKioqjI4pFAo89NBDmDJlCgYPHmzxhhIRERFZmtmrxgCgrKwMv//+O7KzsyGRSODh4QFvb2/OCSIiIqIWpVFBiIiIiKg1MHtnaSIiIqLWgkGIiIiIRMvsDRWJiIiI7kaFXkCpTo+yCgGuSjvIpZKG32QlZgehuLg4/P3vf4eHh4c12kNEREQtRIVeQGmFHqU6AWUVt8LN7a+lFQLKdJX1ql8rbpud3EZlh+d7utnsGhoVhB577DFrtIWIiIhswBKBxlwSAPYyCbo4yy12HY1hdhDy9fXFwYMHMW3aNGu0h4iIiBrJloFGZSetepXAXiY1vNrbSaCqeq0uV8kkUEolRg9vtxWzl8/PmTMHycnJUKvV6NKlC5RKpdHxjRs3WrSBREREYiMIArR6oKRCbwgvJRV6lFRUBpeSCusFGns7KVS3BZrbg8ydgcZeJoGimQSaxjK7R8jJyQljx461QlOIiIhaJ70gGMJMaYWAEp2+KtDcVlZRWVb9fWNDjRSAqoFAc3uPTWsJNI1l0w0V8/LyEBUVhe+++w5FRUXw9vbGK6+8An9//1rrZ2VlYcWKFfjxxx8BAAEBAVi8eDHat28PANDr9fjkk0+wZcsWXL9+Hffddx/+9a9/4R//+EeTXRMREbVugiBAo78j2FT30ujuDDaVr2W6xv1VaycBHGRSOMgqQ4uDrHL4yYGBxmIatXz+2rVr+OKLL5CWlgaZTAYvLy88/fTTZj9kdf78+cjJyUFUVBTc3d2RmJiI0NBQbN++Hd26datRf968edDpdIiPjwcAvPHGG5g5cya2b98OAIiNjUV8fDzeeOMN9O7dG0eOHMEbb7wBmUyGcePGNeZSiYioldNV9dbcPuRkGIbS6VGirRlwGplrDOHlVrCpCjXVZXZSOMirXmVSyKVgqLEys3uE0tPTMWnSJKhUKvTr1w86nQ6///47SktLsWnTJnh5eZl0nkuXLuHRRx/Fpk2b4OfnB6AyZT/22GMYM2YM5s6da1S/oKAAAwcOxLp16xAUFAQAOHDgAGbOnIkjR47Azc0NgYGBeOaZZ/Diiy8a3hcREYGLFy/iiy++MOcyiYiohRKEyrkzxRV6FGv1KK4QUKytDDmVr1UBpyrslDcy1ciqemuqe2iMemvuCDgOVcNUUoaaZsfsHqF3330XAQEBWLVqFRQKBQCgvLwcCxYswKpVqxAbG2vSedzc3BAXF4c+ffoYyiQSCQRBQH5+fo36SqUSDg4O2LlzJx588EEAwK5du9ClSxe4uLhAr9fj7bffhqenZ4331nY+IiJqOQRBQKlOqAo2epRoq4KOIezcCjnFFXroG5Ftbh9yqivY3P69LTcBJMsxOwilpqZi8+bNhhAEVIaUmTNnYtKkSSafR61WIzAw0KgsKSkJly9fxtChQ2vUVyqViIyMxJtvvgl/f39IJBK0bdsWCQkJkEornxQyePBgo/dkZmZi7969mDBhgjmXSERETUCoGpIyDjS3h52q8goBJVo99GaeX2UngaOscqjJUSaFo1xa+bNRsLk1v4a9NeJkdhBydHSERqOpUV5bmTlSU1MRHh6OESNGGIa+bicIAtLS0jBgwAA899xz0Ol0iI6OxqxZs7Bp0yY4OTkZ1c/KysILL7yANm3a4KWXXqr3szUaTb3tl0qlsLOza9yFERGJiN4wLCVUrYgSUFI1TFVyW1mxrjIEmdtxo7IDHOyq59ZI4Fj1emeZvZ0EsgZ7bKqilU4Hra4xV0vN3Z1b/NTG7CAUEBCAd999Fx988AFcXV0BALm5uVi1ahUCAgLMbiQAJCcnIywsDL6+voiKiqq1zt69e5GYmIhDhw4ZQk9MTAyGDx+Obdu2YerUqYa6Fy5cwAsvvACtVovPP/8cLi4u9X5+bGws1q5dW+dxPz8/+Pj4NOLKiIhaCaU9JEoHQOkAidH3lT9Xfw+lChKJec/zFspLIZSXAFWvt39f+VoMobwUKC9FuaAHJzuQqZYuXdpgHbODUFhYGCZMmIDhw4ejS5cukEgkyMjIgFqtRkJCgtmNTEhIQGRkJIKDg43mHd0pNTUVnp6eRj0/Li4u8PT0xMWLF43qvfTSS2jbti0+//xzdOjQocE2zJgxo96dstkjREStlUYvoEgroKhCj+Kq16KKyrLiCn3Vq2D2sFTlPJuqHprbvneQSSt7carK7GUS2EmcrXJtRKYwOwh5eHhg79692LVrF/744w8IgoCnnnoKISEhcHY27w9zYmIili1bhsmTJyM8PNww16c2HTp0wL59+1BeXm7o6iotLUVmZiZCQkIAACdPnsRzzz2HXr164aOPPmqwJ6iaQqGoM4AREbVEekFAkVaPIq0ehXe+am79rDFjVnHlsNOtuTaVr1Xh5rYyB66OohbE7OXz48ePx/Lly+96qCgjIwMhISF45JFHanRdqVQqODg4IDc3F87OzlCpVLhx4wZCQkLg5+dnWFq/evVqnDp1Cnv37oWDgwNGjx4NvV6P+Ph4qFQqw/ns7Ozg7u5+V+0lImoOBKFyufft4cYo6FSFnOIKvcnzb5RSCZzkUjjJpXBWSOFc9b2TvPJ7Z3llwGG4odbI7B6hP//8Ew4ODnf9wd9++y20Wi3279+P/fv3Gx0bN24cZs+ejREjRmDFihUYP3482rVrh8TERKxcuRJTp06FVCqFv78/Nm3aBLVajWPHjuHSpUsAgJEjRxqd77777sPBgwfvus1ERNZUoa+nF0erM/TmmProBSlgFHCqg43xqx0Udgw4JF5m9wh9/PHH+P777xEaGor777/fqOcFgNm7SxMRtXbVy8QLawk2t/fqlJrxcCmVncQ41CiMw41z1RAVdyUmqp/ZQej2IbHb/wMTBAESiQRnzpyxXOuIiFoAnb4y5ORpdCjQ6JGv0SG/6rWgaqjK1M2L7SQwGpK61aNjZ/QzN/Mjsgyzh8Y2btxojXYQETVbFXrBKOAU3BZ08jWVvTmmcJBJjHptDENWsltzc1R27MUhakpmB6H4+HiEhYXV+lBUIqKWSKsXDL03ht6c8urQo0dRRcNBRyYB1Ao7uCikcKl6VVd9Xx127NiLQ9TsmB2Ejh49atJOjUREzUW5Tl8Vcmrv1SkxYW6OXApDwHFR2BlCTvXPnI9D1DKZPUdo+fLluHHjBmbNmoXOnTtz/x0isrkynR755fqavTpVr2UmTNBRSiW3wo1SCrVcChelHVyrQo89h6yomdALAq4UaVGsFeAol6CTk5xbG9wFs4NQUFAQ/vrrrzr/h8DJ0kRkSULVs6vyawk41b065SYEHZWdpM7eHBeFFCqZeY+FILKFtLxyJGcWG81Lc5ZLMbKjI7xdOVrTGGYPjc2ZM8ca7SAiEROqdkHOLdcht1yHnDIdbpbrDMNZpux+7CCT1Jijcyv0SKG0Y9Chli0trxw7MgprlBdq9diRUYhxnmAYagSze4SIiBqrXKfHzXI9cst0yCmvQG5ZZfC5Wd5w2HGUSeqco6NWcFNAat30goB1p27Wu0LRWS7FS73dOExmJpN6hF5//XUsWLAAjo6OhrI//vgDnp6ekMkqT3Hz5k1MnDgR+/bts05LiahF0AuVw1g5VSGnOuzkllduIFgXCQBXpRTuSrvKL1Xl/Jzq0CPjiisSsStF2ga3aSjU6nGlSIvOzpy7aw6TgtDmzZsxZ84coyD09NNPY9euXejUqRMAQKfTISMjwzqtJKJmpXqn5JzyO8JOmQ43NTrU17njIJMYgk6bqlf3qknJXF5OVLtirWmDN6bWo1tMCkK1jZ5xRI2o9dPqBdy8o1en+vv6VmLJJIBbLWHHXWnHSclEjeAoN+0fCabWo1vMnixNRK2LIAgo0OprhJ2cqsnK9VErpGijtIOb0g5tVLeGtNRyKZeaE1lQJyc5nOXSBucIdXKSN2GrWgcGISKRKKvQ1+jVqV6dVd9+gko7Sa1hx01px+ddETURqUSCkR0da101Vm1kR0dOlG4EBiGiVkQvVA1l3darUx166ts9WSoB3BR2cKtlKIs7JhM1D96uSozzBPcRsjCTg9C1a9dQXl5uVHb9+nXY2dkBAHJycizbMiKqV4VeQHaZDtdLKnC9tALXSipwo7Si3t4dJ5n0Vsi5Ley4KqX8lyRRC+DtqoSXi4I7S1uQSfsI+fj41PgXoSAIRmXVP3NnaSLL0+oF3CitwPWSClyres0qq311llwKoyXo7ko7tFHJ4KbkpoJERHcyqUdo48aN1m4HEVUp0+lxo0RnCDzXSyuQU6ZDbf9iUdlJ4GEvQ3sHGTzsZfBwqAw+HMoiIjINd5YmsqGSCr3R0Nb10grcLK99VYijTAIPBxna28vgURV8XBRcnUVEdDc4WZqoCQiCgKIKPa6X6AyB53pJBQrqWAqrlksrQ09V4GnvIIOTnMNaRESWxiBEZGFC1SMmDENbVcGnuI5ZzG5KqVEvj4eDDA7cdJCIqEkwCBHdherl6tdvm9NzrbQC5bXsuiwB0EZld2tOj4MMHvZ2nMBMzZ5eELhKiVotBiEiE+kEATllxkNb10srUNvollQCtFXZGQ1ttbWXcQNCanHS8sq5bw21ao2aLF1WVoZvvvkG58+fR2hoKNLT09G9e3e4u7tbo41ETa5CLyCrtHqpug7XSyv36Knt8VoyCYyGtTzsZWir4gNEqeVLyyuvdyfjcZ7ODEPU4pndI5SdnY0JEyYgOzsbGo0G//znP/HJJ5/gt99+w8aNG9GtWzdrtJPIagRBwPVSHTKLtZVDWyUVyK5jubpSKkE7BzvDnJ729jK4q+w4TECtjl4QkJxZXG+d5MxieLko+OefWjSze4TCwsJQVFSE6OhoPPTQQ/j666+hVqsxf/582NnZIS4uzlptJbKYIq0eGQUaXCzUIqNQU+vjJ+ztJIa5PNXBx5XL1UkkLhVqsOlcQYP1numuRmdnRRO0iMg6zO4ROnLkCOLi4mBvb28oc3FxwYIFCzBlyhSLNo7IUrR6AZlFWmQUapFRoEFWmc7ouFxa+XTnDrfN6XHmE9RJxIq1pv0b2dR6RM2V2UGouLjYKATdrqKi4q4bRGQJglD5HK7q4HOlSFvjGVztHWTwdJbD01mB+xxlnNNDdBtHuWn/PZhaj6i5MjsIDRw4EF988QVeffVVQ5lWq8WHH34IPz8/izaOyBwlWr1hqCujUIuiO5ZzOcmllcFHrUAXJzkcuEEhUZ06OcnhLJcarRa7k7Ncik5O8iZsFZHlmT1H6Pz585g4cSLatWuHCxcuYNCgQbhw4QIKCwuRkJAAHx8fa7WVyIhOLyCzWFsZfgq0uFZq3CMpkwD3O8nRRa2Ap7Mc96j4DC4ic3DVGIlBo5bP37hxA4mJiThz5gz0ej28vLzw7LPPomPHjtZoIxGAyuGu3PJbw12Xi7Q19vBpZ28HT+fK4NPRSQ4Zh7uI7gr3EaLWzuwgtHbtWoSGhtaYJ1RUVIT3338fERERFm0giVtZhfFwV4HGOPk4yCSVwUctRxdnBZ/HRWQF3FmaWjOTgtD58+eRm5sLAJgyZQrWrFkDFxcXozrp6el49913ceLECeu0lERBLwj4q7iiMvgUaHG1pMJoPx87CdDRUQ5PdeUk53b2HO4iIqLGM2my9JUrV/Diiy8a/sKZPXt2rfWefPJJy7WMRCOvXGcIPpcKtSjXG2fze1R26FK1uquTkxwKOwYfIiKyDJOHxv766y/o9XqMHDkSW7ZsMXqchkQigYODA1xdXa3VTmpFynV6XCrUGoa8bpYbD3ep7CTwdL41yVmtsLNRS4mIqLUze47Qn3/+iXvvvZfDEWQyvSDgWkmFYZLzX8UVuD36SAHc5yQzTHL2cJBx/gERETUJs/cR2rFjR73H6xo2I3Ep0Nxa3XWxUIuyO55W6qaUGiY53+8kh9KOk5yJiKjpmR2Etm/fbvRzRUUFcnNzIZfLMWDAAIs1jFoWja5yVUn16q6cOx5hobSToLPTrUnOrkoOdxERke2ZHYQOHjxYo6yoqAgLFy7EoEGDLNIoahkq9AJ+yy3D2ZsaZBZrcXunjwRABweZIfjc68jhLiIian4ataFibdLT0zFjxgwcOnTI5Pfk5eUhKioK3333HYqKiuDt7Y1XXnkF/v7+tdbPysrCihUr8OOPPwIAAgICsHjxYrRv395QJykpCWvWrMGVK1fQpUsXLFiwAMOGDbu7iyMjZTo9jmeV4WhWKYpve4CXWiFFV2cFuqjl6OIkh0rG4S4iImrezO4Rqkv1EJk55s+fj5ycHERFRcHd3R2JiYkIDQ3F9u3b0a1btxr1582bB51Oh/j4eADAG2+8gZkzZxqG644cOYIFCxZg0aJFGDx4MLZu3YpZs2Zh586dtZ6PzFOk1ePojVIczy4zLHFXy6V4oK0KXi5KuCn5tHYiImpZzO4R2rlzp9HPgiCgsLAQmzdvRtu2bfHpp5+adJ5Lly7h0UcfxaZNmwwPaxUEAY899hjGjBmDuXPnGtUvKCjAwIEDsW7dOgQFBQEADhw4gJkzZ+LIkSNwc3NDaGgo1Go1oqOjDe+bMGECevTogTfffNOcy6Tb3CzX4ZcbpTiZU2YY/rpHZYcAD3v0dFPCjuGHiIhaKLN7hBYtWlTzJDIZ/Pz8sHTpUpPP4+bmhri4OPTp08dQJpFIIAgC8vPza9RXKpVwcHDAzp078eCDDwIAdu3ahS5dusDFxQV6vR7Hjh2r0b5BgwZh//79JreLbrleUoEj10twNk9j2N35PkcZAjzs0V2tYO8PERG1eGYHobNnz1rkg9VqNQIDA43KkpKScPnyZQwdOrRGfaVSicjISLz55pvw9/eHRCJB27ZtkZCQAKlUiry8PJSUlBjNFwKAdu3a4erVqxZpsxgIgoArRZUB6EKh1lDeTS1HgIcDOjrKGICIiKjVsNgcobuVmpqK8PBwjBgxwjD0dTtBEJCWloYBAwbgueeeg06nQ3R0NGbNmoVNmzahrKwMAKBQKIzep1QqUV5eXu9nazQaaDSaOo9LpVLY2bXu5d6CIOBCkQ6/ZpfjamnldocSAN5qGfzvUaCtyg6Avt7fExERUXOiVCobrGNSEAoKCjK5F+DAgQMm1btdcnIywsLC4Ovri6ioqFrr7N27F4mJiTh06BCcnJwAADExMRg+fDi2bduGJ554AgBq/EVdXl4Oe3v7ej8/NjYWa9eurfO4n58ffHx8zLmklkMihfQ+L9h194NUXfnYFEFXAf3lM6g4/z+cKCkAH6NLREQtkSlTdkwKQuPGjbPacEhCQgIiIyMRHByMVatW1ejRqZaamgpPT09DCAIAFxcXeHp64uLFi3B1dYWDgwNu3Lhh9L4bN27UGC6704wZMzBt2rQ6j7fGHiGtXsDvN7VIzdWgUFs5A0ghBXzdFBjQxhGOfR8C8JBtG0lERGRlJgWhOXPmWOXDExMTsWzZMkyePBnh4eGQSuved6ZDhw7Yt28fysvLDV1dpaWlyMzMREhICCQSCfz8/PDLL7/gH//4h+F9P//8Mx544IF626FQKOoMYK1NaYUeqVllSM0qRWnVEjBHmQQD29mj/z0qqPioCyIiEpFGbah46tQpbNiwAWlpaZDJZOjevTumTp2Kfv36mXyOjIwMhISE4JFHHqnRdaVSqeDg4IDc3Fw4OztDpVLhxo0bCAkJgZ+fn2Fp/erVq3Hq1Cns3bsXarUaP/zwA1544QXDJorbtm3DF198Uee+RGJSoNHh1xul+F9OGbRVTzx1VUgxyMMefd1VkEk5Abql0wuVjzkp1gpwlEvQyUnO3byJiBpgdhA6evQopk2bhh49esDf3x86nQ7Hjh1Deno6PvvsswZ7X6rFxMQY7fdzu3HjxmH27NkYMWIEVqxYgfHjxwMAzp8/j5UrV+L48eOQSqXw9/fHwoUL0bFjR8N7d+7ciY8++gjXrl1D9+7dsWDBAgwePNicS2xVcsoq8PP1Uvx+sxxVeyCinb0dBns4wNtVwb8oW4m0vHIkZxajsDrlAnCWSzGyoyO8XRueLEhEJFZmB6Fnn30WPj4+WLJkiVH5G2+8gXPnzuHzzz+3aAOpca4Wa/HT9VKk59+aPN7JSYbBHg7wdJZzCXwrkpZXjh0ZhXUeH+fpzDBERFQHs5fPnzp1Cm+99VaN8kmTJuGpp56ySKOocQRBwKXCygB0qejWHkBeLgoEeNjjPke5DVtH1qAXBCRnFtdbJzmzGF4u7P0jIqqN2UHIzc0NOTk56Nq1q1F5Tk6OaCYcNzd6QUB6ngZHrpfiWmkFAEAKoJe7EgHt7HGPfbPZLoos7EqR1mg4rDaFWj2uFGnR2Zn/fRIR3cnsvyGHDx+OZcuWITo62jAB+dy5c4iMjMTw4cMt3kCqW4VewKmb5fj5eilyy3UAALkU8G2jwsB29nBRtK4l/1RTsda0kW1T6xERiY3Zc4Ty8/Mxbdo0nDlzBs7OzpBIJCgoKECPHj0QHx8Pd3d3a7WVqpTr9Phfdhl+zSpDUVVvgMpOggfaqvBAW3s4yLgEXiwuFWqw6VxBg/We6a5mjxARUS0atXxer9cjJSUFf/zxBwRBQI8ePTB06NBWt+lgc1Oi1eNoVilSs8tQXrUHkLNcWrkHUBsVFHacAyI2ekHAulM36x0ec5ZL8VJvN84RIiKqRaOC0O1yc3Pxyy+/oE+fPkbL2Mly8jU6/HKjFCeyy1BRdbfclXYY5GGPPm5K2HEPIFHjqjEiosYzOwilp6djzpw5eOutt+Dj44PRo0cjKysLCoUCcXFxCAgIsFZbRSertAJHrpfi9M1yVN+k9g4yDPaw5yogMsJ9hIiIGsfsydLvvPMOOnfujK5duyIpKQlarRbff/89EhMTsXr1anz55ZfWaKeoZBZpceR6Kc4V3NoDqIuzHAEe9ujsxD2AqCZvVyW8XBTcWZqIyExmB6Hjx49jy5YtaNOmDVJSUhAYGAgPDw889dRT+Oyzz6zRRlEQBAEXCrT46XoJMosrDOXergoM9nBAewcugaf6SSUSTogmIjKT2X+7SqVSKBQK6HQ6HDlyBBEREQCA4uJiqFQqizewtdMLAs7e1OCn6yXIKqtcAi+VAH3dlRjUzgHuKk5AJyIishazg1D//v0RExODe+65B6WlpRg2bBiuX7+OqKgo9O/f3wpNbJ20egG/5ZTh5xulyNdUzutQSCXof48KA9uq4Mw9gIiIiKzO7CD02muvYd68ebhy5QrCw8Ph7u6OZcuW4dy5c/j444+t0cZWafO5fMMQmL1MAv+29njgHhVU3AOIiIioydz18nmg8vEarq6u3EfIDNsuFCCnTIcH2qrQr40Kci6BJyIianKNCkLl5eXYvXs3/vjjDygUCvTo0QOjRo2CTMYJvURERNRymB2Erly5gmeffRZFRUXw9PSETqfDxYsXce+992L9+vXcVJGIiIhaDLOD0PTp06FQKLBy5Uo4OzsDqNxdet68ebC3t0dMTIxVGkpERERkaWYHIV9fX2zbtg3du3c3Kj9z5gyeeeYZ/O9//7Nk+4isTi8I3IiQiEikzJ7U0759e9y4caNGEMrPz4ebm5vFGkbUFPhoCiIicTNprfZff/1l+Jo8eTJeffVVHDp0CAUFBSgqKsLPP/+MpUuXYu7cudZuL5HFVD+s9M4ntxdq9diRUYi0vHIbtYyIiJqKSUNjPj4+Rs+3qn7LnWUSiQRnzpyxQjOJLEsvCFh36maNEHQ7Z7kUL/V24zAZEVErZtLQ2MaNG63dDqImdaVIW28IAip7hq4Uafn8LiKiVsykIPTggw82WOfatWv46quvTKpLZGvFWtPWCJhaj4iIWqa73gHx8OHD+PLLL/H9999DEAS8/PLLlmgXkVU5yk0b7jK1HhERtUyNCkK5ubnYunUrvvrqK/z555+QyWT4+9//junTp1u6fURW0clJDme5tME5Qp2c5E3YKiIiampmBaFff/0VmzZtwv79+6HVatGtWzdIJBIkJCTA19fXWm0ksjipRIKRHR2xI6OwzjojOzpyojQRUStnUhD6/PPP8eWXX+L8+fPo2LEjpk2bhjFjxsDb2xu9e/eGo6OjtdtJZHHerkqM8wT3ESIiEjGTglBkZCS6du2KdevWYfjw4dZuE1GT8XZVwstFwZ2liYhEyqQNFWfMmIGSkhLMnDkTTzzxBD766CNcvHjRyk0jahpSiQSdnRXo5a5EZ2cFQxARkYiY/KwxQRCQkpKC7du34+DBg9BqtfDx8UFaWho2bdrEOUJERETU4pj90FWg8rliu3fvxvbt23H69GnI5XI8/vjjmDRpEgMRERERtRiNCkK3O3v2LLZu3Yq9e/ciLy+Pj9ggIiKiFuOug1A1rVaLQ4cO4dFHH7XE6YiIiIiszmJBiIiIiKilMWnVGBEREVFrxCBEREREotXoh65mZ2dDq9XizpG1e++9964bRURERNQUzA5C//vf/7Bw4UJcvnzZqFwQBEgkEq4aIyIiohbD7MnSTz31FKRSKWbMmAFnZ+caxx988EGLNY6IiIjImszuEUpLS8NXX32Fnj173vWH5+XlISoqCt999x2Kiorg7e2NV155Bf7+/jXqrlmzBmvXrq31POPHj8eKFSsAALt370ZMTAwyMzNx3333ITQ0FE8++eRdt5WIiIhaH7ODUIcOHaDVai3y4fPnz0dOTg6ioqLg7u6OxMREhIaGYvv27ejWrZtR3enTp2PChAlGZVu3bkVMTAymTp0KAPjpp5+waNEivPbaaxgyZAgOHz6MV199Fe7u7nxYLBEREdVg9qqxmTNnYvny5UhLS7urQHTp0iX8+OOPWLp0Kfz9/dG1a1dERETAw8MDe/bsqVHf0dERbdu2NXyVlpYiNjYWixYtgo+PDwDg4MGD8Pb2xoQJE9CpUydMnDgRPj4++OGHHxrdTiIiImq9zO4R+uCDD3Djxg2MHTu21uOmTpZ2c3NDXFwc+vTpYyiTSCQQBAH5+fkNvv/tt9+Gl5cXnn76aUOZq6srzp07hyNHjmDQoEH45ZdfcP78eUybNs2kNhEREZG4mB2E5syZY5EPVqvVCAwMNCpLSkrC5cuXMXTo0Hrf+9tvv+HAgQP47LPPIJXe6tSaMmUKfvvtN0ydOhV2dnbQ6XR4/vnn8cQTT9R7Po1GA41GU+dxqVQKOzs7E66KiIiImgulUtlgHbOD0Lhx4xrVmIakpqYiPDwcI0aMQFBQUL11P/30U/j6+iIgIMCo/OrVq8jLy8OSJUvg5+eHI0eOIDo6Gl27dsX48ePrPF9sbGydE7EBwM/PzzD8RkRERC3D0qVLG6zTqGeNHTx4EGlpadDpdIYyjUaDEydO4LPPPjP3dEhOTkZYWBh8fX0RGxsLlUpVZ92SkhIEBARg6dKlNVaDPf744wgJCcGsWbMMZatXr8amTZvw008/GfUe3Y49QkRERK2PVXqEoqOjERsbi3bt2iErKwseHh7Izs6GTqfDmDFjzG5kQkICIiMjERwcjFWrVkGhUNRbPyUlBXq9HsHBwUblubm5yMjIQN++fY3K+/fvj3Xr1iEvLw/u7u61nlOhUDT4uURERNT6mL1qbNeuXXjttddw+PBheHh4IDExET/88AP8/PzQqVMns86VmJiIZcuWYeLEiVi9erVJYSQ1NRW9e/eGWq02Knd1dYW9vT3S0tKMytPT06FWq+sMQURERCReZgeh7OxswyRnHx8fnDx5Eq6urpg3bx727dtn8nkyMjKwfPlyBAcHY8aMGcjJyUFWVhaysrJQWFgInU6HrKwslJWVGb3v7Nmz6NGjR80LkUoxdepUrFu3Djt37sSVK1ewc+dOxMTEYMaMGeZeJhEREYmA2UNjLi4uKC4uBgB07twZ586dA1D5sNXr16+bfJ5vv/0WWq0W+/fvx/79+42OjRs3DrNnz8aIESOwYsUKo4nO2dnZ8PX1rfWcL7/8MlxdXREbG4urV6+iY8eOWLBgQY2NGImIiIiARkyWDgsLw82bN/HWW2/h6NGjiImJweeff45du3bh888/x8GDB63VViIiIiKLMntobMGCBcjJycG3336Lxx57DEqlEkOGDMG7775reNQFERERUUvQqOXzAFBeXg6lUomysjKkpKTAw8MD/fr1s3T7iIiIiKzG7B6haidPnsSXX36JiooKeHp6WuRp9ERERERNyezJ0kVFRQgNDcWJEycgkUgwZMgQrFq1CpcuXUJ8fDzat29vjXYSERERWZzZPUJRUVGQSCTYv3+/YQfof//737C3t8e7775r8QYSERERWYvZQejQoUP497//bbR5YteuXbF06VL89NNPFm0cERERkTWZHYRyc3PRtm3bGuVOTk4oLS21SKOIiIiImoLZQahv37617iC9ceNG9OrVyyKNIiIiImoKZk+Wnj9/PqZNm4bjx4+joqIC69atw7lz53D69Gls2LDBGm0kIiIisopG7SN09uxZfPLJJzh9+jT0ej28vLwwffr0Oh99QURERNQcNXpDRSIiIqKWzqShsbVr15p8wtmzZze6MURERERNyaQeIR8fH0il0gY3S5RIJDhw4IDFGkdERERkTSb1CP3zn//E/v37AQBjxozBmDFj4OPjY9WGtWZ6QcCVIi2KtQIc5RJ0cpJDKpHYullERESiY/IcIZ1OhyNHjmDfvn1ITk6Gu7s7/va3v2HMmDHo0qWLlZvZeqTllSM5sxiFWr2hzFkuxciOjvB2VdqwZUREROLTqMnSWq0WP/zwA5KSknDgwAHcf//9GD16NMaMGYN7773XGu1sFdLyyrEjo7DO4+M8nRmGiIiImtBdrxrTaDTYsmULoqOjUVxcjDNnzliqba2KXhCw7tRNo56gOznLpXiptxuHyYiIiJqI2RsqVrt+/TqSkpLwzTff4MSJE+jcuTMmT55syba1KleKtPWGIAAo1OpxpUiLzs6KJmoVERGRuJkVhO4MP506dcKoUaPw+uuvc/J0A4q1pnW8mVqPiIiI7p5JQejTTz/FN998g5MnT+Lee+/FqFGj8Nprr6F3797Wbl+r4Sg3bbjL1HpERER090zeR0gul+Ohhx5C3759663LDRVrxzlCREREzY9JQSgoKMi0k3FDxXpx1RgREVHzwmeNNTHuI0RERNR8MAjZAHeWJiIiah4YhIiIiEi0pLZuABEREZGtMAgRERGRaDEIERERkWgxCBEREZFoMQgRERGRaDEIERERkWgxCBEREZFoMQgRERGRaDEIERERkWgxCBEREZFoMQgRERGRaDEIERERkWgxCBEREZFo2TQI5eXlYcmSJRg2bBj8/PzwzDPP4OjRo7XWXbNmDby9vWv9Wrx4saHeyZMnMXHiRPTr1w+BgYH44IMPoNfrm+qSiIiIqAWRCIIg2OrDp0+fjpycHLz22mtwd3dHYmIitmzZgu3bt6Nbt25GdYuLi1FSUmJUtnXrVsTExGDz5s3w8fFBRkYGxo0bh1GjRuGFF17A2bNnER4ejpkzZ+L5559vyksjIiKiFkBmqw++dOkSfvzxR2zatAl+fn4AgIiICBw+fBh79uzB3Llzjeo7OjrC0dHR8PPly5cRGxuLRYsWwcfHBwAQGxuL7t27Y/ny5ZBIJPD09MQff/yBY8eONd2FERERUYthsyDk5uaGuLg49OnTx1AmkUggCALy8/MbfP/bb78NLy8vPP3004aylJQUPP/885BIJIayl19+2bINJyIiolbDZkFIrVYjMDDQqCwpKQmXL1/G0KFD633vb7/9hgMHDuCzzz6DVFo5zamoqAjZ2dlwdnZGeHg4Dh8+DLVajbFjxyI0NBR2dnZ1nk+j0UCj0dR5XCqV1vt+IiIian6USmWDdWwWhO6UmpqK8PBwjBgxAkFBQfXW/fTTT+Hr64uAgABDWVFREQDgnXfewZQpU7B+/XqcOXMGkZGRKC0trTHUdrvY2FisXbu2zuN+fn6G4TciIiJqGZYuXdpgHZtOlq6WnJyMsLAw+Pr6IjY2FiqVqs66JSUlCAgIwNKlS/Hkk08aynNycvDQQw9h1KhRWL16taH8448/xocffohjx44ZDZndjj1CRERErU+L6BFKSEhAZGQkgoODsWrVKigUinrrp6SkQK/XIzg42Kjc1dUVSqUSPXr0MCr38vJCSUkJcnNz0aZNm1rPqVAoGvxcIiIian1suo9QYmIili1bhokTJ2L16tUmhZHU1FT07t0barXaqNzOzg5+fn44ceKEUXlaWhrUajVcXV0t2XQiIiJqBWwWhDIyMrB8+XIEBwdjxowZyMnJQVZWFrKyslBYWAidToesrCyUlZUZve/s2bM1en2qvfTSS0hJScGaNWtw+fJlJCUlIS4uDlOnTuXQFhEREdVgsyD07bffQqvVYv/+/Rg6dKjRV2RkJK5evYqhQ4di3759Ru/Lzs6us3dn0KBBiI2NxaFDhzB69Gi8++67eOGFFzBz5swmuCIiIiJqaZrFZGkiIiIiW+BDV4mIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItGwahPLy8rBkyRIMGzYMfn5+eOaZZ3D06NFa665Zswbe3t61fi1evLhGfY1Gg5CQECxatMjal0FEREQtlMyWHz5//nzk5OQgKioK7u7uSExMRGhoKLZv345u3boZ1Z0+fTomTJhgVLZ161bExMRg6tSpNc797rvvIj09Hb1797bqNRAREVHLZbMeoUuXLuHHH3/E0qVL4e/vj65duyIiIgIeHh7Ys2dPjfqOjo5o27at4au0tBSxsbFYtGgRfHx8jOqmpKQgKSkJXl5eTXU5RERE1ALZLAi5ubkhLi4Offr0MZRJJBIIgoD8/PwG3//222/Dy8sLTz/9tFF5bm4uFi9ejGXLlsHNzc3i7SYiIqLWw2ZDY2q1GoGBgUZlSUlJuHz5MoYOHVrve3/77TccOHAAn332GaRS4ywXERGB4cOHIygoCPHx8Sa1RaPRQKPR1HlcKpXCzs7OpHMRERFR86BUKhusY9M5QrdLTU1FeHg4RowYgaCgoHrrfvrpp/D19UVAQIBR+Zdffonz58/jvffeM+uzY2NjsXbt2jqP+/n51Rh+IyIiouZt6dKlDdZpFkEoOTkZYWFh8PX1RVRUVL11S0pKsH///hoXd+HCBaxcuRIbNmyAg4ODWZ8/Y8YMTJs2rc7j7BEiIiJqnWwehBISEhAZGYng4GCsWrUKCoWi3vopKSnQ6/UIDg42Kt+3bx+Ki4uNAk1ZWRmOHTuGb7/9Fnv37sW9995b6zkVCkWDn0tEZC69IOBKkRbFWgGOcgk6OckhlUhs3Swiuo1Ng1BiYiKWLVuGyZMnIzw8vMZ8n9qkpqaid+/eUKvVRuWTJk1CSEiIUVlYWBjat2+PsLAwtGvXzqJtJyKqT1peOZIzi1Go1RvKnOVSjOzoCG/XhuctEFHTsFkQysjIwPLlyxEcHIwZM2YgJyfHcEylUsHBwQG5ublwdnaGSqUyHDt79ix69OhR43yurq5wdXU1KlOpVHB0dETnzp2tdh1ERHdKyyvHjozCGuWFWj12ZBRinCcYhoiaCZstn//222+h1Wqxf/9+DB061OgrMjISV69exdChQ7Fv3z6j92VnZ9cIPEREzYVeEJCcWVxvneTMYugFoYlaRET1kQgC/2skIrKUS4UabDpX0GC9Z7qr0dmZcxOJbI0PXSUisqBirWn/tjS1HhFZF4MQEZEFOcpNWxVmaj0isi4GISIiC+rkJIezvP7/tTrLpejkJG+iFhFRfRiEiIgsSCqRYGRHx3rrjOzoyP2EiJoJTpYmIrIC7iNE1DIwCBERWQl3liZq/hiEiIiISLQ4R4iIiIhEi0GIiIiIRItBiIiIiESLQYiIiIhEi0GIiIiIRItBiIiIiESLQYiIiIhEi0GIiIiIRItBiIiIiESLQYiIiIhEi0GI7opGo8GaNWug0Whs3RQC70dzxHvSvPB+NC/N4X4wCNFd0Wg0WLt2Lf+n0kzwfjQ/vCfNC+9H89Ic7geDEBEREYkWgxARERGJFoMQERERiRaDEBEREYkWgxARERGJFoMQERERiRaDEN0VhUKB2bNnQ6FQ2LopBN6P5oj3pHnh/WhemsP9kAiCINjs04mIiIhsiD1CREREJFoMQkRERCRaDEJEREQkWgxCREREJFoMQmSSvLw8LFmyBMOGDYOfnx+eeeYZHD161HD8zJkzmDRpEvr3749HHnkEGzZssGFrxSUjIwMDBgzA9u3bDWW8H7axc+dOjB49Gn379sWYMWOQlJRkOMZ70rS0Wi2io6PxyCOPYMCAAXj22Wdx7Ngxw3Hej6bz0UcfYfLkyUZlDf3+9Xo9PvjgAzz88MPw9fXF9OnTcenSJes0UCAywbRp04QnnnhC+PXXX4Xz588Ly5YtE/r16yecO3dOyM3NFQYNGiREREQI586dE7Zu3Sr07dtX2Lp1q62b3eppNBph/PjxQo8ePYRt27YJgiDwftjIzp07hZ49ewqffvqpcPHiRWHt2rWCj4+PcOzYMd4TG3j//feFIUOGCCkpKcLFixeFiIgIwc/PT7h27RrvRxOKj48XvL29hUmTJhnKTPn9r1mzRhg8eLDw3XffCWfOnBGmT58uBAcHC+Xl5RZvI4MQNejixYtCjx49hNTUVEOZXq8XgoODhdWrVwsxMTHCww8/LGi1WsPx9957T3jsscds0VxRee+994TJkycbBSHej6an1+uF4cOHC2+//bZR+fTp04WYmBjeExt44oknhBUrVhh+LiwsFHr06CF88803vB9N4Nq1a0JoaKjQv39/4fHHHzcKQg39/svLy4UBAwYIiYmJhuP5+flCv379hD179li8rRwaowa5ubkhLi4Offr0MZRJJBIIgoD8/HwcPXoUAwcOhEwmMxwPCAhARkYGcnJybNFkUfj111+xefNmvPPOO0blvB9N78KFC/jzzz8REhJiVL5hwwbMmDGD98QGXF1dcejQIWRmZkKn02Hz5s1QKBTo2bMn70cTOHXqFFxcXPD111/D19fX6FhDv/+zZ8+iuLgYAQEBhuNqtRq9evXCr7/+avG2MghRg9RqNQIDA412/kxKSsLly5cxdOhQXLt2De3btzd6T7t27QAAf/31V5O2VSwKCgrw73//G6+++io6dOhgdIz3o+ldvHgRAFBSUoLQ0FAMHjwY//jHP3Dw4EEAvCe2EBERAZlMhhEjRqBv376Ijo7G6tWrcf/99/N+NIGgoCC899576NSpU41jDf3+r127BgA1/t/Wrl07XL161eJtZRAis6WmpiI8PBwjRoxAUFAQysrKamyPrlQqAQDl5eW2aGKr9/rrr6N///41eiAA8H7YQFFREQBg4cKF+Nvf/oZPPvkEQ4YMwcyZM/HTTz/xntjA+fPnoVar8eGHH2Lz5s0YP348Fi5ciLNnz/J+2FhDv//S0lIAqLWONe6PrOEqRLckJycjLCwMvr6+iIqKAgCoVCpoNBqjetV/WB0cHJq8ja3dzp07cfToUezevbvW47wfTU8ulwMAQkNDMW7cOABAz549cfr0acTHx/OeNLE///wTCxYswKeffgp/f38AQN++fXHu3DmsWbOG98PGGvr9q1QqAIBGozF8X13H3t7e4u1hjxCZLCEhAXPmzMGwYcOwfv16wx/Q9u3b48aNG0Z1q3/28PBo8na2dtu2bUNOTo5hWfCAAQMAAEuXLsWYMWN4P2ygupu/R48eRuXdu3dHZmYm70kTO3nyJLRaLfr27WtU7uvri4sXL/J+2FhDv//qIbHa6tw5pGYJDEJkksTERCxbtgwTJ07E6tWrjbosBw4ciNTUVOh0OkPZTz/9BE9PT7Rp08YWzW3VVq1ahX379mHnzp2GLwB4+eWXERcXx/thA7169YKjoyNOnDhhVJ6eno7777+f96SJVf9FmpaWZlSenp6Ozp07837YWEO/fx8fHzg5OeHnn382HC8oKMDp06cNPXwWZfF1aNTqXLhwQejdu7cwa9Ys4caNG0ZfBQUFQnZ2tjBw4EBh4cKFwh9//CFs27ZN6Nu3r7B9+3ZbN100bl8+z/thGx9++KEwYMAAYffu3cKlS5eEjz76SPDx8RGOHDnCe9LEdDqd8OyzzwqPP/648NNPPwkZGRlCdHS00LNnT+H48eO8H01s4cKFRsvnTfn9R0VFCQ8++KCQnJxs2Efo0Ucftco+QhJBEATLxytqTWJiYhAdHV3rsXHjxuHtt9/GyZMnERkZidOnT6Nt27aYPn06Jk2a1MQtFS9vb2+sWLEC48ePBwDeDxuJj49HQkICrl+/jm7dumHOnDkYOXIkAN6Tppafn4/Vq1fju+++Q35+Pnr06IH58+fjwQcfBMD70ZQWLVqEP//8E59//rmhrKHfv06nQ1RUFLZv346ysjIMHDgQS5YsQceOHS3ePgYhIiIiEi3OESIiIiLRYhAiIiIi0WIQIiIiItFiECIiIiLRYhAiIiIi0WIQIiIiItFiECIiIiLR4kNXichkkydPxi+//FLn8R9++AFt27a1ejt+/vlnTJkyBQcOHLDIBmuLFi3Cjh076q1z5+MaTDV58mTcd999ePvttxv1flNs374dixcvbnQbicSMQYiIzDJq1ChERETUeqylPqcpIiICr7zyiuHnoUOHIjw8HKNHj77rc69ZswZ2dnZ3fR4isg4GISIyi0qlapJen6bk7OwMZ2fnGmWWuE5XV9e7PgcRWQ/nCBGRxQUFBSEmJgYzZsxAv379EBwcjC1bthjVOX78OKZMmYIHHngAgwYNQnh4OPLz8w3HKyoqsGbNGgQFBcHX1xfjx4/H4cOHjc7x/fffIyQkBH369MGYMWPw3XffGY5dvHgRoaGheOCBBzBgwACEhobe1dDR9u3bERQUhMjISPj7++PFF18EABw8eBATJkzAgAED0LdvXzz11FP473//a3jf5MmTsWjRIqNz7NixA8HBwejTpw+efPJJHD9+3FBfo9Fg5cqVePjhhzFgwAD885//xA8//GDUlv379yMkJAT9+vXDpEmT8NdffzX6uojEjkGIiKziww8/RN++fbFz505MnDgRS5Yswb59+wBUPnBx8uTJ6N69OzZv3owPPvgAJ0+exPTp06HX6wEAy5cvxxdffIGwsDDs3r0bgYGBmDlzJs6dO2f4jI0bN+LVV1/F7t270aVLF/zf//0fiouLAQDz589Hu3btsG3bNmzZsgVSqRSzZ8++q2v6888/cf36dezYsQOvvPIKfv/9d8yaNQuPPvoovv76a2zZsgVt2rRBWFgYNBpNree4ceMGvvzyS6xcuRKbN2+GVCrFwoULUf3Yx8WLFyMlJQUrV67Ejh07MGrUKLz44ouGkHfs2DHMmTMHjz76KHbt2oW///3viIuLu6vrIhIzDo0RkVl2796Nb7/9tkb58OHDERUVZfh5yJAhhuDRtWtXnDhxAp999hlGjx6NTz75BN7e3liyZAkAoHv37njvvffwxBNPICUlBQ888AC++uorvPrqq4Z5OnPnzoVerzcEHQAIDw/HoEGDAACzZs1CcnIyzp8/j379+uHy5csYMmQIOnbsCJlMhuXLl+PChQvQ6/WQShv/b8CZM2eiU6dOAIAzZ87g1VdfxcSJEw3Hp0yZgunTpyMnJwcdOnSo8X6tVovXX38dPXv2BADMmDEDs2bNQlZWFkpLS7Fnzx5s3boVffv2BQBMmzYNZ8+exYYNG/DII48gISEBfn5+mDNnDgDA09MT6enp2LhxY6OviUjMGISIyCxBQUEICwurUe7g4GD0c3VAqda/f39Dr0Z6ejqGDBlidNzb2xtqtRppaWlwd3eHVqtF//79jerMmzcPQOWqMaAyBFRTq9UAgLKyMkPd5cuXY9OmTQgICMDDDz+MUaNG3VUIAoAuXboYvu/ZsydcXFywfv16ZGRk4OLFizhz5gwAQKfT1XmObt26Gb6vnpuk1Wpx+vRpAJVh6nZardZwfbX97gYMGMAgRNRIDEJEZBZHR0d07ty5wXoymfH/XgRBMIQQQRAgkUhqvEev10Mul0Mul5vUltpCTfUQ08SJE/H444/j+++/x08//YSoqCisWbMGO3fuxD333GPS+WujUqkM3//666+YPn06AgMD4e/vjzFjxqC0tBSzZs2q9xwKhaLWdle3/YsvvoCjo6PR8duvtbpeNVN/X0RUE+cIEZFV/Pbbb0Y/Hzt2DL169QIA9OjRA0ePHjU6fvbsWRQVFaFbt27o3Lkz5HJ5jXM89dRT+Pjjjxv87OzsbLz55pvQarUYP348Vq5cia+//hpZWVn17oNkrg0bNmDQoEFYu3Yt/vWvf2HIkCG4evUqgJphxRReXl4AKucRde7c2fC1fft2bNu2DUBlL9SxY8eM3nfn74mITMcgRERmKSsrQ1ZWVq1f5eXlhnp79+5FQkICLl68iI8//hj79+/Hc889BwD417/+hbNnz+LNN9/E+fPn8csvvyAsLAy9evXC4MGDYW9vj0mTJuH999/HgQMHcPnyZURHR+PcuXMYPnx4g210dXXFd999h1dffRVnzpzBlStXkJiYCLlcjj59+ljsd9GhQwekpaXh6NGjyMzMxLZt2/D+++8DQJ2Tpevj5eWF4cOHY+nSpThw4ACuXLmCDRs2IDY21jAvafr06Th79izeeecdZGRk4Ouvv8YXX3xhsWsiEhsOjRGRWZKSkpCUlFTrsaioKIwZMwYAMHbsWPznP//BO++8gy5dumD16tUIDAwEUDmnZf369Xj//fcxduxYODk5YeTIkXjllVcMwzzz58+HTCbD66+/joKCAnh7eyMuLg7dunVDdnZ2vW2UyWRYv3493nnnHfzrX/9CaWkpevbsibi4ONx///0W+128/PLLyM7ONiyl7969O5YvX44FCxbg5MmTRnOBTBUdHY3o6GgsXboU+fn56NSpE5YtW4Ynn3wSQGWP0Pr167Fy5UokJCTAy8sLL774IlatWmWx6yISE4nQmP5bIqJ6BAUFYdy4cYaVTUREzRWHxoiIiEi0GISIiIhItDg0RkRERKLFHiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhItBiEiIiISLQYhIiIiEi0GISIiIhKt/weZ3xgqInSIoQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "df = pd.read_csv(r'Data\\Raw Experiment Data\\epoch experiment.csv')\n",
    "# Display the data for use as a figure\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"white\")\n",
    "plt.xlabel('Epochs Trained')\n",
    "plt.ylabel('Mean Absolute Error (m/s)')\n",
    "# plt.ylim(y_lim)\n",
    "plt.scatter(df['Epochs'], df['MAE'], color='skyblue')\n",
    "plt.grid(True, axis='y', color='grey')\n",
    "\n",
    "# Plot the line of best fit\n",
    "# plot_best_fit(df['Epochs'], df['MAE'], 3)\n",
    "plot_log_fit(df['Epochs'], df['MAE'], 1)\n",
    "\n",
    "# Remove the border from the graph\n",
    "for direction in ['top', 'right', 'bottom', 'left']:\n",
    "    ax.spines[direction].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9849809ad1fda",
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "# Train models for every selected site\n",
    "i = 1\n",
    "for filename in os.listdir(\"Data/NOW-23 Great Lakes [2000-2020] 60min\"):\n",
    "    print(f\"Point number {i} of 100\")\n",
    "    i += 1\n",
    "\n",
    "    model = define_model()\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename, cy=2015)\n",
    "    \n",
    "    model.fit(X_train,y_train,epochs=50,validation_data=(X_test,y_test),batch_size=128)\n",
    "    model.save(\"Data/Models/\" + filename[:-4] + \".keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a9d64c64daf62",
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "# Test models for every selected site\n",
    "mae, sites = list(), list()\n",
    "\n",
    "for filename in os.listdir(\"Data/Models\"):\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename[:-6] + '.csv', cy=2015)\n",
    "    model = keras.saving.load_model(\"Data/Models/\" + filename)\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    mae.append(mean_absolute_error(y_test[:, 0] * test_norms[2], predictions * train_norms[2]))\n",
    "    sites.append(filename[:-6])\n",
    "    print(mae[-1])\n",
    "df = pd.DataFrame()\n",
    "df['MAE'] = pd.Series(mae)\n",
    "df['SiteID'] = pd.Series(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29331c2c6d132ff6",
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "# Finally, we repeat this analysis with a persistence model that uses the wind speed from 24h before as a prediction, demonstrating the superiority of the LSTM model\n",
    "\n",
    "mae, rmse, sites = list(), list(), list()\n",
    "\n",
    "for filename in os.listdir(\"Data/Models\"):\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename[:-6] + '.csv', cy=2015)\n",
    "\n",
    "    predictions = [x[-1] for x in X_test[:, :, -1]]\n",
    "    mae.append(mean_absolute_error(y_test * test_norms[2], np.array(predictions) * test_norms[2]))\n",
    "    rmse.append(np.sqrt(mean_squared_error(y_test * test_norms[2], np.array(predictions) * test_norms[2])))\n",
    "    sites.append(filename[:-6])\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "df1['MAE'] = pd.Series(mae)\n",
    "df1['RMSE'] = pd.Series(rmse)\n",
    "df1['SiteID'] = pd.Series(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100b6dca60797aa",
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "# We compare the persistence model on a variety of loss metrics\n",
    "print(f\"Average MAE of the persistence model: {np.average(df1['MAE'])}\")\n",
    "print(f\"Median MAE of the persistence model: {np.median(df1['MAE'])}\")\n",
    "print(f\"Std MAE of the persistence model: {np.std(df1['MAE'])}\")\n",
    "print(f\"Average RMSE of the persistence model: {np.average(df1['RMSE'])}\")\n",
    "print(f\"Median RMSE of the persistence model: {np.median(df1['RMSE'])}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 386,
   "id": "ec63d005f23bdfaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T04:04:58.508225Z",
     "start_time": "2024-08-18T04:04:58.389411Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
=======
   "execution_count": 139,
   "id": "ec63d005f23bdfaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T21:07:59.476176Z",
     "start_time": "2024-08-22T21:07:59.403239Z"
>>>>>>> Stashed changes
    }
   },
   "outputs": [
    {
     "data": {
<<<<<<< Updated upstream
      "image/png": "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",
=======
      "image/png": "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",
>>>>>>> Stashed changes
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(r'Data/Raw Experiment Data/Final Model Performance.csv', index_col=0)\n",
    "df1 = pd.read_csv(r\"Data/Raw Experiment Data/WTK Model Performance.csv\", index_col=0)\n",
    "\n",
    "# Generate a box plot to describe the MAE distribution for use as a figure\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"white\")\n",
    "fig = plt.gcf()\n",
    "frame = plt.gca()\n",
    "fig.set_size_inches(12, 6)\n",
    "frame.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-ticks')\n",
    "bplot = plt.boxplot([df['MAE'], df1['MAE']], vert=False, patch_artist=True, medianprops=dict(color='black'),\n",
    "            boxprops=dict(facecolor='white'), widths=0.25)\n",
    "\n",
    "for patch, color in zip(bplot['boxes'], ['skyblue', 'seagreen']):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.xlabel(\"Mean Absolute Error (m/s)\")\n",
    "plt.tick_params(axis='x', which='major', reset=True, direction='in', top=False)\n",
    "ax.legend([bplot['boxes'][0], bplot['boxes'][1]], ['LSTM model', 'WTK Model'], loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479075149f92bed",
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "# We can print out some statistics of the distribution in detail\n",
    "print(f\"Mean: {np.average(df['MAE'])}\")\n",
    "print(f\"Median: {np.median(df['MAE'])}\")\n",
    "print(f\"Standard Deviation: {np.std(df['MAE'])}\")\n",
    "print(f\"n: {len(df['MAE'])}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"The persistence model has a higher MAE by {(np.average(df1['MAE'])/np.average(df['MAE']) - 1) * 100}%\")\n",
    "print(f\"Median difference: {(np.median(df1['MAE'])/np.median(df['MAE']) - 1) * 100}%\")\n",
    "# Unsurpisingly, the both the median and average MAE of the persistence model are around 25-30% higher than the average LSTM model MAE over all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89cd0e7d8d966cd",
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "# Parameters we need for statistical inference\n",
    "diff = pd.DataFrame()\n",
    "diff['MAE'] = df1['MAE'] - df['MAE']\n",
    "print('Difference statistics')\n",
    "print(f\"Mean: {np.average(diff['MAE'])}\")\n",
    "print(f\"Median: {np.median(diff['MAE'])}\")\n",
    "print(f\"Standard Deviation: {np.std(diff['MAE'])}\")\n",
    "print(f\"n: {len(diff['MAE'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e1cc26f7ca134",
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "# To describe the differences in train time with different model parameters, we train 3 models, each encompassing the 100 selected points in the study.\n",
    "df = pd.DataFrame()\n",
    "df['Average MAE'] = list()\n",
    "df['Median MAE'] = list()\n",
    "df['Average RMSE'] = list()\n",
    "df['Median RMSE'] = list()\n",
    "df['Train_time'] = list()\n",
    "df['Train_time std'] = list()\n",
    "\n",
    "for pair in [[100, 2000], [100, 2015], [50, 2015]]:\n",
    "    # Train models for every selected site\n",
    "\n",
    "    mae, rmse, time_elapsed = list(), list(), list()\n",
    "    i = 1\n",
    "    for filename in os.listdir(\"Data/NOW-23 Great Lakes [2000-2020] 60min\"):\n",
    "        print(f\"Point number {i} of 100\")\n",
    "        i += 1\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        model = define_model()\n",
    "        X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename, cy=pair[1])\n",
    "        model.fit(X_train,y_train,epochs=pair[0],validation_data=(X_test,y_test),batch_size=128)\n",
    "\n",
    "        predictions = model.predict(X_test)\n",
    "        mae.append(mean_absolute_error(y_test[:, 0] * test_norms[2], np.array(predictions) * train_norms[2]))\n",
    "        rmse.append(np.sqrt(mean_squared_error(y_test[:, 0] * test_norms[2], np.array(predictions) * train_norms[2])))\n",
    "    \n",
    "        time_elapsed.append((datetime.now() - start_time).total_seconds())\n",
    "    df.loc[len(df)+1] = [np.average(mae), np.median(mae), np.average(rmse), np.median(rmse), np.average(time_elapsed), np.std(time_elapsed)]\n",
    "df['Model'] = ['100 epochs, 20 years', '100 epochs, 5 years', '50 epochs, 5 years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34e8b15485d8ab",
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/Raw Experiment Data/Train Time Experiment.csv\")\n",
    "\n",
    "# Now we can generate a figure to demonstrate the significant change in train time between the models\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"white\")\n",
    "ax.set_yticks(range(5, 40, 5))\n",
    "plt.grid(True, axis='y', color='grey')\n",
    "plt.bar(df['Model'], df['Train_time'], width=0.4, color=\"skyblue\")\n",
    "plt.errorbar(df['Model'], df['Train_time'], yerr=df['Train_time std'], capsize=3, linestyle=\"\", color='deepskyblue')\n",
    "plt.ylabel(\"Average train time (seconds)\")\n",
    "plt.xlabel(\"Network parameters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175c7c6f159a3c8",
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "# We also compare the models to persistence, finding no major differences between the models in performance but significant improvements over persistence\n",
    "df.loc[len(df)+1] = [np.average(df1['MAE']), np.median(df1['MAE']), np.average(df1['RMSE']), np.median(df1['RMSE']), 0, \"Persistence\"]\n",
    "df.drop('Train_time', inplace=True, axis=1)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.9.19"
=======
   "version": "3.11.0"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
