{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b73532717ceab7c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Analysis Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89411512a29a662",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this notebook, we train, test, and evaluate the performance of an LSTM model in wind speed prediction and compare results to the persistence method, which is a common benchmark for wind speed prediction algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T04:34:52.388165Z",
     "start_time": "2024-08-18T04:34:52.383117Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "482b2cba580d03b1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T04:34:55.985319Z",
     "start_time": "2024-08-18T04:34:55.981804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define how many time steps will be used in observation and prediction\n",
    "n_past = 7 # The last seven days of data\n",
    "n_features = 6\n",
    "\n",
    "# Set the universal font size for matplotlib\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "db83f8ff572993e6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T04:34:56.747745Z",
     "start_time": "2024-08-18T04:34:56.743742Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to split the series using a sliding window\n",
    "def split_series(series, n_past=n_past):\n",
    "    X, y = list(), list()\n",
    "    n_past = n_past * 24\n",
    "    for i in range(n_past, len(series)):\n",
    "        X.append(series[range(i - n_past, i, 24), :])\n",
    "        y.append(float(series[i, 2]))\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "bbda487b9b141fb5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T04:34:57.290203Z",
     "start_time": "2024-08-18T04:34:57.282337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process and split the data for a site given its filename\n",
    "def prep_data(filename, cy=2018):\n",
    "    # Import the data for a single point\n",
    "    data = pd.read_csv(\"Data\\WTK_LED CONUS [2018-2020] 60min/\" + filename, index_col=0)\n",
    "    startyear = 2018\n",
    "\n",
    "    # Restrict the data to the last 5 years, giving us 4 years of training and 1 year of testing data\n",
    "    data = data.iloc[int(len(data)*(cy-startyear)/20):]\n",
    "\n",
    "    # Split the data into training and testing samples\n",
    "    cutoff = int(len(data)*0.8)\n",
    "    test_data = data.iloc[cutoff:]\n",
    "    data = data.iloc[:cutoff]\n",
    "    \n",
    "    # Designate which columns are used for training\n",
    "    columns = [5, 6, 7, 8, 9, 10]\n",
    "    \n",
    "    # Normalize the testing and training data\n",
    "    test_data.iloc[:, columns], test_norms = normalize(test_data.iloc[:, columns], axis=0, norm='max', return_norm=True)\n",
    "    data.iloc[:, columns], train_norms = normalize(data.iloc[:, columns], axis=0, norm='max', return_norm=True)\n",
    "\n",
    "    # Split the data into series for training\n",
    "    X_train, y_train = split_series(np.array(data.iloc[:, columns]))\n",
    "    X_test, y_test = split_series(np.array(test_data.iloc[:, columns]))\n",
    "\n",
    "    # Adjust the expected output to contain only the wind speed\n",
    "    # y_train, y_test = y_train[:, :, 2], y_test[:, :, 2]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, train_norms, test_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "be6ecadfc9437cad",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T04:35:00.210891Z",
     "start_time": "2024-08-18T04:35:00.204976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "def define_model():\n",
    "    # Lighter model used for additional training\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Input(shape=(n_past, n_features)))\n",
    "    model.add(LSTM(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(Dense(1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "6343d8cad2878f10",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T04:35:00.849969Z",
     "start_time": "2024-08-18T04:35:00.770876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"sequential_343\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_343\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm_352 (\u001B[38;5;33mLSTM\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │       \u001B[38;5;34m269,312\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_350 (\u001B[38;5;33mDropout\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_905 (\u001B[38;5;33mDense\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │        \u001B[38;5;34m32,896\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_906 (\u001B[38;5;33mDense\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │         \u001B[38;5;34m8,256\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_907 (\u001B[38;5;33mDense\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)             │         \u001B[38;5;34m2,080\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_908 (\u001B[38;5;33mDense\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │            \u001B[38;5;34m33\u001B[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm_352 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">269,312</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_350 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_905 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_906 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_907 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_908 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m312,577\u001B[0m (1.19 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">312,577</span> (1.19 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m312,577\u001B[0m (1.19 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">312,577</span> (1.19 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(None, 7, 6)"
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_model()\n",
    "model.summary()\n",
    "model.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 16ms/step - loss: 0.3390 - val_loss: 0.1700\n",
      "Epoch 2/15\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 14ms/step - loss: 0.1480 - val_loss: 0.1366\n",
      "Epoch 3/15\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 14ms/step - loss: 0.1270 - val_loss: 0.1245\n",
      "Epoch 4/15\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 14ms/step - loss: 0.1194 - val_loss: 0.1188\n",
      "Epoch 5/15\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 14ms/step - loss: 0.1167 - val_loss: 0.1171\n",
      "Epoch 6/15\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step - loss: 0.1147 - val_loss: 0.1142\n",
      "Epoch 7/15\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 15ms/step - loss: 0.1124 - val_loss: 0.1163\n",
      "Epoch 8/15\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 15ms/step - loss: 0.1122 - val_loss: 0.1192\n",
      "Epoch 9/15\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1110 - val_loss: 0.1147\n",
      "Epoch 10/15\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1105 - val_loss: 0.1110\n",
      "Epoch 11/15\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 15ms/step - loss: 0.1087 - val_loss: 0.1119\n",
      "Epoch 12/15\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1095 - val_loss: 0.1162\n",
      "Epoch 13/15\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1086 - val_loss: 0.1105\n",
      "Epoch 14/15\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 15ms/step - loss: 0.1072 - val_loss: 0.1088\n",
      "Epoch 15/15\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 15ms/step - loss: 0.1070 - val_loss: 0.1100\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "2.355576342782884\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data('7871' + '.csv', cy=2018)\n",
    "model = define_model()\n",
    "model.fit(X_train, y_train, epochs=15, validation_data=(X_test, y_test), batch_size=128)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print(mean_absolute_error(y_test * test_norms[2], predictions * train_norms[2]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T04:35:42.015729Z",
     "start_time": "2024-08-18T04:35:01.166697Z"
    }
   },
   "id": "582fd03776ec9732",
   "execution_count": 433
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on site number 1 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 16ms/step - loss: 0.3325 - val_loss: 0.1971\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step - loss: 0.1400 - val_loss: 0.1664\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step - loss: 0.1214 - val_loss: 0.1736\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step - loss: 0.1156 - val_loss: 0.1599\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step - loss: 0.1120 - val_loss: 0.1579\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step - loss: 0.1095 - val_loss: 0.1486\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step - loss: 0.1080 - val_loss: 0.1463\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 14ms/step - loss: 0.1066 - val_loss: 0.1513\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step - loss: 0.1060 - val_loss: 0.1478\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step - loss: 0.1053 - val_loss: 0.1485\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step - loss: 0.1047 - val_loss: 0.1463\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step - loss: 0.1053 - val_loss: 0.1476\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1040 - val_loss: 0.1503\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1034 - val_loss: 0.1486\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1034 - val_loss: 0.1442\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1034 - val_loss: 0.1530\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1018 - val_loss: 0.1536\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1019 - val_loss: 0.1467\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1031 - val_loss: 0.1493\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1025 - val_loss: 0.1557\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1019 - val_loss: 0.1593\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1023 - val_loss: 0.1494\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1013 - val_loss: 0.1507\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1007 - val_loss: 0.1463\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1005 - val_loss: 0.1446\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1007 - val_loss: 0.1536\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0996 - val_loss: 0.1548\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0986 - val_loss: 0.1488\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0991 - val_loss: 0.1534\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1003 - val_loss: 0.1516\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 2 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - loss: 0.2800 - val_loss: 0.2594\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step - loss: 0.0972 - val_loss: 0.2090\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0796 - val_loss: 0.2027\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0725 - val_loss: 0.2230\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0704 - val_loss: 0.2104\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0685 - val_loss: 0.2106\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0672 - val_loss: 0.2076\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0671 - val_loss: 0.1894\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0666 - val_loss: 0.2008\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0650 - val_loss: 0.2013\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0656 - val_loss: 0.1958\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0648 - val_loss: 0.1973\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0653 - val_loss: 0.2020\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0645 - val_loss: 0.2016\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0646 - val_loss: 0.2004\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0647 - val_loss: 0.1832\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0637 - val_loss: 0.1946\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0635 - val_loss: 0.2136\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0641 - val_loss: 0.2058\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0637 - val_loss: 0.2119\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0634 - val_loss: 0.2154\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0642 - val_loss: 0.1931\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0631 - val_loss: 0.2051\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.0633 - val_loss: 0.1978\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0632 - val_loss: 0.1967\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0636 - val_loss: 0.2097\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0632 - val_loss: 0.2101\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0630 - val_loss: 0.1955\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0629 - val_loss: 0.2000\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0625 - val_loss: 0.2004\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 3 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 18ms/step - loss: 0.3531 - val_loss: 0.1875\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1601 - val_loss: 0.1531\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1392 - val_loss: 0.1420\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1331 - val_loss: 0.1399\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1290 - val_loss: 0.1410\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1267 - val_loss: 0.1343\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1263 - val_loss: 0.1321\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1232 - val_loss: 0.1326\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1226 - val_loss: 0.1359\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1220 - val_loss: 0.1341\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1230 - val_loss: 0.1313\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1212 - val_loss: 0.1342\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1216 - val_loss: 0.1319\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1200 - val_loss: 0.1332\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1195 - val_loss: 0.1307\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1199 - val_loss: 0.1323\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1180 - val_loss: 0.1332\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1187 - val_loss: 0.1337\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1172 - val_loss: 0.1338\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1159 - val_loss: 0.1332\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1181 - val_loss: 0.1340\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1178 - val_loss: 0.1345\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1160 - val_loss: 0.1362\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1152 - val_loss: 0.1404\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1167 - val_loss: 0.1331\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1153 - val_loss: 0.1355\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1158 - val_loss: 0.1370\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1144 - val_loss: 0.1355\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1154 - val_loss: 0.1346\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1148 - val_loss: 0.1343\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 4 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - loss: 0.3350 - val_loss: 0.1823\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1503 - val_loss: 0.1489\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1300 - val_loss: 0.1404\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1228 - val_loss: 0.1377\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1203 - val_loss: 0.1416\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1186 - val_loss: 0.1353\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1174 - val_loss: 0.1373\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1169 - val_loss: 0.1358\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1172 - val_loss: 0.1354\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1168 - val_loss: 0.1340\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1151 - val_loss: 0.1351\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1156 - val_loss: 0.1369\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1159 - val_loss: 0.1338\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1151 - val_loss: 0.1360\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1150 - val_loss: 0.1347\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1149 - val_loss: 0.1359\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1136 - val_loss: 0.1362\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1142 - val_loss: 0.1363\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1141 - val_loss: 0.1357\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1130 - val_loss: 0.1363\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1118 - val_loss: 0.1367\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1126 - val_loss: 0.1365\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1115 - val_loss: 0.1370\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1121 - val_loss: 0.1379\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1112 - val_loss: 0.1401\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1110 - val_loss: 0.1366\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1119 - val_loss: 0.1379\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1095 - val_loss: 0.1390\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1094 - val_loss: 0.1401\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1085 - val_loss: 0.1406\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 5 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - loss: 0.3538 - val_loss: 0.1936\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1598 - val_loss: 0.1532\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1375 - val_loss: 0.1432\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1322 - val_loss: 0.1417\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1280 - val_loss: 0.1388\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1257 - val_loss: 0.1411\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1254 - val_loss: 0.1357\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1239 - val_loss: 0.1339\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1210 - val_loss: 0.1410\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1219 - val_loss: 0.1319\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1214 - val_loss: 0.1318\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1206 - val_loss: 0.1329\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1184 - val_loss: 0.1379\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1204 - val_loss: 0.1325\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1211 - val_loss: 0.1347\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1174 - val_loss: 0.1330\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1189 - val_loss: 0.1354\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1184 - val_loss: 0.1355\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1177 - val_loss: 0.1373\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1174 - val_loss: 0.1355\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1172 - val_loss: 0.1340\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1167 - val_loss: 0.1346\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1167 - val_loss: 0.1381\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1156 - val_loss: 0.1332\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1159 - val_loss: 0.1339\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1157 - val_loss: 0.1347\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1143 - val_loss: 0.1361\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1144 - val_loss: 0.1350\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1140 - val_loss: 0.1395\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1155 - val_loss: 0.1411\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 6 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - loss: 0.3530 - val_loss: 0.1563\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1620 - val_loss: 0.1211\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1381 - val_loss: 0.1087\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1269 - val_loss: 0.1068\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1241 - val_loss: 0.1033\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1215 - val_loss: 0.1012\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1196 - val_loss: 0.1092\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1178 - val_loss: 0.0994\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1160 - val_loss: 0.0977\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1168 - val_loss: 0.0961\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1154 - val_loss: 0.1013\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1154 - val_loss: 0.0970\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1141 - val_loss: 0.0975\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1136 - val_loss: 0.0953\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1118 - val_loss: 0.0948\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1115 - val_loss: 0.0967\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1117 - val_loss: 0.0947\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1113 - val_loss: 0.0952\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1108 - val_loss: 0.0967\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1115 - val_loss: 0.0973\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1109 - val_loss: 0.0968\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1106 - val_loss: 0.0975\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1103 - val_loss: 0.0964\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1083 - val_loss: 0.0967\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1079 - val_loss: 0.0980\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1083 - val_loss: 0.0982\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1089 - val_loss: 0.0984\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1068 - val_loss: 0.0984\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1069 - val_loss: 0.0988\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1071 - val_loss: 0.1012\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 7 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - loss: 0.2999 - val_loss: 0.1960\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1232 - val_loss: 0.1541\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1034 - val_loss: 0.1499\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0984 - val_loss: 0.1549\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0953 - val_loss: 0.1489\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0934 - val_loss: 0.1462\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.0938 - val_loss: 0.1378\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0926 - val_loss: 0.1461\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0927 - val_loss: 0.1421\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0920 - val_loss: 0.1449\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0912 - val_loss: 0.1335\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0917 - val_loss: 0.1368\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0926 - val_loss: 0.1358\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0910 - val_loss: 0.1329\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0908 - val_loss: 0.1392\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.0904 - val_loss: 0.1349\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0891 - val_loss: 0.1416\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0910 - val_loss: 0.1309\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0903 - val_loss: 0.1336\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0901 - val_loss: 0.1342\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0885 - val_loss: 0.1491\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0886 - val_loss: 0.1378\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0873 - val_loss: 0.1389\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0882 - val_loss: 0.1376\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0868 - val_loss: 0.1374\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0868 - val_loss: 0.1343\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0873 - val_loss: 0.1482\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0864 - val_loss: 0.1359\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0856 - val_loss: 0.1391\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0860 - val_loss: 0.1480\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 8 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - loss: 0.3257 - val_loss: 0.1625\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 15ms/step - loss: 0.1389 - val_loss: 0.1367\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1206 - val_loss: 0.1317\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1161 - val_loss: 0.1291\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1144 - val_loss: 0.1288\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1124 - val_loss: 0.1276\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1121 - val_loss: 0.1282\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1112 - val_loss: 0.1266\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1126 - val_loss: 0.1266\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1109 - val_loss: 0.1265\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1111 - val_loss: 0.1296\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1107 - val_loss: 0.1267\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1098 - val_loss: 0.1265\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1107 - val_loss: 0.1279\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1107 - val_loss: 0.1277\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1090 - val_loss: 0.1291\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1107 - val_loss: 0.1257\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1091 - val_loss: 0.1266\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1094 - val_loss: 0.1252\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1084 - val_loss: 0.1255\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1086 - val_loss: 0.1269\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1072 - val_loss: 0.1246\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1075 - val_loss: 0.1260\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1073 - val_loss: 0.1250\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1066 - val_loss: 0.1257\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1050 - val_loss: 0.1243\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1046 - val_loss: 0.1281\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1048 - val_loss: 0.1273\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1047 - val_loss: 0.1269\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1028 - val_loss: 0.1289\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 9 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - loss: 0.2684 - val_loss: 0.2000\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 15ms/step - loss: 0.0887 - val_loss: 0.1556\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0707 - val_loss: 0.1489\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0642 - val_loss: 0.1641\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0616 - val_loss: 0.1473\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0601 - val_loss: 0.1333\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0590 - val_loss: 0.1302\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0579 - val_loss: 0.1329\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0584 - val_loss: 0.1405\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0572 - val_loss: 0.1366\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0565 - val_loss: 0.1326\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0563 - val_loss: 0.1436\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0557 - val_loss: 0.1352\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0561 - val_loss: 0.1415\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0560 - val_loss: 0.1389\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0558 - val_loss: 0.1364\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0550 - val_loss: 0.1477\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0550 - val_loss: 0.1303\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0560 - val_loss: 0.1371\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0554 - val_loss: 0.1369\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0554 - val_loss: 0.1276\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0545 - val_loss: 0.1295\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0546 - val_loss: 0.1349\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0545 - val_loss: 0.1251\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0554 - val_loss: 0.1480\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0552 - val_loss: 0.1430\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0539 - val_loss: 0.1289\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0550 - val_loss: 0.1361\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0538 - val_loss: 0.1415\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0548 - val_loss: 0.1401\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 10 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - loss: 0.3270 - val_loss: 0.1638\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1451 - val_loss: 0.1352\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1241 - val_loss: 0.1288\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1169 - val_loss: 0.1257\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1155 - val_loss: 0.1250\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1139 - val_loss: 0.1243\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1140 - val_loss: 0.1240\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1122 - val_loss: 0.1241\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1090 - val_loss: 0.1247\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1103 - val_loss: 0.1238\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1091 - val_loss: 0.1255\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1074 - val_loss: 0.1263\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1081 - val_loss: 0.1268\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1087 - val_loss: 0.1257\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1067 - val_loss: 0.1272\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1063 - val_loss: 0.1251\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1051 - val_loss: 0.1251\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1055 - val_loss: 0.1244\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1040 - val_loss: 0.1256\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1042 - val_loss: 0.1265\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1047 - val_loss: 0.1250\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1035 - val_loss: 0.1266\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1028 - val_loss: 0.1264\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1039 - val_loss: 0.1265\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1022 - val_loss: 0.1260\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1035 - val_loss: 0.1265\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1031 - val_loss: 0.1272\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1000 - val_loss: 0.1317\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0997 - val_loss: 0.1282\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1001 - val_loss: 0.1289\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 11 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - loss: 0.2959 - val_loss: 0.2495\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1144 - val_loss: 0.1900\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0921 - val_loss: 0.1892\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0840 - val_loss: 0.1734\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0809 - val_loss: 0.1736\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0792 - val_loss: 0.1766\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0778 - val_loss: 0.1711\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0767 - val_loss: 0.1693\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0765 - val_loss: 0.1741\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.0757 - val_loss: 0.1651\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0753 - val_loss: 0.1758\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0746 - val_loss: 0.1746\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0745 - val_loss: 0.1601\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0742 - val_loss: 0.1609\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0739 - val_loss: 0.1729\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0735 - val_loss: 0.1645\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0743 - val_loss: 0.1709\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0739 - val_loss: 0.1636\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0738 - val_loss: 0.1760\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0725 - val_loss: 0.1571\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0739 - val_loss: 0.1672\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0734 - val_loss: 0.1755\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.0727 - val_loss: 0.1709\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0723 - val_loss: 0.1783\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.0725 - val_loss: 0.1622\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0728 - val_loss: 0.1687\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0719 - val_loss: 0.1725\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0724 - val_loss: 0.1582\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0724 - val_loss: 0.1731\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0716 - val_loss: 0.1831\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 12 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - loss: 0.3143 - val_loss: 0.2151\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1371 - val_loss: 0.1694\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1098 - val_loss: 0.1567\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1026 - val_loss: 0.1596\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0969 - val_loss: 0.1388\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0949 - val_loss: 0.1367\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0940 - val_loss: 0.1439\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0924 - val_loss: 0.1347\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0916 - val_loss: 0.1517\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0915 - val_loss: 0.1471\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0913 - val_loss: 0.1300\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0913 - val_loss: 0.1382\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0899 - val_loss: 0.1462\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0900 - val_loss: 0.1246\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0901 - val_loss: 0.1393\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0903 - val_loss: 0.1397\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0897 - val_loss: 0.1467\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0894 - val_loss: 0.1359\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0890 - val_loss: 0.1404\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0898 - val_loss: 0.1364\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0879 - val_loss: 0.1392\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0889 - val_loss: 0.1337\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0884 - val_loss: 0.1285\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0878 - val_loss: 0.1325\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0886 - val_loss: 0.1430\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0872 - val_loss: 0.1370\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0875 - val_loss: 0.1465\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0878 - val_loss: 0.1397\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0881 - val_loss: 0.1375\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.0881 - val_loss: 0.1348\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 13 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 17ms/step - loss: 0.3326 - val_loss: 0.1728\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1496 - val_loss: 0.1447\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1298 - val_loss: 0.1406\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1238 - val_loss: 0.1346\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1206 - val_loss: 0.1335\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1196 - val_loss: 0.1327\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1179 - val_loss: 0.1333\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1175 - val_loss: 0.1330\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1162 - val_loss: 0.1314\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1172 - val_loss: 0.1314\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1176 - val_loss: 0.1312\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1162 - val_loss: 0.1323\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1168 - val_loss: 0.1313\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1158 - val_loss: 0.1312\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1158 - val_loss: 0.1307\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1153 - val_loss: 0.1324\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1157 - val_loss: 0.1329\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1160 - val_loss: 0.1308\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1140 - val_loss: 0.1315\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1146 - val_loss: 0.1330\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1140 - val_loss: 0.1325\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1151 - val_loss: 0.1324\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1141 - val_loss: 0.1327\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1132 - val_loss: 0.1340\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1131 - val_loss: 0.1315\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1121 - val_loss: 0.1325\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1132 - val_loss: 0.1326\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1123 - val_loss: 0.1310\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1107 - val_loss: 0.1325\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1101 - val_loss: 0.1338\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 14 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - loss: 0.3388 - val_loss: 0.1581\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1503 - val_loss: 0.1266\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1278 - val_loss: 0.1204\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1208 - val_loss: 0.1171\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1180 - val_loss: 0.1178\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1173 - val_loss: 0.1145\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1161 - val_loss: 0.1146\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1156 - val_loss: 0.1149\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1134 - val_loss: 0.1153\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1131 - val_loss: 0.1152\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1133 - val_loss: 0.1148\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1127 - val_loss: 0.1164\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1118 - val_loss: 0.1172\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1135 - val_loss: 0.1157\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1129 - val_loss: 0.1151\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1101 - val_loss: 0.1163\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1115 - val_loss: 0.1160\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1106 - val_loss: 0.1169\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1101 - val_loss: 0.1167\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1086 - val_loss: 0.1160\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1090 - val_loss: 0.1172\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1074 - val_loss: 0.1167\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1074 - val_loss: 0.1147\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1062 - val_loss: 0.1196\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1065 - val_loss: 0.1180\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1063 - val_loss: 0.1171\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1060 - val_loss: 0.1185\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1043 - val_loss: 0.1202\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1050 - val_loss: 0.1164\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1036 - val_loss: 0.1173\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 15 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - loss: 0.3464 - val_loss: 0.1769\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1589 - val_loss: 0.1425\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1361 - val_loss: 0.1325\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1279 - val_loss: 0.1277\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1238 - val_loss: 0.1262\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1235 - val_loss: 0.1239\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1212 - val_loss: 0.1207\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1205 - val_loss: 0.1223\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1198 - val_loss: 0.1228\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1188 - val_loss: 0.1196\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1191 - val_loss: 0.1223\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1182 - val_loss: 0.1201\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1184 - val_loss: 0.1226\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1164 - val_loss: 0.1192\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1149 - val_loss: 0.1270\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1156 - val_loss: 0.1189\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1147 - val_loss: 0.1200\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1133 - val_loss: 0.1203\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1142 - val_loss: 0.1208\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1143 - val_loss: 0.1203\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1139 - val_loss: 0.1212\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1132 - val_loss: 0.1212\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1121 - val_loss: 0.1213\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1115 - val_loss: 0.1232\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1116 - val_loss: 0.1243\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1111 - val_loss: 0.1265\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1118 - val_loss: 0.1224\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1106 - val_loss: 0.1261\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1099 - val_loss: 0.1240\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1092 - val_loss: 0.1231\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 16 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - loss: 0.3241 - val_loss: 0.1634\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 16ms/step - loss: 0.1382 - val_loss: 0.1384\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1203 - val_loss: 0.1332\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1153 - val_loss: 0.1307\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1138 - val_loss: 0.1305\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1123 - val_loss: 0.1296\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1116 - val_loss: 0.1293\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1111 - val_loss: 0.1294\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1105 - val_loss: 0.1292\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1109 - val_loss: 0.1297\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1114 - val_loss: 0.1285\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1100 - val_loss: 0.1296\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1101 - val_loss: 0.1289\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1113 - val_loss: 0.1286\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1091 - val_loss: 0.1289\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1098 - val_loss: 0.1285\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1074 - val_loss: 0.1287\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1091 - val_loss: 0.1303\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1096 - val_loss: 0.1287\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1078 - val_loss: 0.1281\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1076 - val_loss: 0.1302\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1078 - val_loss: 0.1338\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1070 - val_loss: 0.1294\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1062 - val_loss: 0.1284\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1048 - val_loss: 0.1301\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1059 - val_loss: 0.1285\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1053 - val_loss: 0.1299\n",
      "Epoch 28/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1055 - val_loss: 0.1317\n",
      "Epoch 29/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1063 - val_loss: 0.1292\n",
      "Epoch 30/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1049 - val_loss: 0.1296\n",
      "\u001B[1m159/159\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
      "Training on site number 17 of 100\n",
      "Epoch 1/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 18ms/step - loss: 0.3442 - val_loss: 0.1696\n",
      "Epoch 2/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 17ms/step - loss: 0.1493 - val_loss: 0.1398\n",
      "Epoch 3/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1272 - val_loss: 0.1325\n",
      "Epoch 4/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1219 - val_loss: 0.1295\n",
      "Epoch 5/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1195 - val_loss: 0.1294\n",
      "Epoch 6/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1181 - val_loss: 0.1257\n",
      "Epoch 7/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1161 - val_loss: 0.1253\n",
      "Epoch 8/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1165 - val_loss: 0.1249\n",
      "Epoch 9/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1149 - val_loss: 0.1239\n",
      "Epoch 10/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1152 - val_loss: 0.1219\n",
      "Epoch 11/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1138 - val_loss: 0.1217\n",
      "Epoch 12/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1133 - val_loss: 0.1214\n",
      "Epoch 13/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1126 - val_loss: 0.1251\n",
      "Epoch 14/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1131 - val_loss: 0.1219\n",
      "Epoch 15/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1113 - val_loss: 0.1221\n",
      "Epoch 16/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1103 - val_loss: 0.1216\n",
      "Epoch 17/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1099 - val_loss: 0.1211\n",
      "Epoch 18/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1118 - val_loss: 0.1219\n",
      "Epoch 19/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1097 - val_loss: 0.1210\n",
      "Epoch 20/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1107 - val_loss: 0.1233\n",
      "Epoch 21/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1087 - val_loss: 0.1222\n",
      "Epoch 22/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1102 - val_loss: 0.1218\n",
      "Epoch 23/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1093 - val_loss: 0.1223\n",
      "Epoch 24/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 19ms/step - loss: 0.1096 - val_loss: 0.1257\n",
      "Epoch 25/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1071 - val_loss: 0.1244\n",
      "Epoch 26/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1072 - val_loss: 0.1248\n",
      "Epoch 27/30\n",
      "\u001B[1m163/163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 18ms/step - loss: 0.1070 - val_loss: 0.1228\n",
      "Epoch 28/30\n",
      "\u001B[1m110/163\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - loss: 0.1068"
     ]
    }
   ],
   "source": [
    "# Train models for every selected site\n",
    "i = 1\n",
    "df1 = pd.DataFrame()\n",
    "df1['SiteID'], df1['MAE'] = list(), list()\n",
    "\n",
    "for filename in os.listdir(\"Data/Models\"):\n",
    "    print(f\"Training on site number {i} of 100\")\n",
    "    i += 1\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename[:-6] + '.csv', cy=2018)\n",
    "    model = define_model()\n",
    "    model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), batch_size=128)\n",
    "    model.save(\"Data/WTK Models/\" + filename)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    df1.loc[len(df1)] = [filename[:-6], mean_absolute_error(y_test * test_norms[2], predictions * train_norms[2])]\n",
    "    \n",
    "df1.to_csv(\"Data/Raw Experiment Data/WTK Model Performance.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-08-18T04:35:57.123696Z"
    }
   },
   "id": "418e8571b6a50426",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd7e806169f1a8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train one model with data starting at different years\n",
    "df = pd.DataFrame()\n",
    "df['Years'] = list()\n",
    "df['MAE'] = list()\n",
    "for year in range(2000, 2020):\n",
    "    print(f\"{year}\")\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data('7871.csv', cy=year)\n",
    "    model = define_model()\n",
    "    model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), batch_size=128)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test[:, 0] * test_norms[2], predictions * train_norms[2])\n",
    "\n",
    "    df.loc[len(df) + 1] = [int(year), mae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861bcd17d03a0dc5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'Data\\Raw Experiment Data\\starting year experiment.csv')\n",
    "df['Years'] = [2020 - x for x in df['Years']]\n",
    "# Display the data for use as a figure\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"white\")\n",
    "plt.xlabel('Years of Data')\n",
    "plt.ylabel('Mean Absolute Error (m/s)')\n",
    "plt.xlim(-2, 22)\n",
    "plt.scatter(df['Years'], df['MAE'], color='skyblue')\n",
    "y_lim = ax.get_ylim()\n",
    "plt.grid(True, axis='y', color='grey')\n",
    "\n",
    "# Plot the polynomial line of best fit\n",
    "def plot_best_fit(x, y, deg):\n",
    "    coeffs = np.polyfit(x, y, deg)\n",
    "    \n",
    "    ylist = list()\n",
    "    for n in x:\n",
    "        yvalue = np.sum([coeffs[i]*n**(len(coeffs)-1-i) for i in range(len(coeffs))])\n",
    "        ylist.append(yvalue)\n",
    "    plt.plot(x, ylist, color='skyblue')\n",
    "\n",
    "# plot_best_fit(df['Years'], df['MAE'], 3)\n",
    "\n",
    "# Plot the logarithmic line of best fit\n",
    "def plot_log_fit(x, y, deg):\n",
    "    coeffs = np.polyfit(np.log(x), y, deg)\n",
    "\n",
    "    ylist = list()\n",
    "    for n in np.log(x):\n",
    "        yvalue = np.sum([coeffs[i]*n**(len(coeffs)-1-i) for i in range(len(coeffs))])\n",
    "        ylist.append(yvalue)\n",
    "    plt.plot(x, ylist, color='skyblue')\n",
    "\n",
    "plot_log_fit(df['Years'], df['MAE'], 3)\n",
    "\n",
    "# Remove the border from the graph\n",
    "for direction in ['top', 'right', 'bottom', 'left']:\n",
    "    ax.spines[direction].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2650c1c0f9da24",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train one model with a varying number of epochs\n",
    "df = pd.DataFrame()\n",
    "df['Epochs'] = list()\n",
    "df['MAE'] = list()\n",
    "for epoch in range(10, 160, 10):\n",
    "    print(f\"{epoch}\")\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data('7871.csv')\n",
    "    model = define_model()\n",
    "    model.fit(X_train,y_train,epochs=epoch,validation_data=(X_test,y_test),batch_size=128)\n",
    "        \n",
    "    predictions = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test[:, 0] * test_norms[2], predictions * train_norms[2])\n",
    "        \n",
    "    df.loc[len(df)+1] = [int(epoch), mae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b14b9e73fe7e0b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'Data\\Raw Experiment Data\\epoch experiment.csv')\n",
    "# Display the data for use as a figure\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"white\")\n",
    "plt.xlabel('Epochs Trained')\n",
    "plt.ylabel('Mean Absolute Error (m/s)')\n",
    "plt.ylim(y_lim)\n",
    "plt.scatter(df['Epochs'], df['MAE'], color='skyblue')\n",
    "plt.grid(True, axis='y', color='grey')\n",
    "\n",
    "# Plot the line of best fit\n",
    "# plot_best_fit(df['Epochs'], df['MAE'], 3)\n",
    "plot_log_fit(df['Epochs'], df['MAE'], 1)\n",
    "\n",
    "# Remove the border from the graph\n",
    "for direction in ['top', 'right', 'bottom', 'left']:\n",
    "    ax.spines[direction].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9849809ad1fda",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train models for every selected site\n",
    "i = 1\n",
    "for filename in os.listdir(\"Data/NOW-23 Great Lakes [2000-2020] 60min\"):\n",
    "    print(f\"Point number {i} of 100\")\n",
    "    i += 1\n",
    "\n",
    "    model = define_model()\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename, cy=2015)\n",
    "    \n",
    "    model.fit(X_train,y_train,epochs=50,validation_data=(X_test,y_test),batch_size=128)\n",
    "    model.save(\"Data/Models/\" + filename[:-4] + \".keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a9d64c64daf62",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test models for every selected site\n",
    "mae, sites = list(), list()\n",
    "\n",
    "for filename in os.listdir(\"Data/Models\"):\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename[:-6] + '.csv', cy=2015)\n",
    "    model = keras.saving.load_model(\"Data/Models/\" + filename)\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    mae.append(mean_absolute_error(y_test[:, 0] * test_norms[2], predictions * train_norms[2]))\n",
    "    sites.append(filename[:-6])\n",
    "    print(mae[-1])\n",
    "df = pd.DataFrame()\n",
    "df['MAE'] = pd.Series(mae)\n",
    "df['SiteID'] = pd.Series(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29331c2c6d132ff6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, we repeat this analysis with a persistence model that uses the wind speed from 24h before as a prediction, demonstrating the superiority of the LSTM model\n",
    "\n",
    "mae, rmse, sites = list(), list(), list()\n",
    "\n",
    "for filename in os.listdir(\"Data/Models\"):\n",
    "\n",
    "    X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename[:-6] + '.csv', cy=2015)\n",
    "\n",
    "    predictions = [x[-1] for x in X_test[:, :, -1]]\n",
    "    mae.append(mean_absolute_error(y_test * test_norms[2], np.array(predictions) * test_norms[2]))\n",
    "    rmse.append(np.sqrt(mean_squared_error(y_test * test_norms[2], np.array(predictions) * test_norms[2])))\n",
    "    sites.append(filename[:-6])\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "df1['MAE'] = pd.Series(mae)\n",
    "df1['RMSE'] = pd.Series(rmse)\n",
    "df1['SiteID'] = pd.Series(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100b6dca60797aa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We compare the persistence model on a variety of loss metrics\n",
    "print(f\"Average MAE of the persistence model: {np.average(df1['MAE'])}\")\n",
    "print(f\"Median MAE of the persistence model: {np.median(df1['MAE'])}\")\n",
    "print(f\"Std MAE of the persistence model: {np.std(df1['MAE'])}\")\n",
    "print(f\"Average RMSE of the persistence model: {np.average(df1['RMSE'])}\")\n",
    "print(f\"Median RMSE of the persistence model: {np.median(df1['RMSE'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1200x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(r'Data\\Raw Experiment Data\\Final Model Performance.csv', index_col=0)\n",
    "df1 = pd.read_csv(r\"Data/Raw Experiment Data/WTK Model Performance.csv\", index_col=0)\n",
    "\n",
    "# Generate a box plot to describe the MAE distribution for use as a figure\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"white\")\n",
    "fig = plt.gcf()\n",
    "frame = plt.gca()\n",
    "fig.set_size_inches(12, 6)\n",
    "frame.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-ticks')\n",
    "bplot = plt.boxplot([df['MAE'], df1['MAE']], vert=False, patch_artist=True, medianprops=dict(color='black'),\n",
    "            boxprops=dict(facecolor='white'), widths=0.25)\n",
    "\n",
    "for patch, color in zip(bplot['boxes'], ['skyblue', 'seagreen']):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.xlabel(\"Mean Absolute Error (m/s)\")\n",
    "plt.tick_params(axis='x', which='major', reset=True, direction='in', top=False)\n",
    "ax.legend([bplot['boxes'][0], bplot['boxes'][1]], ['LSTM model', 'WTK Model'], loc='upper center')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T04:04:58.508225Z",
     "start_time": "2024-08-18T04:04:58.389411Z"
    }
   },
   "id": "ec63d005f23bdfaa",
   "execution_count": 386
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# We can print out some statistics of the distribution in detail\n",
    "print(f\"Mean: {np.average(df['MAE'])}\")\n",
    "print(f\"Median: {np.median(df['MAE'])}\")\n",
    "print(f\"Standard Deviation: {np.std(df['MAE'])}\")\n",
    "print(f\"n: {len(df['MAE'])}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"The persistence model has a higher MAE by {(np.average(df1['MAE'])/np.average(df['MAE']) - 1) * 100}%\")\n",
    "print(f\"Median difference: {(np.median(df1['MAE'])/np.median(df['MAE']) - 1) * 100}%\")\n",
    "# Unsurpisingly, the both the median and average MAE of the persistence model are around 25-30% higher than the average LSTM model MAE over all sites"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e479075149f92bed",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Parameters we need for statistical inference\n",
    "diff = pd.DataFrame()\n",
    "diff['MAE'] = df1['MAE'] - df['MAE']\n",
    "print('Difference statistics')\n",
    "print(f\"Mean: {np.average(diff['MAE'])}\")\n",
    "print(f\"Median: {np.median(diff['MAE'])}\")\n",
    "print(f\"Standard Deviation: {np.std(diff['MAE'])}\")\n",
    "print(f\"n: {len(diff['MAE'])}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d89cd0e7d8d966cd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e1cc26f7ca134",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To describe the differences in train time with different model parameters, we train 3 models, each encompassing the 100 selected points in the study.\n",
    "df = pd.DataFrame()\n",
    "df['Average MAE'] = list()\n",
    "df['Median MAE'] = list()\n",
    "df['Average RMSE'] = list()\n",
    "df['Median RMSE'] = list()\n",
    "df['Train_time'] = list()\n",
    "df['Train_time std'] = list()\n",
    "\n",
    "for pair in [[100, 2000], [100, 2015], [50, 2015]]:\n",
    "    # Train models for every selected site\n",
    "\n",
    "    mae, rmse, time_elapsed = list(), list(), list()\n",
    "    i = 1\n",
    "    for filename in os.listdir(\"Data/NOW-23 Great Lakes [2000-2020] 60min\"):\n",
    "        print(f\"Point number {i} of 100\")\n",
    "        i += 1\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        model = define_model()\n",
    "        X_train, y_train, X_test, y_test, train_norms, test_norms = prep_data(filename, cy=pair[1])\n",
    "        model.fit(X_train,y_train,epochs=pair[0],validation_data=(X_test,y_test),batch_size=128)\n",
    "\n",
    "        predictions = model.predict(X_test)\n",
    "        mae.append(mean_absolute_error(y_test[:, 0] * test_norms[2], np.array(predictions) * train_norms[2]))\n",
    "        rmse.append(np.sqrt(mean_squared_error(y_test[:, 0] * test_norms[2], np.array(predictions) * train_norms[2])))\n",
    "    \n",
    "        time_elapsed.append((datetime.now() - start_time).total_seconds())\n",
    "    df.loc[len(df)+1] = [np.average(mae), np.median(mae), np.average(rmse), np.median(rmse), np.average(time_elapsed), np.std(time_elapsed)]\n",
    "df['Model'] = ['100 epochs, 20 years', '100 epochs, 5 years', '50 epochs, 5 years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34e8b15485d8ab",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/Raw Experiment Data/Train Time Experiment.csv\")\n",
    "\n",
    "# Now we can generate a figure to demonstrate the significant change in train time between the models\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"white\")\n",
    "ax.set_yticks(range(5, 40, 5))\n",
    "plt.grid(True, axis='y', color='grey')\n",
    "plt.bar(df['Model'], df['Train_time'], width=0.4, color=\"skyblue\")\n",
    "plt.errorbar(df['Model'], df['Train_time'], yerr=df['Train_time std'], capsize=3, linestyle=\"\", color='deepskyblue')\n",
    "plt.ylabel(\"Average train time (seconds)\")\n",
    "plt.xlabel(\"Network parameters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175c7c6f159a3c8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We also compare the models to persistence, finding no major differences between the models in performance but significant improvements over persistence\n",
    "df.loc[len(df)+1] = [np.average(df1['MAE']), np.median(df1['MAE']), np.average(df1['RMSE']), np.median(df1['RMSE']), 0, \"Persistence\"]\n",
    "df.drop('Train_time', inplace=True, axis=1)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
